VERS 00000001 4
HEAD 00001147 <?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/transform.xslt"?><header><componentName>correlator</componentName><version>9.10.0.3.284318</version><build>rel/9.10.0.x@284318</build><buildPlatform>amd64-win</buildPlatform><platform>Windows 8</platform><cputype>GenuineIntel family 6 model 5 stepping 1 Intel(R) Core(TM) i5-4278U CPU @ 2.60GHz</cputype><cpus>1</cpus><javaEnabled>false</javaEnabled><replayLogMode>inputLog</replayLogMode><args><arg>C:\SoftwareAG\Apama\bin\correlator</arg><arg>-p</arg><arg>45647</arg><arg>-f</arg><arg>correlator.log</arg><arg>--inputLog</arg><arg>C:\Users\moray\Development\cycle-monitor\test\testcases\CycleMonitor_cor_009\Output\win32\correlator-input.log</arg></args><environment><variable>TMP=C:\Users\moray\AppData\Local\Temp</variable><variable>COMPUTERNAME=WIN-0RKCERDGPP2</variable><variable>APAMA_HOME=C:\SoftwareAG\Apama</variable><variable>USERDOMAIN=WIN-0RKCERDGPP2</variable><variable>PSMODULEPATH=C:\Windows\system32\WindowsPowerShell\v1.0\Modules\</variable><variable>COMMONPROGRAMFILES=C:\Program Files\Common Files</variable><variable>PROCESSOR_IDENTIFIER=Intel64 Family 6 Model 69 Stepping 1, GenuineIntel</variable><variable>LOGONSERVER=\\WIN-0RKCERDGPP2</variable><variable>PROGRAMFILES=C:\Program Files</variable><variable>APAMA_LIBRARY_VERSION=9.10</variable><variable>PATH=C:\SoftwareAG\Apama\third_party\python;C:\Users\Public\SoftwareAG\ApamaWork_9.10\lib;C:\SoftwareAG\Apama\bin;C:\SoftwareAG\Apama\adapters\bin;C:\SoftwareAG\jvm\jvm\jre\bin\server;C:\SoftwareAG\jvm\jvm\jre\bin;C:\SoftwareAG\Apama\third_party\apache_ant\bin;C:\SoftwareAG\Apama\third_party\python;C:\SoftwareAG\jvm\jvm\jre\..\bin;C:\ProgramData\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Notepad++\</variable><variable>SYSTEMROOT=C:\Windows</variable><variable>APAMA_ENVSET=9.10:C:\SoftwareAG\Apama:amd64-win</variable><variable>PROGRAMFILES(X86)=C:\Program Files (x86)</variable><variable>TK_LIBRARY=C:\SoftwareAG\Apama\third_party\python\tcl\tk8.5</variable><variable>APAMA_CORRELATOR_HOME=C:\SoftwareAG\Apama</variable><variable>APAMA_WORK=C:\Users\Public\SoftwareAG\ApamaWork_9.10</variable><variable>TEMP=C:\Users\moray\AppData\Local\Temp</variable><variable>COMMONPROGRAMFILES(X86)=C:\Program Files (x86)\Common Files</variable><variable>APAMA_JRE=C:\SoftwareAG\jvm\jvm\jre</variable><variable>PROCESSOR_ARCHITECTURE=AMD64</variable><variable>APAMA_PLATFORM=amd64-win</variable><variable>TIX_LIBRARY=C:\SoftwareAG\Apama\third_party\python\tcl\tix8.4.3</variable><variable>ALLUSERSPROFILE=C:\ProgramData</variable><variable>LOCALAPPDATA=C:\Users\moray\AppData\Local</variable><variable>HOMEPATH=\Users\moray</variable><variable>USERDOMAIN_ROAMINGPROFILE=WIN-0RKCERDGPP2</variable><variable>JAVA_HOME=C:\SoftwareAG\jvm\jvm</variable><variable>PROGRAMW6432=C:\Program Files</variable><variable>USERNAME=moray</variable><variable>APAMA_MANAGEMENT_HOME=C:\SoftwareAG\Apama</variable><variable>PROMPT=$P$G</variable><variable>COMSPEC=C:\Windows\system32\cmd.exe</variable><variable>APAMA_COMMON_JRE=C:\SoftwareAG\jvm\jvm\jre</variable><variable>PROGRAMDATA=C:\ProgramData</variable><variable>PYTHONPATH=C:\SoftwareAG\Apama\third_party\python\Lib\site-packages</variable><variable>TCL_LIBRARY=C:\SoftwareAG\Apama\third_party\python\tcl\tcl8.5</variable><variable>TEAMCITY_DATA_PATH=C:\ProgramData\JetBrains\TeamCity</variable><variable>SESSIONNAME=Console</variable><variable>PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC</variable><variable>APAMA_DEV_HOME=C:\SoftwareAG\Apama</variable><variable>FP_NO_HOST_CHECK=NO</variable><variable>WINDIR=C:\Windows</variable><variable>COMMONPROGRAMW6432=C:\Program Files\Common Files</variable><variable>PROCESSOR_REVISION=4501</variable><variable>HOMEDRIVE=C:</variable><variable>APPDATA=C:\Users\moray\AppData\Roaming</variable><variable>ANT_HOME=C:\SoftwareAG\Apama\third_party\apache_ant</variable><variable>SYSTEMDRIVE=C:</variable><variable>NUMBER_OF_PROCESSORS=1</variable><variable>PROCESSOR_LEVEL=6</variable><variable>JRE_HOME=C:\SoftwareAG\jvm\jvm\jre</variable><variable>APAMA_DASHBOARD_HOME=C:\SoftwareAG\Apama</variable><variable>OS=Windows_NT</variable><variable>PUBLIC=C:\Users\Public</variable><variable>USERPROFILE=C:\Users\moray</variable></environment><version>9.10.0.3.284318</version></header>
RAND 00000009 842923698
TIME 00000010 1474016706.366,1
TIME 0000000e 1474016706.4,1
TIME 0000000e 1474016706.5,1
CONN 0000003c 6330853567618646350:6330572092641935694 from 127.0.0.1:53164
TIME 0000000e 1474016711.6,1
CONN 0000003e 14826393442096512897:14826392888045600641 from 127.0.0.1:53161
TIME 0000000e 1474016711.7,1
CONN 0000003c 6330853567799787854:6330572092823077198 from 127.0.0.1:53172
TIME 0000000e 1474016711.9,1
MONF 0001252f package com.apama.scenario;

/**
 * This file contains the shared event definitions that are generic across 
 * all Scenarios.  
 *
 * WARNING: 
 *    The event definitions contained in this file form an internal protocol 
 *    and may change between software releases.
 *
 * Notes:
 * 1) The Event definitions contained in this file MUST be considered
 *    as an internal implementation of the communications protocol
 *    between the Apama client API and an Apama server.  As such
 *    these event definitions MUST NOT be considered "stable" and are 
 *    subject to change in any future software release.
 *
 *    The ONLY supported public APIs to the Scenario Service are the 
 *    Java client API (in the com.apama.services.scenario package) and the 
 *    .NET client API (in the Apama.Services.Scenario namespace).  
 *    Customers should not attempt to interface at the event or 
 *    EPL layer. Some events have been changed over time (as 
 *    noted here).
 *
 * 2) Most events now contain an initial field called "scenarioId". This string 
 *    uniquely identifies a scenario inside the correlator, and is used as 
 *    the package name, in several events, and for making up a part of the 
 *    data and control channel names.
 *
 * 3) Events that are intended to be used in a request-response pattern contain
 *    a "messageId" field.  The value of this field must be copied from the 
 *    request event into the response event. The mechanism allows clients to 
 *    match up request-response pairs.
 *
 *
 * $Copyright(c) 2005-2011 Progress Software Corporation (PSC). All rights reserved.$
 * $Copyright (c) 2013-2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
 * Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
 *
 * $Revision: 267710 $
 */

/**
 * Request that a new instance of a specific scenario is created.
 *
 * See also: Created(), Acknowledge()
 *
 * Direction: From the client to the correlator.
 *
 * Response: Acknowledge()
 */
event Create {
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	string owner;                        // the owner (user) of the instance.
	sequence<string> inputFieldValues;   // sequence of INPUT field values in string form
}

/**
 * Notifies all interested clients that a new instance of a specific 
 * scenario has been created. 
 * The event provides the owner (user), initial state, and initial values for 
 * all input fields and all output fields.
 *
 * See also: Create()
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Data and <scenarioId>.Data.Raw
 */
event Created { 
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the ID of the scenario instance
	string owner;                        // the owner (user) of the instance.
	string state;                        // the initial state of the instance.
	sequence<string> inputFieldValues;   // sequence of INPUT field values in string form
	sequence<string> outputFieldValues;  // sequence of OUTPUT field values in string form
}


/**
 * Notifies the interested clients of child scenarios created from
 * a parent scenario.
 *
 * Direction: Internal in the correlator
 *
 * Channel: <none>
 */
event ParentChildRelationship {
	string parentScenarioId;
	integer parentScenarioInstanceId;
	string childScenarioId;
	integer childScenarioInstanceId;
}


/**
 * Request that a specific instance of a specific scenario is edited (the input 
 * field values are changed).
 *
 * See also: Edited(), Acknowledge()
 *
 * Direction: From the client to the correlator.
 *
 * Response: Acknowledge()
 */
event Edit { 
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the ID of the scenario instance
	sequence<string> inputFieldValues;   // sequence of INPUT field values in string form
}


/**
 * Notifies all interested clients that a specific instance of a specific 
 * scenario has been edited (the input field values have changed).
 *
 * See also: Edit()
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Data and <scenarioId>.Data.Raw
 */
event Edited { 
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the ID of the scenario instance
	sequence<string> inputFieldValues;   // sequence of INPUT field values in string form
	sequence<string> outputFieldValues;  // sequence of OUTPUT field values in string form
}


/**
 * Request that a specific instance of a specific scenario is deleted.
 *
 * See also: Deleted(), Acknowledge()
 *
 * Direction: From the client to the correlator.
 *
 * Response: Acknowledge()
 */
event Delete {
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the ID of the scenario instance
}


/**
 * Notifies all interested clients that a specific instance of a specific 
 * scenario has been deleted.
 *
 * See also: Delete()
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Data and <scenarioId>.Data.Raw
 */
event Deleted {
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the ID of the scenario instance
}


/**
 * Indicates that a specific instance of a specific scenario has died.
 * This is sent as a result of a scenario instance use of MonitorScript ondie.
 * This occurs for any of deleting a running scenario, a scenario failing, or 
 * entering the end state.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Data and <scenarioId>.Data.Raw
 */
event InstanceDied {
	string scenarioId;                   // the unique name of the scenario
	integer scenarioInstanceId;          // the ID of the scenario instance
}


/**
 * Contains updated scenario instance output fields.
 * Note that this is the ONLY event type that is sendted on the 
 * "<scenarioId>.Data" channel.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Data and <scenarioId>.Data.Raw
 */
event Update { 
	string scenarioId;                   // the unique name of the scenario
	integer scenarioInstanceId;          // the ID of the scenario instance
	float timeStamp;                     // the time of the update (seconds since epoch)
	sequence<string> outputFieldValues;  // sequence of OUTPUT field values in string form
}


/**
 * A general Acknowledgement event that is the "response" to various "request"
 * events such as Create, Edit, Delete.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Control
 */
event Acknowledge { 
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the id of the scenario instance
	boolean success;                     // boolean indication of success
	sequence<string> outputFieldValues;  // sequence of the OUTPUT field values in string form, 
	                                     //   or an empty sequence if success = false
}


/**
 * Indicates that a specific instance of a specific scenario has changed 
 * state, where valid states include "ENDED", "FAILED", "RUNNING".
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Data and <scenarioId>.Data.Raw
 */
event StateChange { 
	string scenarioId;                   // the unique name of the scenario.
	integer scenarioInstanceId;          // the ID of the scenario instance.
	string state;                        // the new state.
}


/**
 * Request that each scenario loaded in the correlator send its meta
 * information out on the supplied channel. When all scenarios have
 * sent out this information, a final RequestScenariosDone event will
 * be sent on the same channel.
 *
 * As soon as this event is received, a RequestScenariosAck will be
 * sent on the same channel so client can stop resending the RequestScenarios
 * event.
 *
 * See also: Scenario, RequestScenariosDone, RequestScenariosAck
 *
 * Direction: From the client to the correlator.
 *
 * Response: Scenario() from each loaded scenario.
 */
event RequestScenarios { 
	// renamed for clarity - was Request
	string channel;                      // Name of the private response channel.
}


/**
 * This is a simple marker event that is sent by the correlator to indicate 
 * that it has finished sending all of the Scenario events in response
 * to a RequestScenarios event.
 * 
 * See also: RequestScenarios, Scenario.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: The private channel that was specified in the RequestScenarios event.
 */
event RequestScenariosDone {}

/* This is a simple ack event that is sent by the correlator to indicate the
 * RequestScenarios event is received and the request is being processed
 *
 * See also: RequestScenarios
 *
 * Direction: From the correlator to the client.
 *
 * Channel: The private channel that was specified in the RequestScenarios event.
 */
event RequestScenariosAck {}

/**
 * Request that each instance of the specified scenario send an Instance event 
 * out on the supplied channel. When all instances for the scenario have been 
 * sent out, a final RequestInstancesDone event will be sent on the same channel.
 *
 * See also: Instance, RequestInstancesDone
 *
 * Direction: From the client to the correlator.
 *
 * Response: Instance() from each scenario instance.
 */
event RequestInstancesOnChannel { 
	string scenarioId;                   // Identifier of the scenario for which to return instances. 
	integer messageId;                   // the unique message ID (for request-response matching)
	string channel;                      // Name of the private response channel.
}

/**
 * Request that each instance for the specified user of the specified scenario
 * send an Instance event out on the supplied channel. When all instances for
 * the scenario have been sent out, a final RequestInstancesDone event will be 
 * sent on the same channel.
 *
 * See also: Instance, RequestInstancesDone
 *
 * Direction: From the client to the correlator.
 *
 * Response: Instance() from each scenario instance.
 */
event RequestInstancesOnChannelByUser { 
	string scenarioId;                   // Identifier of the scenario for which to return instances. 
	integer messageId;                   // the unique message ID (for request-response matching)
	string channel;                      // Name of the private response channel.
	string owner;                        // the username to filter by
}


/**
 * This is a simple marker event that is sent by the correlator to indicate 
 * that it has finished sending all of the Instance events in response
 * to a RequestInstancesInternal event.
 * 
 * See also: RequestInstancesInternal, Scenario.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: A unique private channel as specified by the client in a 
 *          RequestScenariosOnChannel event.
 *
 *          Note that previously it would have been on <scenarioId>.Data and 
 *          <scenarioId>.Data.Raw, but that behaviour is now deprecated.
 */
event RequestInstancesDone {
	string scenarioId;                   // Identifier of the scenario for instances were returned. 	
	integer messageId;                   // the unique message ID (for request-response matching)
}


/**
 * Describes the meta-information about a scenario that is loaded in the 
 * correlator.
 * 
 * See also: RequestScenarios, RequestScenariosDone, ScenarioUnloaded
 * 
 * Direction: From correlator to client.
 *
 * Channel:   1) com.apama.scenario to broadcast when loaded.
 *            2) A unique private channel as specified by the client in a 
 *               RequestScenarios event.
 *
 * This event has gained the executionMode field in Apama 4.2
 */
event Scenario {
	string scenarioId;                   // unique identifier for Scenario, e.g. Scenario_statistical$002darbitrage
	string displayName;                  // user-specified name for Scenario, e.g. statistical-arbitrage
	string description;                  // description of the Scenario
	sequence<string> inputNames;         // input parameter names
	sequence<string> inputTypes;         // input parameter types
	sequence<string> inputConstraints;   // input parameter contraints
	sequence<string> inputDefaults;      // input parameter default values
	sequence<string> outputNames;        // output parameter names
	sequence<string> outputTypes;        // output parameter types
	integer executionMode;               // 0 = serial, 1 = parallel, 2 = parallel child. New as of 4.2
	dictionary<string, string> extraParams; // Additional parameters
}



/**
 * Indicates that a specific Scenario definition is being unloaded.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: <scenarioId>.Control
 */
event ScenarioUnloaded { 
	string scenarioId;                   // the unique name of the scenario
}

/**
 * Provides a dump of the current state of a scenario instance.
 * The event definition is identical to the Created event, and includes the 
 * owner (user), initial state, and current values for all input fields and 
 * all output fields.
 *
 * Instance events are sent in response to RequestInstancesOnChannel event 
 * and the deprecated RequestInstancesInternal event; 
 *
 * See also: RequestInstancesOnChannel, RequestInstancesDone, RequestInstancesInternal
 *
 * Direction: From the correlator to the client.
 *
 * Channel: A unique private channel as specified by the client in a 
 *          RequestScenariosOnChannel event.
 *
 *          Note that previously it would have been on <scenarioId>.Data and 
 *          <scenarioId>.Data.Raw, but that behaviour is now deprecated.
 */
event Instance {
	string scenarioId;                   // the unique name of the scenario. 
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the ID of the scenario instance. 
	string owner;                        // the owner (user) of the instance.
	string state;                        // the initial state of the instance.
	sequence<string> inputFieldValues;   // sequence of INPUT field values in string form. 
	sequence<string> outputFieldValues;  // sequence of OUTPUT field values in string form. 
}


/**
 * Indicates this shared MonitorScript has been loaded.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: com.apama.scenario to broadcast when loaded.
 */
event ScenarioServiceLoaded { 
}

/**
 * Indicates this shared MonitorScript is being unloaded.
 *
 * Direction: From the correlator to the client.
 *
 * Channel: com.apama.scenario to broadcast when unloaded.
 */
event ScenarioServiceUnloaded { 
}


/**
 * Set the period over which updates will be queued and coalesced before being
 * sent out on the data channel. Defaults to zero, which means they won't be
 * queued. If it is negative then we won't send any updates on the data channel.
 * This event is deprecated and the ConfigureUpdates event should be used 
 * instead (see below)
 */
event SetThrottlingPeriod {
	float period;   // The period in seconds. Default zero.
}


/**
 * Configures how updates are sent from scenarios.
 * Each scenario is controlled by two configurations - a global default,
 * and an optional per scenario configuration.  The per scenario
 * configuration takes precedence over the global default.
 * The configuration is made up of a number of entries in 
 * the configuration dictionary. The ConfigureUpdate event 
 * is merged into any previous configuration.
 *
 * Global configuration can be modified by specifying an empty string 
 * for scenarioId, and empty values remove values.
 */
event ConfigureUpdates {
	/** Specifies the scenario the configuratio will apply to, or use 
		empty string "" to specify a global default. 
	*/
	string scenarioId;
	/** 
	 * A set of the configurations modified by this event
	 * the key and meaning is one of:
	 * sendThrottled - boolean - whether to send Updates to the 
	 *                           Throttled (.Data) channel (default=true)
	 * throttlePeriod - float - period with which to send Updates. 
	 *                          0.0 means updates are not throttled - 
	 *                          every update is sent on the Throttled 
	 *                          channel. (default=0.0)
	 * sendRaw - boolean - whether to send Updates on the Raw channel 
	 *                     (.Raw) (default=true)
 	 * sendThrottledUser - boolean - whether to send Updates to the 
	 *                     throttled filtered (.Data:username) channel
	 *                     (default=false)
 	 * sendRawUser - boolean - whether to send Updates to the Raw 
	 *                     channel (.Raw:username) (default=false)
	 * routeUpdate - boolean - whether to route Update (and Edited, Deleted) events.
	 * An empty value removes that entry from the configuration
	 */
	dictionary<string,string> configuration;
}

/**
 * Immediately flushes to receivers any scenario Update events that were 
 * waiting for the next throttling period before being sent. 
 * 
 */
event SendQueuedUpdatesNow {
	
}

/* ==========================================================================
 * The following describes INTERNAL event definitions that should not be sent 
 * into the correlator, nor relied upon.
 * ==========================================================================
 */

/**
 * Request that each instance of the specified scenario send an Instance event 
 * out on the scenario Data or Raw channel. When all instances for the scenario
 * have been sent out, a final RequestInstancesDone event will be sent on the 
 * same channel.
 *
 * See also: Instance, RequestInstancesDone, RequestInstancesOnChannel, RequestInstancesOnChannelByUser
 *
 * Direction: From the ScenarioService to the scenario
 *
 * Response: Instance() from each scenario instance, RequestInstancesDone when finished.
 */
event RequestInstancesInternal { 
	string scenarioId;                   // Identifier of the scenario for which to return instances. 
	integer messageId;                   // the unique message ID (for request-response matching)
	string channel;                      // Name of the private response channel.
	boolean internal;                    // if true, events should be routed/ send-to'd the main context
	string owner;                        // owner filter (optional)
	boolean ownerFilter;                 // whether to filter by owner
}

/**
 * A scenario has finished running, but is still discoverable.
 * (i.e. entered end state or failed - but not deleted).
 * Note that this event contains the state as of the last
 * Update/ Edited event - i.e. if an action modified an output variable 
 * and then caused the scenario to fail, the prior modification
 * would not be reflected in this event.
 *
 * This event contains sufficient information for discovery of the
 * scenario instance later
 * 
 * Direction: from scenarios to the ScenarioService sub-monitor (spawned per scenario)
 */
event ScenarioFinished {
	string scenarioId;                   // Identifier of the scenario which has failed. 
	integer scenarioInstanceId;          // the ID of the scenario instance. 
	string owner;                        // the owner (user) of the instance.
	string state;                        // the initial state of the instance.
	sequence<string> inputFieldValues;   // sequence of INPUT field values in string form. 
	sequence<string> outputFieldValues;  // sequence of OUTPUT field values in string form. 
}
/**
 * Only used internally to tell all the scenarios to start routing their
 * meta data (Scenario). It is followed by a sweeper FinishedScenarioRecovery
 * event which indicates that all the scenarios have reported in.
 */
event StartScenarioRecovery {}

/**
 * Sweeper event to indicate that scenario recovery is done. Only used internally.
 */
event FinishedScenarioRecovery {}

/**
 * Trigger discovery of a parallel scenario. Sent from a sub-monitor of
 * ScenarioService to RequestInstancesHandler to create a new sub-monitor.
 */
event RequestInstancesParallel {
	RequestInstancesInternal request;          // the original request event
	dictionary<integer, context> instances;    // all scenario instances and their running context
	integer highestInstanceId;                 // the highest scenarioInstanceId listed in instances (or more accurately, the highest when discovery started)
}

/**
 * Discovery protocol of a parallel scenario. Sent from a scenario instance
 * to RequestInstancesHandler.
 */
event RequestInstancesParallelDone {
	string scenarioId;                   // the unique name of the scenario
	integer messageId;                   // the unique message ID (for request-response matching)
	integer scenarioInstanceId;          // the ID of the scenario instance
}

/**
 * Notification of a new scenario instance
 */
event ParallelStarting {
	string scenarioId;                   // the unique name of the scenario
	integer scenarioInstanceId;          // the ID of the scenario instance
	string owner;                        // the owner of the scenario instance
	context runningCtx;                  // the context the scenario is running in
}

/**
 * Get the current configuration for a given scenario and the default configuration
 */
event GetConfiguration {
	string scenarioId;                   // the unique name of the scenario
}

/**
 * The current configuration for a given scenario and the default configuration
 * @see ConfigureUpdates
 */
event Configuration {
	string scenarioId;                        // the unique name of the scenario
	dictionary<string, string> defaults;      // the global defaults
	dictionary<string, string> configuration; // the scenario configuration (takes precedence)
}


/**
 * An operation has completed. Sent from parallel scenarios to the main context.
 */
event OperationCompleted {
	string scenarioId;
	integer scenarioInstanceId;
	integer messageId;
}

/**
 * Notification that a scenario has loaded a ConfigureUpdates event
 */
event ScenarioProcessedUpdates {
	string scenarioId;
}

/**
 * Request all configuration
 */
event GetAllConfiguration {
	integer requestId;
}

/** 
 * Response all configuration
 */
event AllConfiguration {
	integer requestId;
	dictionary<string, string> defaultConfig;
	dictionary<string, dictionary<string, string> > configurations;
}


/**
 * Library of utiltiy actions
 */
event ScenarioServiceLibrary {

	/**
	 * Get the control channel for a scenario Id. This channel
	 * is always enabled (uses an event set scenario ID)
	 */
	action getControlChannel(string scenarioId) returns string
	{
		return scenarioId+".Control";
	}
	/**
	 * Get the data channel for a scenario ID. This channel 
	 * is enabled by the sendThrottled configuration key and 
	 * the throttlePeriod key (Update events may be throttled)
	 */
	action getDataChannel(string scenarioId) returns string
	{
		return scenarioId+".Data";
	}
	/**
	 * Get the raw channel for a scenario ID. This channel
	 * is enabled by the sendRaw configuration key.
	 */
	action getRawChannel(string scenarioId) returns string
	{
		return scenarioId+".Data.Raw";
	}
	/**
	 * Get the data channel for a scenario Id. This channel 
	 * is enabled by the sendThrottledUser configuration key and 
	 * the throttlePeriod key (Update events may be throttled)
	 */
	action getDataUserChannel(string channel, string owner) returns string
	{
		return channel+":"+owner;
	}
	/**
	 * Get the raw channel for a scenario Id. This channel
	 * is enabled by the sendRawUser configuration key.
	 */
	action getRawUserChannel(string channel, string owner) returns string
	{
		return channel+":"+owner;
	}
	
	/**
	 * Merge configuration. Any entries in updates overwrite entries in configuration.
	 * An empty string value removes the value.
	 * @see ConfigureUpdates
	 */
	action mergeConfiguration(dictionary<string, string> updates, dictionary<string, string> configuration) {
		string k;
		for k in updates.keys() {
			configuration[k]:=updates[k];
			if updates[k]="" then {
				configuration.remove(k);
			}
		}
	}
	
	// implementation note: the defaults for sendThrottled, sendRaw, throttlePeriod, etc are in the following actions:
	/**
	 * Get the sendThrottled value from the given configuration
	 * @see ConfigureUpdates
	 */
	action getSendThrottled(dictionary<string,string> defaults, dictionary<string,string> config) returns boolean {
		string c:=getConfig(defaults, config, "sendThrottled", "true");
		return c = "true";
	}

	/**
	 * Get the sendThrottledUser value from the given configuration
	 * @see ConfigureUpdates
	 */
	action getSendThrottledUser(dictionary<string,string> defaults, dictionary<string,string> config) returns boolean {
		string c:=getConfig(defaults, config, "sendThrottledUser", "false");
		return c = "true";
	}
	
	/**
	 * Get the sendRaw value from the given configuration
	 * @see ConfigureUpdates
	 */
	action getSendRaw(dictionary<string,string> defaults, dictionary<string,string> config) returns boolean {
		string c:=getConfig(defaults, config, "sendRaw", "true");
		return c = "true";
	}
	
	/**
	 * Get the sendRawUser value from the given configuration
	 * @see ConfigureUpdates
	 */
	action getSendRawUser(dictionary<string,string> defaults, dictionary<string,string> config) returns boolean {
		string c:=getConfig(defaults, config, "sendRawUser", "false");
		return c = "true";
	}
	
	/**
	 * Get the throttlePeriod value from the given configuration
	 * @see ConfigureUpdates
	 */
	action getThrottlePeriod(dictionary<string,string> defaults, dictionary<string,string> config) returns float {
		string c:=getConfig(defaults, config, "throttlePeriod", "0.0");
		return float.parse(c);
	}
	
	/**
	 * Get the routeUpdate value from the given configuration
	 * @see ConfigureUpdates
	 */
	action getRouteUpdate(dictionary<string,string> defaults, dictionary<string,string> config) returns boolean {
		string c:=getConfig(defaults, config, "routeUpdate", "false");
		return c = "true";
	}
	

	/**
	 * Get the specified key from the configuration dictionaries, reverting to the default 
	 * if none specified.
	 */
	action getConfig(dictionary<string,string> defaults,
	                 dictionary<string,string> config, 
	                 string key, string _default) returns string {
		if config.hasKey(key) then {
			return config[key];
		}
		if defaults.hasKey(key) then {
			return defaults[key];
		}
		return _default;
	}

	action configurationManager(dictionary<string, string> defaultConfig, dictionary<string, dictionary<string, string> > configurations) {
		// the recognised ConfigureUpdates configuration keys, as above:
		dictionary<string,boolean> KNOWN_CONFIG_KEYS := {"sendThrottled":true, "throttlePeriod":false,
		 "sendRaw":true, "sendThrottledUser":true, "sendRawUser":true, "routeUpdate":true};

		dictionary <string,string> EMPTY_DICT:=new dictionary<string,string>;
		ConfigureUpdates cu;
		on all ConfigureUpdates():cu {
			string key;
			for key in cu.configuration.keys() {
				if not KNOWN_CONFIG_KEYS.hasKey(key) then {
					log "Unrecognized configuration property "+key+" in event "+cu.toString() at WARN;
				}
			}
			if cu.scenarioId = "" then {
				mergeConfiguration(cu.configuration, defaultConfig);
				log "Received "+cu.toString()+" : defaults applicable to all scenarios" at INFO;
			} else {
				if not configurations.hasKey(cu.scenarioId) then {
					configurations.add(cu.scenarioId, new dictionary<string,string>);
				}
				mergeConfiguration(cu.configuration, configurations[cu.scenarioId]);
				on ScenarioProcessedUpdates(cu.scenarioId) -> completed ConfigureUpdates(scenarioId=cu.scenarioId) {
					log "Received "+cu.toString()+" : applied to scenario" at INFO;
				}
				on completed ConfigureUpdates(scenarioId=cu.scenarioId) and not ScenarioProcessedUpdates(cu.scenarioId) {
					log "Received "+cu.toString()+" : for scenario that is not yet defined" at INFO;
				}
			}
		}

		GetConfiguration getConfig;
		on all GetConfiguration():getConfig {
			dictionary<string,string> config:=EMPTY_DICT;
			if configurations.hasKey(getConfig.scenarioId) then {
				config:=configurations[getConfig.scenarioId];
			}
			route Configuration(getConfig.scenarioId, defaultConfig, config);
		}	
		GetAllConfiguration gac;
		on all GetAllConfiguration():gac {
			route AllConfiguration(gac.requestId, defaultConfig, configurations);
		}
	}
		
}

event CallbackHelper {
	sequence<action<> > callbacks;
	action callback() {
		action<> c;
		for c in callbacks {
			c();
		}
	}
}



/**
 * Base event for tracking configuration for a given scenario
 */
event ScenarioServiceUpdaterBase {
	// these are internal and should not be set by users
	string scenarioId;
	dictionary<string,string> defaultConfig;
	dictionary<string,string> config;
	ScenarioServiceLibrary lib;
	boolean sendThrottled;
	boolean sendRaw;
	boolean sendAny;
	boolean emitAny;
	boolean sendThrottledUser;
	boolean sendRawUser;
	boolean routeUpdate;
	float throttlePeriod;
	float throttleStart;
	sequence<listener> listeners;
	string controlChannel;
	string rawChannel;
	string dataChannel;

	/**
	 * Called by monitor at onload time
	 * Will maintain configuration for this scenario
	 * @param sId the scenarioId
	 */
	action init(string sId, action<> cb_onUpdate) {
		scenarioId:=sId;
		controlChannel := lib.getControlChannel(scenarioId);
		rawChannel := lib.getRawChannel(scenarioId);
		dataChannel := lib.getDataChannel(scenarioId);
		route GetConfiguration(scenarioId);
		Configuration c;
		listener l:=on Configuration(scenarioId=scenarioId):c {
			config := c.configuration;
			defaultConfig := c.defaults;
			configurationUpdated();
			cb_onUpdate();
		}
		listeners.append(l);
	}

	/**
	 * Listen for further configuration changes.
	 * @param cb_onUpdate callback upon configuration having been updated
	 */	
	action listenToConfigureUpdates(action<> cb_onUpdate) {
		ConfigureUpdates cu;
		listener l:=on all ConfigureUpdates(scenarioId=""):cu or all ConfigureUpdates(scenarioId=scenarioId):cu {
			if(cu.scenarioId != "") then {
				route ScenarioProcessedUpdates(cu.scenarioId);
			}
			onConfigureUpdates(cu);
			cb_onUpdate();
		}
		listeners.append(l);
	}

	/**
	 * Called when new ConfigureUpdates event available
	 * @param sId the scenarioId
	 */
	action onConfigureUpdates(ConfigureUpdates cu) {
		if cu.scenarioId = "" then {
			lib.mergeConfiguration(cu.configuration, defaultConfig);
		} else {
			lib.mergeConfiguration(cu.configuration, config);
		}
		configurationUpdated();
	}
	
	/**
	 * Called when the configuration should be re-parsed.
	 * @param sId the scenarioId
	 */
	action configurationUpdated() {
		sendThrottled:=lib.getSendThrottled(defaultConfig, config);
		sendThrottledUser:=lib.getSendThrottledUser(defaultConfig, config);
		if sendThrottled or sendThrottledUser then {
			throttlePeriod:=lib.getThrottlePeriod(defaultConfig, config);
			if(throttlePeriod < 0.0) then {
				sendThrottled:=false;
				sendThrottledUser:=false;
			}
			throttleStart:=currentTime;
		}
		sendRaw:=lib.getSendRaw(defaultConfig, config);
		sendRawUser:=lib.getSendRawUser(defaultConfig, config);
		routeUpdate:=lib.getRouteUpdate(defaultConfig, config);
		sendAny:= sendRaw or sendThrottled or routeUpdate or sendRawUser or sendThrottledUser;
		emitAny:= sendRaw or sendThrottled or sendRawUser or sendThrottledUser;
	}

	/** 
	 * Get whether this scenario should send on the throttled (Data) channel
	 */
	action isSendThrottled() returns boolean {
		return sendThrottled or sendThrottledUser;
	}

	/** 
	 * Get whether this scenario should send on the raw channel
	 */
	action isSendRaw() returns boolean {
		return sendRaw or sendRawUser;
	}

	action doEmit(string emitted, string owner) {
		if sendRaw then {
			emit emitted to rawChannel;
		}
		if sendThrottled then {
			emit emitted to dataChannel;
		}
		if sendRawUser then {
			emit emitted to lib.getRawUserChannel(rawChannel, owner);
		}
		if sendThrottledUser then {
			emit emitted to lib.getDataUserChannel(dataChannel, owner);
		}
	}

	/**
 	 * Kill any listeners this object has started
	 */
	action destroy() {
		listener l;
		for l in listeners {
			l.quit();
		}
	}
}


/**
 * Utility event for tracking configuration for a given scenario.
 * This event is suitable for use by monitors which spawn per instance
 * (e.g. Scenarios)
 * 
 * This event also uses a callback to get the updates (supplied in
 * instanceInit). If the scenario is configured to only send throttled 
 * updates, the callback is only called when the throttling period 
 * determines an update should be sent - thus, the scenario does not
 * need to generate the output sequence<string> except when needed,
 * which can improve performance in such a configuration.
 * 
 * actions starting with an underscore should be considered private 
 * and not called by users of this event.
 */
event ScenarioServiceUpdaterSingleInstance {
	// these are internal and should not be set by users
	string scenarioId;
	integer scenarioInstanceId;
	ScenarioServiceLibrary lib;
	listener throttlingListener;
	ScenarioServiceUpdaterBase base;
	boolean havePending;
	float latestUpdate;
	action<> returns sequence<string> getUpdate;
	Update update;
	boolean needUpdate;
	context mainContext;
	string owner;
	string controlChannel;
	string rawChannel;
	string dataChannel;

	/**
	 * Called by monitor at onload time
	 * Will maintain configuration for this scenario
	 */
	action init(string sId) {
		init_cb(sId, _noopAction);
	}
	
	/**
	 * Called by monitor at onload time
	 * Will maintain configuration for this scenario.
	 * cb_init is called when initialisation is complete
	 */
	action init_cb(string sId, action<> cb_init) {
		base.init(sId, cb_init);
		scenarioId:=sId;
		controlChannel := lib.getControlChannel(scenarioId);
		rawChannel := lib.getRawChannel(scenarioId);
		dataChannel := lib.getDataChannel(scenarioId);
		base.listenToConfigureUpdates(_noopAction);
		mainContext:=context.current();
	}

	action _noopAction() {
	}
	

	/**
	 * Called by monitor after spawn
	 * Will maintain configuration for this scenario, and update listeners appropriately.
	 * @param getUpdateCallback a callback to get the latest outputFieldValues - may be called at any time, must 
         *        always return a consistent set of outputs
	 */
	action instanceInit(integer id, action<> returns sequence<string> getUpdateCallback, string _owner) {
		scenarioInstanceId:=id;
		owner:=_owner;
		getUpdate:=getUpdateCallback;
		update.scenarioId:=scenarioId;
		update.scenarioInstanceId:=scenarioInstanceId;
		base.listenToConfigureUpdates(_configurationUpdated);
		_configurationUpdated();
		listener l:=on all SendQueuedUpdatesNow() {
			_sendThrottledUpdate();
		}
		base.listeners.append(l);
	}

	/** 
	 * Get whether this scenario should route updates
	 */
	action isRouteUpdate() returns boolean {
		return base.routeUpdate;
	}

	/**
 	 * Kill any listeners this object has started
	 */
	action destroy() {
		base.destroy();
		throttlingListener.quit();
	}
	  
	/**
	 * called in instances (not factories) when configuration has changed
	 */
	action _configurationUpdated() {
		if base.isSendThrottled() then {
			throttlingListener.quit();
			flushPending();
			if(base.throttlePeriod >= 0.0) then {
				if havePending then {
					_setupThrottleListener();
				}
			}
		}
	}

	/**
	 * set up a throttled listener according to throttlePeriod control
	 */
	action _setupThrottleListener() {
		if base.throttlePeriod > 0.0 then {
			float offset:=currentTime-base.throttleStart;
			float t:=((offset/base.throttlePeriod).floor()+1).toFloat();
			throttlingListener:=on wait((t*base.throttlePeriod)-offset) {
				_sendThrottledUpdate();
			}
		} else {
			_sendThrottledUpdate();
		}
	}

	/**
	 * actually send an update on the throttled channel
	 */
	action _sendThrottledUpdate() {
		if not havePending then {
			return;
		}
		if needUpdate then {
			update.outputFieldValues:=getUpdate();
			update.timeStamp:=latestUpdate;
			needUpdate:=false;
		}
		if base.sendThrottled then {
			send update to dataChannel;
		}
		if base.sendThrottledUser then {
			send update to lib.getDataUserChannel(dataChannel, owner);
		}
		havePending:=false;
	}

	/**
	 * flush any pending update on the throttled channel.
	 */
	action flushPending() {
		if havePending then {
			_sendThrottledUpdate();
			throttlingListener.quit();
		}
	}	

	/**
 	 * Called when a new update is available.
	 */
	action newUpdateAvailable() {
		needUpdate:=true;
		latestUpdate:=currentTime;
 		if base.sendRaw or base.routeUpdate then {
			if needUpdate then {
				update.outputFieldValues:=getUpdate();
				update.timeStamp:=currentTime;
				needUpdate:=false;
			}
			if base.sendRaw then {
				send update to rawChannel;
			}
			if base.sendRawUser then {
				send update to lib.getRawUserChannel(rawChannel, owner);
			}
			if base.routeUpdate then {
				route update;
			}
		}
		if base.isSendThrottled() and not havePending then {
			havePending:=true;
			_setupThrottleListener();
		}
	}
	
	/**
 	 * Called to send a new Acknowledgement event.
	 */
	action emitAcknowledgement(integer messageId) {
		Acknowledge ack:=Acknowledge(scenarioId, messageId, scenarioInstanceId, true, getUpdate());
		if base.routeUpdate then {
			route ack;
		}
		if base.emitAny then {
			send ack to controlChannel;
		}
	}
				
	/**
 	 * Called to send a new Nak event.
	 */
	action emitNack(integer messageId) {
		Acknowledge nack:=Acknowledge(scenarioId, messageId, scenarioInstanceId, false, new sequence<string>);
		if base.routeUpdate then {
			route nack;
		}
		if base.emitAny then {
			send nack to controlChannel;
		}
	}
	
	/**
 	 * Called to send a new Created event.
	 */
	action emitCreated(integer messageId, string owner, string state, sequence<string> inputVariables, sequence<string> outputVariables) {
		if base.sendAny then {
			Created created:=Created(scenarioId, messageId, scenarioInstanceId, owner, state, inputVariables, outputVariables);
			route created;
			if base.emitAny then {
				base.doEmit(created.toString(), owner);
			}
		}
	}
	
	/**
 	 * Called to send a new Edited event.
	 */
	action emitEdited(integer messageId, sequence<string> inputVariables) {
		flushPending();
		if base.sendAny then {
			Edited edited:=Edited(scenarioId, messageId, scenarioInstanceId, inputVariables, getUpdate());
			if base.routeUpdate then {
				route edited;
			}
			if base.emitAny then {
				base.doEmit(edited.toString(), owner);
			}
		}
		emitOpCompleted(messageId);
	}

	/**
 	 * Called to say an operation has completed; implied by emitEdited. Must be called before emitInstanceDied.
	 */
	action emitOpCompleted(integer messageId) {
		if(context.current().getId()!=mainContext.getId()) then {
			send OperationCompleted(scenarioId, scenarioInstanceId, messageId) to mainContext;
		}
	}

	/**
 	 * Called to send a new Deleted event.
	 */
	action emitDeleted(integer messageId) {
		flushPending();
		if base.sendAny then {
			Deleted deleted:=Deleted(scenarioId, messageId, scenarioInstanceId);
			if base.routeUpdate then {
				route deleted;
			}
			if base.emitAny then {
				base.doEmit(deleted.toString(), owner);
			}
		}
		throttlingListener.quit();
		notifyInstanceDied();
	}

	/**
 	 * Called to send an instance Died event (either of failed, ended)
	 */
	action emitInstanceDied() {
		flushPending();
		if base.sendAny then {
			InstanceDied iDied:=InstanceDied(scenarioId, scenarioInstanceId);
			if base.emitAny then {
				base.doEmit(iDied.toString(), owner);
			}
			if mainContext.getId()=context.current().getId() then {
				route iDied;
			}
		}
		throttlingListener.quit();
	}

	/**
 	 * Called to notify the scenario service monitor that we have gone away - must be called 
 	 * after emitting any events regarding this instance going away. Implied by emitDeleted.
	 */
	action notifyInstanceDied() {
		if mainContext.getId()!=context.current().getId() then {
			InstanceDied iDied:=InstanceDied(scenarioId, scenarioInstanceId);
			
			// give a chance for anyone monitoring this from its own context 
			// to handle the InstanceDied before the main context
			if base.routeUpdate then {
				route InstanceDied(scenarioId, scenarioInstanceId);
			}

			send iDied to mainContext;
		}
	}
	
	/**
 	 * Called to send a new StateChange event.
	 */
	action emitStateChange(string state) {
		flushPending();
		StateChange stchange:=StateChange(scenarioId, scenarioInstanceId, state);
		if mainContext.getId()!=context.current().getId() then {
			send stchange to mainContext;
		} else {
			route stchange;
		}
		if base.emitAny then {
			base.doEmit(stchange.toString(), owner);
		}
	}

	/**
 	 * Called to send an instance in response to a RequestInstancesInternal event
	 */
	action emitInstance(RequestInstancesInternal request, string owner, string state, sequence<string> input, sequence<string> output) {
		Instance instance:=Instance(scenarioId, request.messageId, scenarioInstanceId, owner, state, input, output);
		if request.internal then {
			if mainContext.getId() != context.current().getId() then {
				send instance to mainContext;
			} else {
				route instance;
			}
		} else {
			if(base.emitAny) then {
				send instance to request.channel;
			}
		}
		if mainContext.getId() != context.current().getId() then {
			send RequestInstancesParallelDone(scenarioId, request.messageId, scenarioInstanceId) to mainContext;
		}
	}

	/**
 	 * Called when the instance has failed.  input and output are the input
 	 * and output variables as at the last Update/ Edited point.
	 */
	action finished(string state, string owner, sequence<string> input, sequence<string> output) {
		ScenarioFinished scenFinished:=ScenarioFinished(scenarioId, scenarioInstanceId, owner, state, input, output);
		if mainContext.getId() != context.current().getId() then {
			send scenFinished to mainContext;
		} else {
			route scenFinished;
		}
	}
	
}


/**
 * Utilitiy event for tracking configuration for a given scenario.
 * This event is suitable for use by monitors which spawn per scenario,
 * but not per instance. (e.g. ScenarioService, DataViewService)
 * 
 * actions starting with an underscore should be considered private 
 * and not called by users of this event.
 * 
 * Note that this updater only honours sendThrottledUser changes at the next throttling period
 * (it does not record the owner if sendThrottledUser is not true)
 */
event ScenarioServiceUpdaterMultipleInstances {
	// these are internal and should not be set by users
	string scenarioId;
	ScenarioServiceLibrary lib;
	listener throttlingListener;
	ScenarioServiceUpdaterBase base;
	boolean currentlySendingThrottledUser;
	dictionary<integer, Update> updates;
	dictionary<integer, string> owners;
	context mainContext;
	string controlChannel;
	string rawChannel;
	string dataChannel;

	/**
	 * Called by monitor for each scenario
	 * Will maintain configuration for this scenario Id
	 */
	action init(string sId, context mainCtx) {
		_init(sId, mainCtx, _configurationUpdated);
	}
	
	/**
	 * Called by monitor for each scenario
	 * Will maintain configuration for this scenario Id
	 */
	action init_cb(string sId, context mainCtx, action<> cb_initComplete) {
		CallbackHelper callbackHelper:=new CallbackHelper;
		callbackHelper.callbacks.append(cb_initComplete);
		callbackHelper.callbacks.append(_configurationUpdated);
		_init(sId, mainCtx, callbackHelper.callback);
	}

	/**
	 * Implementation of init and init_cb
	 */
	action _init(string sId, context mainCtx, action<> cb_initComplete) {
		base.init(sId, cb_initComplete);
		mainContext:=mainCtx;
		scenarioId:=sId;
		controlChannel := lib.getControlChannel(scenarioId);
		rawChannel := lib.getRawChannel(scenarioId);
		dataChannel := lib.getDataChannel(scenarioId);
		listener l:=on all SendQueuedUpdatesNow() {
			_sendThrottledUpdates();
		}
		base.listeners.append(l);
		base.listenToConfigureUpdates(_configurationUpdated);
	}
	
	/** 
	 * Get whether this scenario should route updates
	 */
	action isRouteUpdate() returns boolean {
		return base.routeUpdate;
	}

	/**
 	 * Kill any listeners this object has started
	 */
	action destroy() {
		base.destroy();
		throttlingListener.quit();
	}
	  

	/**
	 * called in instances (not factories) when configuration has changed
	 */
	action _configurationUpdated() {
		if base.isSendThrottled() then {
			throttlingListener.quit();
			flushPending();
			if(base.throttlePeriod >= 0.0) then {
				_setupThrottleListener();
			}
		}
	}

	/**
	 * set up a throttled listener according to throttlePeriod control
	 */
	action _setupThrottleListener() {
		if base.throttlePeriod > 0.0 then {
			throttlingListener:=on all wait(base.throttlePeriod) {
				_sendThrottledUpdates();
			}
		} else {
			_sendThrottledUpdates();
		}
	}

	/**
	 * actually send an update on the throttled channel
	 */
	action _sendThrottledUpdates() {
		integer instance;
		if base.sendThrottled then {
			for instance in updates.keys() {
				send updates[instance] to dataChannel;
			}
		}
		if currentlySendingThrottledUser then {
			for instance in updates.keys() {
				send updates[instance] to lib.getDataUserChannel(dataChannel, owners[instance]);
			}
		}
		owners.clear();
		updates.clear();
		currentlySendingThrottledUser := base.sendThrottledUser;
	}

	/**
	 * flush any pending update on the throttled channel.
	 */
	action flushPending() {
		_sendThrottledUpdates();
	}	

	/**
 	 * Flush pending throttled data for one instance
	 */
	action flushPendingInstance(integer scenarioInstanceId) {
		if updates.hasKey(scenarioInstanceId) then {
			if base.sendThrottled then {
				send updates[scenarioInstanceId] to dataChannel;
			}
			if currentlySendingThrottledUser then {
				send updates[scenarioInstanceId] to lib.getDataUserChannel(dataChannel, owners[scenarioInstanceId]);
			}
			updates.remove(scenarioInstanceId);
			if owners.hasKey(scenarioInstanceId) then {
				owners.remove(scenarioInstanceId);
			}
		}
	}	

	/**
 	 * Called when a new update is available. (This variant allows setting of the time parameter)
	 */
	action emitUpdate_time(integer scenarioInstanceId, float time, sequence<string> output, string owner) {
		if base.sendAny then {
			Update update:=Update(scenarioId, scenarioInstanceId, time, output);
			if base.sendRaw then {
				send update to rawChannel;
			}
			if base.sendRawUser then {
				send update to lib.getRawUserChannel(rawChannel, owner);
			}
			if base.routeUpdate then {
				route update;
			}
			if base.isSendThrottled() then {
				if base.throttlePeriod > 0.0 then {
					updates.add(scenarioInstanceId, update);
					if currentlySendingThrottledUser then {
						owners.add(scenarioInstanceId, owner);
					}
				} else {
					send update to dataChannel;
				}
			}
		}
	}
	
	/**
 	 * Called when a new update is available.
	 */
	action emitUpdate(integer scenarioInstanceId, sequence<string> output, string owner) {
		emitUpdate_time(scenarioInstanceId, currentTime, output, owner);
	}

	/**
 	 * Called to send a new Acknowledgement event.
	 */
	action emitAcknowledgement(integer messageId, integer scenarioInstanceId, sequence<string> output) {
		Acknowledge ack:=Acknowledge(scenarioId, messageId, scenarioInstanceId, true, output);
		if base.routeUpdate then {
			route ack;
		}
		if base.emitAny then {
			send ack to controlChannel;
		}
	}
				
	/**
 	 * Called to send a new Nak event.
	 */
	action emitNack(integer messageId, integer scenarioInstanceId) {
		Acknowledge nack:=Acknowledge(scenarioId, messageId, scenarioInstanceId, false, new sequence<string>);
		if base.routeUpdate then {
			route nack;
		}
		if base.emitAny then {
			send nack to controlChannel;
		}
	}
	
	/**
 	 * Called to send a new Created event.
	 */
	action emitCreated(integer messageId, integer scenarioInstanceId, string owner, string state, sequence<string> inputVariables, sequence<string> outputVariables) {
		if base.sendAny then {
			Created created:=Created(scenarioId, messageId, scenarioInstanceId, owner, state, inputVariables, outputVariables);
			route created;
			if base.emitAny then {
				string sCreated:=created.toString();
				base.doEmit(sCreated, owner);
			}
		}
	}

	/**
 	 * Called to send any received events (except for instance) for this scenario.
	 */
	action emitReceivedEvents() {
		dictionary<integer, string> instanceOwners:=new dictionary<integer, string>;
		{
			Update update;
			listener l:=on all Update(scenarioId = scenarioId):update {
				if base.sendRaw then {
					send update to rawChannel;
				}
				if base.sendRawUser and instanceOwners.hasKey(update.scenarioInstanceId) then {	
					send update to lib.getRawUserChannel(rawChannel, instanceOwners[update.scenarioInstanceId]);
				}
				if base.isSendThrottled() then {
					if base.throttlePeriod > 0.0 then {
						updates.add(update.scenarioInstanceId, update);
						string owner:="*";
						if instanceOwners.hasKey(update.scenarioInstanceId) then {
							owner:=instanceOwners[update.scenarioInstanceId];
						}
						if currentlySendingThrottledUser then {
							owners.add(update.scenarioInstanceId, owner);
						}
					} else {
						send update to dataChannel;
					}
				}
			}
			base.listeners.append(l);
		}
		{
			Created created;
			listener l:=on all Created(scenarioId = scenarioId):created {
				if(created.owner != "*") then {
					instanceOwners.add(created.scenarioInstanceId, created.owner);
				}
				base.doEmit(created.toString(), created.owner);				
			}
			base.listeners.append(l);
		}
		{
			Deleted deleted;
			listener l:=on all Deleted(scenarioId = scenarioId):deleted {
				flushPendingInstance(deleted.scenarioInstanceId);
				string owner:="*";
				if instanceOwners.hasKey(deleted.scenarioInstanceId) then {
					owner:=instanceOwners[deleted.scenarioInstanceId];
					instanceOwners.remove(deleted.scenarioInstanceId);
				}
				base.doEmit(deleted.toString(), owner);		
			}
			base.listeners.append(l);
		}
		{
			Edited edited;
			listener l:=on all Edited(scenarioId = scenarioId):edited {
				flushPendingInstance(edited.scenarioInstanceId);
				string owner:="*";
				if instanceOwners.hasKey(edited.scenarioInstanceId) then {
					owner:=instanceOwners[edited.scenarioInstanceId];
				}
				base.doEmit(edited.toString(), owner);		
			}
			base.listeners.append(l);
		}
		{
			InstanceDied instanceDied;
			listener l:=on all InstanceDied(scenarioId = scenarioId):instanceDied {
				flushPendingInstance(instanceDied.scenarioInstanceId);
				string owner:="*";
				if instanceOwners.hasKey(instanceDied.scenarioInstanceId) then {
					owner:=instanceOwners[instanceDied.scenarioInstanceId];
				}
				base.doEmit(instanceDied.toString(), owner);		
			}
			base.listeners.append(l);
		}
	}
	
	/**
 	 * Called to send a new Edited event.
	 */
	action emitEdited(integer messageId, integer scenarioInstanceId, sequence<string> inputVariables, sequence<string> outputVariables, string owner) {
		flushPendingInstance(scenarioInstanceId);
		if base.sendAny then {
			Edited edited:=Edited(scenarioId, messageId, scenarioInstanceId, inputVariables, outputVariables);
			if base.routeUpdate then {
				route edited;
			}
			if base.emitAny then {
				base.doEmit(edited.toString(), owner);
			}
		}
		emitOpCompleted(messageId, scenarioInstanceId);
	}

	/**
 	 * Called to say an operation has completed; implied by emitEdited. Must be called before emitInstanceDied.
	 */
	action emitOpCompleted(integer messageId, integer scenarioInstanceId) {
		if(context.current().getId()!=mainContext.getId()) then {
			send OperationCompleted(scenarioId, scenarioInstanceId, messageId) to mainContext;
		}
	}

	/**
 	 * Called to send a new Deleted event.
	 */
	action emitDeleted(integer messageId, integer scenarioInstanceId, string owner) {
		flushPendingInstance(scenarioInstanceId);
		if base.sendAny then {
			Deleted deleted:=Deleted(scenarioId, messageId, scenarioInstanceId);
			if base.routeUpdate then {
				route deleted;
			}
			if base.emitAny then {
				base.doEmit(deleted.toString(), owner);
			}
		}
		notifyInstanceDied(scenarioInstanceId);
	}

	/**
 	 * Called to notify the scenario service monitor that we have gone away - must be called 
 	 * after emitting any events regarding this instance going away. Implied by emitDeleted.
	 */
	action notifyInstanceDied(integer scenarioInstanceId) {
		if mainContext.getId()!=context.current().getId() then {
			InstanceDied iDied:=InstanceDied(scenarioId, scenarioInstanceId);
			send iDied to mainContext;
		}
	}
	

	
	/**
 	 * Called to send an instance Died event (either of failed, ended)
	 */
	action emitInstanceDied(integer scenarioInstanceId, string owner) {
		flushPendingInstance(scenarioInstanceId);
		if base.sendAny then {
			InstanceDied iDied:=InstanceDied(scenarioId, scenarioInstanceId);
			if base.emitAny then {
				base.doEmit(iDied.toString(), owner);
			}
			if mainContext.getId()=context.current().getId() then {
				route iDied;
			}
		}
	}
	
	/**
 	 * Called to send a new StateChange event.
	 */
	action emitStateChange(string state, integer scenarioInstanceId, string owner) {
		flushPendingInstance(scenarioInstanceId);
		StateChange stchange:=StateChange(scenarioId, scenarioInstanceId, state);
		if mainContext.getId()!=context.current().getId() then {
			send stchange to mainContext;
		} else {
			route stchange;
		}
		if base.emitAny then {
			base.doEmit(stchange.toString(), owner);
		}
	}

	/**
 	 * Called to send an instance in response to a RequestInstancesInternal event
	 */
	action emitInstance(RequestInstancesInternal request, integer scenarioInstanceId, string owner, string state, sequence<string> input, sequence<string> output) {
		Instance instance:=Instance(scenarioId, request.messageId, scenarioInstanceId, owner, state, input, output);
		if request.internal then {
			if mainContext.getId() != context.current().getId() then {
				send instance to mainContext;
			} else {
				route instance;
			}
		} else {
			if(base.emitAny) then {
				send instance to request.channel;
			}
		}
		if mainContext.getId() != context.current().getId() then {
				send RequestInstancesParallelDone(scenarioId, request.messageId, scenarioInstanceId) to mainContext;
		}
	}
	/**
 	 * Called to send an instance that has already been seen in the current context
	 */
	action emitReceivedInstance(RequestInstancesInternal request, Instance instance) {
		if request.internal then {
			if mainContext.getId() != context.current().getId() then {
				send instance to mainContext;
			}
		} else {
			if(base.emitAny) then {
				send instance to request.channel;
			}
		}
	}
	
}


/*
 * Monitor that performs the following tasks:
 *   - routing/emitting ScenarioServiceLoaded when the service is loaded
 *   - routing/emitting ScenarioServiceUnloaded when the service is unloaded
 *   - routing/emitting a nack if a Create request is ignored
 *   - maintains latest configuration
 *   - tracks state of ended/ failed scenarios
 *   - forwards Edit, delete events to scenarios in other contexts
 *
 */
monitor ScenarioService {

	event PendingOperation {
		integer messageId;
		integer type; // 0 = delete, 1 = edit
	}

	listener throttledSenderListener;
	ScenarioServiceLibrary lib;
	RequestScenarios requestScenarios;
	boolean requestingScenarios;
	
	// MetaData relating to the interface
	dictionary<string,string> interfaceMetaData := {
		"interface.package"     :"com.apama.scenario",
		"interface.name"        :"ScenarioService",
		"interface.fileName"    :"ScenarioService.mon",
		"interface.vendor"      :"Apama",
		"interface.version"     :"9.10.0.3.284318",
		"interface.fullVersion" :"rel/9.10.0.x@284318",
		"interface.language"    :"MonitorScript"
	};
	
	// Channel names	
	string scenarioServiceChannel := "com.apama.scenario";
	integer highestInstanceId;


	// the first mThread handles scenario discovery and maps some events to internal events
	action onload() {
		// print version
		log "ScenarioService interface loaded. MetaData: "+interfaceMetaData.toString() at INFO;
		
		// generate the ScenarioServiceLoaded event
		route ScenarioServiceLoaded();
		send ScenarioServiceLoaded() to scenarioServiceChannel;

		dictionary <string,string> defaultConfig:=new dictionary<string,string>;
		dictionary <string, dictionary<string,string> > configurations:=new dictionary<string, dictionary<string,string> >;
		lib.configurationManager(defaultConfig, configurations);
		RequestInstancesOnChannel requestInstancesOnChannel;
		on all RequestInstancesOnChannel():requestInstancesOnChannel {
			route RequestInstancesInternal(requestInstancesOnChannel.scenarioId, requestInstancesOnChannel.messageId, requestInstancesOnChannel.channel, false, "", false);
			on RequestInstancesDone(scenarioId=requestInstancesOnChannel.scenarioId, messageId=requestInstancesOnChannel.messageId) {
				send RequestInstancesDone(requestInstancesOnChannel.scenarioId, requestInstancesOnChannel.messageId) to requestInstancesOnChannel.channel;
			}
		}

		RequestInstancesOnChannelByUser requestInstancesOnChannelByUser;
		on all RequestInstancesOnChannelByUser():requestInstancesOnChannelByUser {
			route RequestInstancesInternal(requestInstancesOnChannelByUser.scenarioId, requestInstancesOnChannelByUser.messageId, requestInstancesOnChannelByUser.channel, false, requestInstancesOnChannelByUser.owner, true);
			on RequestInstancesDone(scenarioId=requestInstancesOnChannelByUser.scenarioId, messageId=requestInstancesOnChannelByUser.messageId) {
				send RequestInstancesDone(requestInstancesOnChannelByUser.scenarioId, requestInstancesOnChannelByUser.messageId) to requestInstancesOnChannelByUser.channel;
			}
		}
		Scenario scenario;
		on all Scenario():scenario {
			if requestingScenarios then {
				send scenario to requestScenarios.channel;
			} else {
				spawn trackScenario(scenario);
			}
		}
		
		ScenarioUnloaded scenarioUnloaded;
		on all ScenarioUnloaded(): scenarioUnloaded {
			send scenarioUnloaded to scenarioServiceChannel;
		}

		
		on all RequestScenarios():requestScenarios {
			send RequestScenariosAck() to requestScenarios.channel;
			route StartScenarioRecovery();
			route FinishedScenarioRecovery();
			requestingScenarios:=true;
			on FinishedScenarioRecovery() {
				requestingScenarios:=false;
				send RequestScenariosDone() to requestScenarios.channel;
			}
		}

		dictionary <string,string> EMPTY_DICT:=new dictionary<string,string>;
		// Pick up any requests for operations on invalid scenarioIds
		Create create;
		on all unmatched Create(): create {
			dictionary<string,string> config:=EMPTY_DICT;
			if configurations.hasKey(create.scenarioId) then {
				config:=configurations[create.scenarioId];
			}
			Acknowledge nack := new Acknowledge;
			nack.scenarioId := create.scenarioId;
			nack.messageId := create.messageId;
			nack.success := false;
			log create.scenarioId+": Scenario create ignored - unknown scenarioId." at WARN;
			send nack to lib.getControlChannel(create.scenarioId);
			if lib.getRouteUpdate(defaultConfig, config) then {
				route nack;
			}
		}

		// The old SetThrottlingPeriod is mapped to a ConfigureUpdates event
		SetThrottlingPeriod setThrottlingPeriod;		
		on all SetThrottlingPeriod():setThrottlingPeriod {
			dictionary<string, string> configChanges:=new dictionary<string,string>;
			if setThrottlingPeriod.period >= 0.0 then {
				configChanges["sendThrottled"]:="true";
				configChanges["throttlePeriod"]:=setThrottlingPeriod.period.toString();
			} else {
				configChanges["sendThrottled"]:="false";
			}
			ConfigureUpdates cu:=ConfigureUpdates("", configChanges);
			log "Received deprecated event "+setThrottlingPeriod.toString()+", will re-route as "+cu.toString() at WARN;
			route cu;
		}
	}		
				
	
	action onunload() {
		// generate the ScenarioServiceUnloaded event
		ScenarioServiceUnloaded unloaded := new ScenarioServiceUnloaded;
		route unloaded;
		send unloaded to scenarioServiceChannel;
	}
	
	/**
	 * spawned per scenario, and handles any finished scenario instances.
	 * For parallel scenarios, it also tracks which instance runs in 
	 * which context and forwards Edit and Delete events.
	 */
	action trackScenario(Scenario scenario) {
		on ScenarioUnloaded(scenarioId=scenario.scenarioId) {
			die;
		}
		ScenarioServiceUpdaterMultipleInstances updater:=new ScenarioServiceUpdaterMultipleInstances;
		updater.init(scenario.scenarioId, context.current());
		ScenarioFinished finished;
		on all ScenarioFinished(scenarioId=scenario.scenarioId):finished {
			// Finished scenarios can be deleted (which terminates all listeners for this instance), 
			// discovered, and edits are Nacked 
			Delete delete;
			on Delete(scenarioId=scenario.scenarioId, scenarioInstanceId = finished.scenarioInstanceId):delete {
				updater.emitAcknowledgement(delete.messageId, finished.scenarioInstanceId, finished.outputFieldValues);
				updater.emitDeleted(delete.messageId, finished.scenarioInstanceId, finished.owner);
			}
			Edit edit;
			on all Edit(scenarioId=scenario.scenarioId, scenarioInstanceId=finished.scenarioInstanceId):edit and not 
			            Delete(scenarioId=scenario.scenarioId, scenarioInstanceId = finished.scenarioInstanceId) {
				updater.emitNack(edit.messageId, finished.scenarioInstanceId);
				log scenario.displayName+"("+finished.scenarioInstanceId.toString()+ "): Scenario edit ignored - Scenario is in "+finished.state+" state." at WARN;
			}
			RequestInstancesInternal requestInstances;
			if finished.owner = "*" then {
				on all RequestInstancesInternal(scenarioId=scenario.scenarioId):requestInstances and not
						Delete(scenarioId=scenario.scenarioId, scenarioInstanceId = finished.scenarioInstanceId) {
					updater.emitInstance(requestInstances, finished.scenarioInstanceId,
						finished.owner, finished.state, finished.inputFieldValues, finished.outputFieldValues);
				}
			} else {
				on all (RequestInstancesInternal(scenarioId=scenario.scenarioId, ownerFilter=false):requestInstances or
					RequestInstancesInternal(scenarioId=scenario.scenarioId, owner=finished.owner, ownerFilter=true):requestInstances) and not
						Delete(scenarioId=scenario.scenarioId, scenarioInstanceId = finished.scenarioInstanceId) {
					updater.emitInstance(requestInstances, finished.scenarioInstanceId,
						finished.owner, finished.state, finished.inputFieldValues, finished.outputFieldValues);
				}
			}
		}
		if scenario.executionMode > 0 then {
			// for parallel scenarios, we keep track of instance to context mapping:
			dictionary<integer, context> runningCtxs := new dictionary<integer, context>;
			// and to owner mapping:
			dictionary<integer, string> ownerCtxs := new dictionary<integer, string>;
			// and by user:
			dictionary<string, dictionary<integer, context> > runningCtxsByOwner := new dictionary<string, dictionary<integer, context> >;
			// maps from instanceId to sequence<messageId>
			dictionary<integer, sequence<PendingOperation> > pendingOperations := new dictionary<integer, sequence<PendingOperation> >;
			ParallelStarting starting;
			on all ParallelStarting(scenarioId = scenario.scenarioId):starting {
				runningCtxs.add(starting.scenarioInstanceId, starting.runningCtx);
				ownerCtxs.add(starting.scenarioInstanceId, starting.owner);
				if not runningCtxsByOwner.hasKey(starting.owner) then {
					runningCtxsByOwner.add(starting.owner, new dictionary<integer, context>);
				}
				runningCtxsByOwner[starting.owner].add(starting.scenarioInstanceId, starting.runningCtx);
				highestInstanceId := starting.scenarioInstanceId;
			}
			
			InstanceDied died;
			on all InstanceDied(scenarioId = scenario.scenarioId):died {
				if runningCtxs.hasKey(died.scenarioInstanceId) then {
					runningCtxs.remove(died.scenarioInstanceId);
				}
				if ownerCtxs.hasKey(died.scenarioInstanceId) then {
					string owner:=ownerCtxs[died.scenarioInstanceId];
					ownerCtxs.remove(died.scenarioInstanceId);
					if runningCtxsByOwner.hasKey(owner) then {
						if runningCtxsByOwner[owner].hasKey(died.scenarioInstanceId) then {
							runningCtxsByOwner[owner].remove(died.scenarioInstanceId);
						}
						if runningCtxsByOwner[owner].size()=0 then {
							runningCtxsByOwner.remove(owner);
						}
					}
				}
				if pendingOperations.hasKey(died.scenarioInstanceId) then {
					PendingOperation pending;
					for pending in pendingOperations[died.scenarioInstanceId] {
						if pending.type = 0 then {
							route Delete(scenario.scenarioId, pending.messageId, died.scenarioInstanceId);
						} else {
							if pending.type = 1 then {
								route Edit(scenario.scenarioId, pending.messageId, died.scenarioInstanceId, new sequence<string>);
							} else {
								log "error: unknown pending operation type "+pending.toString() at ERROR;
							}
						}
					}
					pendingOperations.remove(died.scenarioInstanceId);
				}
			}
			
			// and forward edits, deletes:
			Edit edit;
			on all unmatched Edit(scenarioId = scenario.scenarioId):edit {
				if runningCtxs.hasKey(edit.scenarioInstanceId) then {
					send edit to runningCtxs[edit.scenarioInstanceId];
					addPendingOperation(pendingOperations, edit.scenarioInstanceId, edit.messageId, 1);
					on OperationCompleted(scenarioId = scenario.scenarioId, scenarioInstanceId = edit.scenarioInstanceId, messageId = edit.messageId) and not
					   ScenarioFinished(scenarioId = scenario.scenarioId, scenarioInstanceId = edit.scenarioInstanceId){
						removePendingOperation(pendingOperations, edit.scenarioInstanceId, edit.messageId);
					}
				} else {
					log scenario.displayName+"("+edit.scenarioInstanceId.toString()+"): Scenario edit ignored - unknown scenarioInstanceId." at WARN;
					updater.emitNack(edit.messageId, edit.scenarioInstanceId);
				}
			}
			Delete delete;
			on all unmatched Delete(scenarioId = scenario.scenarioId):delete {
				if runningCtxs.hasKey(delete.scenarioInstanceId) then {
					send delete to runningCtxs[delete.scenarioInstanceId];
					addPendingOperation(pendingOperations, delete.scenarioInstanceId, delete.messageId, 0);
					on OperationCompleted(scenarioId = scenario.scenarioId, scenarioInstanceId = delete.scenarioInstanceId, messageId = delete.messageId) and not
					   ScenarioFinished(scenarioId = scenario.scenarioId, scenarioInstanceId = delete.scenarioInstanceId){
						removePendingOperation(pendingOperations, delete.scenarioInstanceId, delete.messageId);
					}
				} else {
					log scenario.displayName+"("+delete.scenarioInstanceId.toString()+"): Scenario delete ignored - unknown scenarioInstanceId." at WARN;
					updater.emitNack(delete.messageId, delete.scenarioInstanceId);
				}
			}
			
			// and request instances is handled by a separate monitor. Note that 
			// Finished scenarios are not in the dictionary, but are handled by the finished
			// listener above.
			RequestInstancesInternal reqInstances;
			on all RequestInstancesInternal(scenarioId=scenario.scenarioId):reqInstances {
				if reqInstances.ownerFilter then {
					if runningCtxsByOwner.hasKey(reqInstances.owner) then {
						dictionary<integer, context> rCtxs:=runningCtxsByOwner[reqInstances.owner];
						if runningCtxsByOwner.hasKey("*") then {
							rCtxs:=rCtxs.clone();
							integer i;
							dictionary<integer, context> wildcards:=runningCtxsByOwner["*"];
							for i in wildcards.keys() {
								rCtxs.add(i, wildcards[i]);
							}
						}
						route RequestInstancesParallel(reqInstances, rCtxs, highestInstanceId);
					} else {
						if runningCtxsByOwner.hasKey("*") then {
							route RequestInstancesParallel(reqInstances, runningCtxsByOwner["*"], highestInstanceId);
						} else {
							route RequestInstancesParallel(reqInstances, new dictionary<integer,context>, highestInstanceId);
						}
					}
				} else {
					route RequestInstancesParallel(reqInstances, runningCtxs, highestInstanceId);
				}
			}
			ConfigureUpdates cu;
			on all ConfigureUpdates(scenarioId=""):cu or all ConfigureUpdates(scenarioId=scenario.scenarioId):cu {
				integer inst;
				for inst in runningCtxs.keys() {
					send cu to runningCtxs[inst];
				}
			}
			on all SendQueuedUpdatesNow() {
				SendQueuedUpdatesNow snow:=SendQueuedUpdatesNow();
				integer inst;
				for inst in runningCtxs.keys() {
					send snow to runningCtxs[inst];
				}
			}
		}
	}
	
	action addPendingOperation(dictionary<integer,  sequence<PendingOperation> > pendingOperations, integer scenarioInstanceId, integer messageId, integer type) {
		if not pendingOperations.hasKey(scenarioInstanceId) then {
			pendingOperations.add(scenarioInstanceId, new sequence<PendingOperation>);
		}
		pendingOperations[scenarioInstanceId].append(PendingOperation(messageId, type));
	}
	
	action removePendingOperation(dictionary<integer, sequence<PendingOperation> > pendingOperations, integer scenarioInstanceId, integer messageId) {
		if not pendingOperations.hasKey(scenarioInstanceId) then {
			return;
		}
		integer idx:=-1, i:=0;
		while(i < pendingOperations[scenarioInstanceId].size()) {
			if pendingOperations[scenarioInstanceId][i].messageId = messageId then {
				idx:=i;
				break;
			}
			i:=i+1;
		}
		if(idx>=0) then {
			pendingOperations[scenarioInstanceId].remove(idx);
		}
		if pendingOperations[scenarioInstanceId].size() = 0 then {
			pendingOperations.remove(scenarioInstanceId);
		}
	}	
}

/*
 * Monitor that handles request instances for parallel scenarios:
 * - on a RequestInstancesOnChannel for a parallel scenario, the ScenarioService 
 *   routes a RequestInstancesParallel which triggers this monitor to spawn.
 * - this waits for a response (be it to say the request instances has been 
 *   handled or that the scenario has finished) before sending the 
 *   RequestInstancesDone event 
 *
 * Note that this monitor assumes scenarioInstancesIds always increase over time.
 */
monitor RequestInstancesHandler {
	RequestInstancesParallel req;
	action onload() {
		on all RequestInstancesParallel():req {
			spawn handleRequest();
		}
	}
	
	/**
	 * Forward the request to every context and await responses. We remove 
	 * instances from the dictionary upon receiving an update for them. 
	 */
	action handleRequest() {
		ScenarioServiceUpdaterMultipleInstances updater:=new ScenarioServiceUpdaterMultipleInstances;
		updater.init(req.request.scenarioId, context.current());
		integer k;
		for k in req.instances.keys() {
			send req.request to req.instances[k];
		}
		// if a scenario dies, it will send events in the order ScenarioFinished, InstanceDied:
		ScenarioFinished finished;
		on all ScenarioFinished(scenarioId=req.request.scenarioId, scenarioInstanceId <= req.highestInstanceId):finished {
			// if a scenario dies after it has sent the Instance, do not send another 
			// (the StateChange/ InstanceDied events are sufficient)
			if(req.instances.hasKey(finished.scenarioInstanceId)) then {
				updater.emitInstance(req.request, finished.scenarioInstanceId,
					finished.owner, finished.state, finished.inputFieldValues, finished.outputFieldValues);
			}
		}
		// A finished scenario should not be counted
		InstanceDied died;
		on all InstanceDied(scenarioId = req.request.scenarioId, scenarioInstanceId <= req.highestInstanceId):died {
			if(req.instances.hasKey(died.scenarioInstanceId)) then {
				req.instances.remove(died.scenarioInstanceId);
				checkFinished();
			}
		}
		RequestInstancesParallelDone pdone;
		on all RequestInstancesParallelDone(scenarioId = req.request.scenarioId, messageId = req.request.messageId):pdone {
			if(req.instances.hasKey(pdone.scenarioInstanceId)) then {
				req.instances.remove(pdone.scenarioInstanceId);
				checkFinished();
			}
		}
		checkFinished();
	}
	
	/**
	 * Check whether we are finished.
	 * We are finished if we have no instances left in our dictionary, in which 
	 * case we route a RequestInstancesDone (handled by the ScenarioService monitor)
	 * and terminate.
	 */
	action checkFinished() {
		if req.instances.size()=0 then {
			route RequestInstancesDone(req.request.scenarioId, req.request.messageId);
			die;
		}
	}
}

 00000030 C:\SoftwareAG\Apama\monitors\ScenarioService.mon
MONF 00004f92 package com.apama.dataview;

/*
 * $Copyright(c) 2007, 2008 Progress Software Corporation (PSC). All rights reserved.$
 * $Copyright (c) 2013-2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
 * Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
 */


/* Apama DataViewService API.
 *
 * This service enables MonitorScript or JMon applications to expose a simple 
 * read-only "DataView" interface. DataViews may be viewed using Apama Dashboard 
 * Studio in a similar way to Scenarios.
 *
 * Several implementations of this interface will exist. 
 * Each implementation is in a separate file - inject the one you wish to use.
 *   e.g. DataViewService_Impl_Dict.mon
 *
 * Route vs. Emit
 * The standard implementations of the DataViewService will always ROUTE 
 * any com.apama.dataview.* events, with the expectation that applications
 * using the service are co-located in the same correlator as the service.
 * If your architecture requires the client application to be in a separate 
 * correlator to the DataView service, then two "emitter" monitors are 
 * available.
 * DataViewService_ServiceEmitter.mon  would be injected into the same 
 * correlator as the DataView service after injecting the DataViewService 
 * interface but before injecting the implementation.
 * DataViewService_ApplicationEmitter.mon  would be injected into the same 
 * correlator as the client application after injecting the DataViewService 
 * interface but before injecting the client application.
 * 
 *
 * Notes:
 * 
 * 1) Every event has an initial field called msgId. Clients may choose to put 
 *    some identifier in this field when sending messages to the service. Any 
 *    event routed by the service in response will contain the same identifier.
 *
 * 2) Most events contain a field called "dvName". This string 
 *    uniquely identifies a DataView inside the correlator. 
 *
 * 3) Every event has a final field called extraParams that is a string:string
 *    dictionary. Some implementations may choose to use this field. It provides
 *    a way of future-proofing the interface to a certain extent.
 *
 * $Revision: 267710 $
 */





/* ==========================================================================
 * ==========================================================================
 * Events to send to the DataViewService to manage the DataView schemas
 * ==========================================================================
 * ========================================================================== */


/** Define a new DataView type.
 *
 * Expect either a DataViewDefinition or a DataViewException in response.
 *
 * @see DataViewDefinition, DataViewException, DataViewAddItem
 *
 * Direction: From the application monitor to the DataViewService.
 */
event DataViewAddDefinition {
	/** Optional application-defined message identifier used to correlate 
		requests and responses. */
	string msgId;
	/** The name that uniquely identifies this DataView (e.g. DataView_XXX). */
	string dvName;
	/** The display name of the DataView (e.g. "XXX manager"). */
	string dvDisplayName;
	/** Optional field containing a description of this DataView 
		(e.g. "This DataView exposes XXX objects"). */
	string dvDescription;
	/** Specifies the names of each field exposed by the DataView. */
	sequence<string> fieldNames;
	/** Specifies the types corresponding to each field in the fieldNames sequence. 
		Supported types are: string, float, integer, boolean. */
	sequence<string> fieldTypes;
	/** Optional set of field names whose values in an Item are to be combined 
		to make a unique key that can be used instead of the dvItemId field of 
		DataViewDeleteItem, DataViewUpdateItem, and DataViewUpdateDelta events.
 	*/
	sequence<string> keyFields;
	
	/** Optional dictionary of extra information about this definition, 
		some of which may be available for display in dashboards and clients. 
		
		DataView implementation-specific parameters may also be stored here. 
		
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	*/
	dictionary<string, string> extraParams;
}


/** Remove an existing DataView type.
 *
 * Direction: From the customer Monitor to the DataViewService.
 *
 * Response: DataViewUnloaded()
 *
 * @see DataViewUnloaded
 */
event DataViewDeleteDefinition {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The DataView Name (e.g. conventionally DataView_XXX). */
	string dvName;
	/** [OPTIONAL] Some implementations may choose to make use of this field. 
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	*/
	dictionary<string, string> extraParams;
}


/** Request helper dictionary to facilitate sequence position lookup
 *    by field name.
 *
 * @see DataViewFieldLookup
 *
 * Direction: From the customer Monitor to the DataViewService.
 *
 * Response: DataViewFieldLookup()
 */
event DataViewGetFieldLookup {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The DataView Name (e.g. conventionally DataView_XXX). */
	string dvName;
	/** [OPTIONAL] Some implementations may choose to make use of this field.
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	 */
	dictionary<string, string> extraParams;
}



/* ==========================================================================
 * ==========================================================================
 * Events to send to the DataViewService to manage individual DataView Items
 * ==========================================================================
 * ========================================================================== */


/** Request that a new Item is added to a specific DataView. Must not already 
* exist. 
*
* @see DataViewItem
*
* Direction: From the customer Monitor to the DataViewService.
*/
event DataViewAddItem {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The owner (user) of the Item. */
	string owner;
	/** The timestamp of the initial update (seconds since epoch). If the value given is -1.0 then the service will populate it using correlator currentTime. */
	float timeStamp;
	/** Complete sequence of field values in string form. */
	sequence<string> fieldValues;
	/** [OPTIONAL] Some implementations may choose to make use of this field.
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	 */
	dictionary<string, string> extraParams;
}


/** Request that a new Item is added to a specific DataView if it does not already exist, 
* or is updated when it does exist. 
* 
* This will ONLY work when keyFields are used.
* Attempts to change the owner of an existing item will be rejected with a DataViewItemException.
*
* @see DataViewItem
*
* Direction: From the customer Monitor to the DataViewService.
*/
event DataViewAddOrUpdateItem {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The owner (user) of the Item - ONLY used for new items. */
	string owner;
	/** The timestamp of the initial update (seconds since epoch). If the value given is -1.0 then the service will populate it using correlator currentTime. */
	float timeStamp;
	/** Complete sequence of field values in string form. */
	sequence<string> fieldValues;
	/** [OPTIONAL] Some implementations may choose to make use of this field. 
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	*/
	dictionary<string, string> extraParams;
}


/** Contains updated DataView Item fields.
 * Use this to update the fields.
 *
 * Direction: From the customer Monitor to the DataViewService.
 *
 */
event DataViewUpdateItem { 
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The ID of the Item within the DataView (may be -1 if using keyFields instead). */
	integer dvItemId;
	/** The timestamp of the update (seconds since epoch). If the value given is -1.0 then the service will populate it using correlator currentTime. */
	float timeStamp;
	/** Sequence of field values in string form. */
	sequence<string> fieldValues;
	/** [OPTIONAL] Some implementations may choose to make use of this field. 
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	*/
	dictionary<string, string> extraParams;
}


/** Contains updated DataView Item fields.
 * Use this to update the fields.
 *
 * Direction: From the customer Monitor to the DataViewService.
 *
 */
event DataViewUpdateItemDelta { 
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The ID of the Item within the DataView (may be -1 if using keyFields instead). */
	integer dvItemId;
	/** The timestamp of the update (seconds since epoch). If the value given is -1.0 then the service will populate it using correlator currentTime. */
	float timeStamp;
	/** Values to be updated. Dictionary Key is index into fields sequence, Value is new field value in sequence. 
	 If not using dvItemId, then the dictionary MUST contain the key values (even though they have not changed). */
	dictionary<integer,string> fieldValues;
	/** [OPTIONAL] Some implementations may choose to make use of this field. 
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	*/
	dictionary<string, string> extraParams;
}


/** Request that a specific Item within a specific DataView is deleted.
 *
 * @see DataViewItemDeleted
 *
 * Direction: From the customer Monitor to the DataViewService.
 */
event DataViewDeleteItem {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The ID of the Item within the DataView (may be -1 if using keyFields instead). */
	integer dvItemId;
	/** [OPTIONAL] Sequence of ONLY key field values (if not using the dvItemId). */
	sequence<string> keyFields;
	/** [OPTIONAL] Some implementations may choose to make use of this field. 
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	*/
	dictionary<string, string> extraParams;
}


/** Request that all Items within a specific DataView are deleted.
 *
 * @see DataViewAllItemsDeleted
 *
 * Direction: From the customer Monitor to the DataViewService.
 */
event DataViewDeleteAllItems {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** [OPTIONAL] Some implementations may choose to make use of this field. 
	
		To avoid collisions with keys that may be added to the product in 
		future, all user-defined extraParams should start with a prefix 
		identifying the application or organisation they were added for. 
	*/
	dictionary<string, string> extraParams;
}


/* ==========================================================================
 * ==========================================================================
 * Events to that are callbacks from DatavViewService
 * ==========================================================================
 * ========================================================================== */


/** Confirmation that a specific DataView definition has been added.
 *
 * Direction: From the DataViewService to the customer Monitor.
 */
event DataViewDefinition {
	/** Optional application-defined message identifier used to correlate 
		requests and responses. */
	string msgId;
	/** The name that uniquely identifies this DataView (e.g. DataView_XXX). */
	string dvName;
	/** The display name of the DataView (e.g. "XXX manager"). */
	string dvDisplayName;
	/** Optional field containing a description of this DataView 
		(e.g. "This DataView exposes XXX objects"). */
	string dvDescription;
	/** Specifies the names of each field exposed by the DataView. */
	sequence<string> fieldNames;
	/** Specifies the types corresponding to each field in the fieldNames sequence. 
		Supported types are: string, float, integer, boolean. */
	sequence<string> fieldTypes;
	/** Optional set of field names whose values in an Item are to be combined 
		to make a unique key that can be used instead of the dvItemId field of 
		DataViewDeleteItem, DataViewUpdateItem, and DataViewUpdateDelta events.
 	*/
	sequence<string> keyFields;
	/** Optional dictionary of extra parameters.
		DataView implementation-specific parameters may also be stored here. */
	dictionary<string, string> extraParams;
	/** Prefix for identifying metadata entries in the extraParams dictionary.
		Entries with key names prefixed by the EXTRA_PARAMS_METADATA_PREFIX 
		string are considered metadata entries. */
	constant string EXTRA_PARAMS_METADATA_PREFIX := "Metadata:";
}


/** Indicates that a specific DataView definition is being unloaded.
 *
 * Direction: From the DataViewService to the customer Monitor.
 */
event DataViewDefinitionDeleted { 
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}


/** Notifies all interested clients that a new Item has been added to a specific 
 * DataView. 
 * The event provides the unique itemID, owner (user), and initial values for 
 * all fields.
 *
 * @see DataViewAddItem
 *
 * Direction: From the DataViewService to the customer Monitor.
 *
 */
event DataViewItem { 
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The ID of the Item within the DataView. */
	integer dvItemId;
	/** The owner (user) of the Item. */
	string owner;
	/** Sequence of field values in string form. */
	sequence<string> fieldValues;
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}


/** Notifies all interested clients that a specific Item within a specific 
 * DataView has been deleted.
 *
 * @see DataViewDeleteItem
 *
 * Direction: From the DataViewService to the customer Monitor.
 *
 */
event DataViewItemDeleted {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The ID of the Item within the DataView. */
	integer dvItemId;
	/** Sequence of ONLY key field values (for those not using the dvItemId). */
	sequence<string> keyFields;
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}


/** Notifies all interested clients that all Items within a specific 
 * DataView have been deleted.
 *
 * @see DataViewDeleteAllItems
 *
 * Direction: From the DataViewService to the customer Monitor.
 *
 */
event DataViewAllItemsDeleted {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}


/** The helper dictionary to facilitate sequence position lookup
 *    by field name.
 *
 * @see DataViewGetFieldLookup
 *
 * Direction: From the DataViewService to the customer Monitor.
 */
event DataViewFieldLookup {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The DataView Name (e.g. conventionally DataView_XXX). */
	string dvName;
	/** A map of fieldName:fieldIndex. */
	dictionary <string, integer> fields;
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}


/** Indicates that an exception occurred within the DataViewService, and indicates 
* the name of the specific DataView.
*
* Direction: From the DataViewService to the customer Monitor.
*/
event DataViewException {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The message in the exception. This is designed to be human readable, and may change between implementations/versions, hence the wildcard. */
	wildcard string message;
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}

/** Indicates that an exception occurred within the DataViewService, and indicates 
* the name of the specific DataView, and the Id of the specific Item.
*
* Direction: From the DataViewService to the customer Monitor.
*/
event DataViewItemException {
	/** [OPTIONAL] A messageId that applications may choose to use to identify "responses". */
	string msgId;
	/** The unique name of the DataView. */
	string dvName;
	/** The ID of the Item within the DataView. */
	integer dvItemId;
	/** The message in the exception. This is designed to be human readable, and may change between implementations/versions, hence the wildcard. */
	wildcard string message;
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}


/** Indicates that the DataView service is being unloaded.
 *
 * Direction: From the DataViewService to the customer Monitor.
 */
event DataViewServiceUnloaded { 
	/** [OPTIONAL] Some implementations may choose to make use of this field. */
	dictionary<string, string> extraParams;
}

/** Just logs the DataViewService details upon injection
  @private
*/
monitor DataViewService_Interface {
	// MetaData relating to the interface
	dictionary<string,string> interfaceMetaData := {
		"interface.package"     :"com.apama.dataview",
		"interface.name"        :"DataViewService_Interface",
		"interface.fileName"    :"DataViewService_Interface.mon",
		"interface.vendor"      :"Apama",
		"interface.version"     :"9.10.0.3.284318",
		"interface.fullVersion" :"rel/9.10.0.x@284318",
		"interface.language"    :"MonitorScript"
	};

	action onload() {
		log "DataViewService interface loaded. MetaData: "+interfaceMetaData.toString() at INFO;
	}
}

 0000003a C:\SoftwareAG\Apama\monitors\DataViewService_Interface.mon
TIME 0000000c 1474016712,1
MONF 0000ed96 package com.apama.dataview;

/*
 * $Copyright(c) 2007-2011 Progress Software Corporation (PSC). All rights reserved.$
 * $Copyright (c) 2013-2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
 * Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
 */


/* This is an implementation of the Apama DataViewService API.
 *
 * You must first inject the "interface" - DataViewService_Interface.mon
 * Several implementations of the interface may exist. 
 * You may also need to inject optional "emitter" (see below).
 * Each implementation is in a separate file - inject the one you wish to use.
 *   e.g. this implementation is DataViewService_Impl_Dict.mon
 * 
 *
 * This service enables MonitorScript or JMon applications to expose a simple 
 * read-only "DataView" interface. DataViews may be viewed using Apama Dashboard 
 * Studio in a similar way to Scenarios.
*
 * Route vs. Send
 * The standard implementations of the DataViewService will always ROUTE 
 * any com.apama.dataview.* events, with the expectation that applications
 * using the service are co-located in the same correlator as the service.
 * If your architecture requires the client application to be in a separate 
 * correlator to the DataView service, then two "emitter" monitors are 
 * available.
 * DataViewService_ServiceEmitter.mon  would be injected into the same 
 * correlator as the DataView service after injecting the DataViewService 
 * interface but before injecting the implementation.
 * DataViewService_ApplicationEmitter.mon  would be injected into the same 
 * correlator as the client application after injecting the DataViewService 
 * interface but before injecting the client application.
 *
 * Notes:
 * 
 * 1) Every event has an initial field called msgId. Clients may choose to put 
 *    some identifier in this field when sending messages to the service. Any 
 *    event routed by the service in response will contain the same identifier.
 *
 * 2) Most events contain a field called "dvName". This string 
 *    uniquely identifies a DataView inside the correlator. 
 *
 * 3) Every event has a final field called extraParams that is a string:string
 *    dictionary. Some implementations may choose to use this field. It provides
 *    a way of future-proofing the interface to a certain extent.
 *
 * $Revision: 268001 $
 */

using com.apama.scenario.Create;
using com.apama.scenario.Delete;
using com.apama.scenario.Edit;
using com.apama.scenario.RequestInstancesDone;
using com.apama.scenario.RequestInstancesInternal;
using com.apama.scenario.Scenario;
using com.apama.scenario.ScenarioServiceUpdaterMultipleInstances;
using com.apama.scenario.ScenarioUnloaded;
using com.apama.scenario.StartScenarioRecovery;

// INTERNAL API CODE

monitor DataViewService_Impl_Dict {

	/** Internal event used to store instances/items in a DataView. */
	event _Item {
		wildcard integer scenarioInstanceId;
		wildcard string owner;
		sequence<string> outputFields;
	}
	

	// MetaData relating to this specific implementation
	dictionary<string,string> implementationMetaData := {
		"implementation.package"       :"com.apama.dataview",
		"implementation.name"          :"DataViewService_Impl_Dict",
		"implementation.fileName"      :"DataViewService_Impl_Dict.mon",
		"implementation.vendor"        :"Apama",
		"implementation.version"       :"9.10.0.3.284318",
		"implementation.fullVersion"   :"rel/9.10.0.x@284318",
		"implementation.language"      :"MonitorScript"
	};

	
	// Variables needed in the parent monitor
	dictionary <string, string> dataViewNamesDictionary;  // scenarioId : dvName
	DataViewAddDefinition dvAddDefinition;

	// Variables required in the per-DataView spawned sub-monitor 
	string dvName;  // this is without the PREFIX (which is defined in the onload)
	string scenarioId; // this is with the PREFIX (which is defined in the onload)
	integer NUM_FIELDS;
	sequence<integer> compoundKeyIndexes; // the indexes of the fields which form the compound unique key of a _Item
	sequence<string> compoundKeyNames;    // the names of the fields which form the compound unique key of a _Item
	dictionary <string, integer> compoundKeyInstanceDictionary; // scenarioInstanceCompoundKey : scenarioInstanceId
	dictionary <integer, _Item> instancesDictionary; // scenarioInstanceId : _Item
	integer scenarioInstanceId := 0;
	
	// Defect 9762.  Space, semi-colon and tab in dvName must be escaped 
	dictionary<string, string> escapeCharMap := {" ":"_",		// space
	                                             ";":"$003b",	// semi-colon
	                                             "	":"$0009",	// tab
	                                             "\t":"$0009"	// tab
	                                            };
	dictionary <string, string> escapedDvNamesDictionary; // unescaped dvname : escaped dvname
	dictionary <string, string> escapedScenarioIdDictionary; // unescaped scenarioId : escaped scenarioId

	ScenarioServiceUpdaterMultipleInstances updater;
	
	
	//===================================================================================
	
	/** A default no-arg constructor for an internal _Item. */
	action createDefaultItem() returns _Item {
		return _Item( -1, "", new sequence<string> );
	}

	/** action to take a string and escape all characters appeared in the escapeCharMap dictionary
	 *
	 * @param s - the string to be escaped
	 * 
	 * @return escaped string.  The original string is returned if there is nothing to be escaped
	**/
	action escapeDvName(string s) returns string {
		integer i;
		string retString := s;
		string unEscapeChar;
		for unEscapeChar in escapeCharMap.keys() {
			retString := _escapeIt(retString, unEscapeChar, escapeCharMap[unEscapeChar]);
		}
		
		return retString;
	}
	
	/** internal recursive method to escape the passed in string s.  s will be scanned for the the 
	    unEscapeChar and if found, replaced that with the escapeChar param.  This process will continue
	    until all characters in s is scanned.
	**/
	action _escapeIt( string s, string unEscapeChar, string escapeChar ) returns string {
		
		if (s.find(unEscapeChar) = -1) then {
			return s;
		}
		
		integer index := s.find(unEscapeChar);

		string rest := s.substring(index+1, s.length());
		string retString := s.substring(0, index) + escapeChar + _escapeIt(rest, unEscapeChar, escapeChar);
		
		return retString;
	}
	
	action getEscapedDvName( string rawDvName ) returns string {
		string s;

		// given the raw dvName, return the cached value
		if (escapedDvNamesDictionary.hasKey(rawDvName)) then {
			return escapedDvNamesDictionary[rawDvName];
		}
		
		// not found, just return original string
		return rawDvName;
	}
	
	action getUnescapedDvName( string escapedDvName ) returns string {
		
		string s;
		for s in escapedDvNamesDictionary.keys() {
			if (escapedDvNamesDictionary[s] = escapedDvName) then {
				return s;
			}
		}
		
		// not found, just return original string
		return escapedDvName;
	}
	
	action getEscapedScenarioId( string rawScenarioId ) returns string {
		string s;

		// given the raw scenarioId, return the cached value
		if (escapedScenarioIdDictionary.hasKey(rawScenarioId)) then {
			return escapedScenarioIdDictionary[rawScenarioId];
		}
		
		// not found, just return original string
		return rawScenarioId;
	}
	
	action getUnescapedScenarioId( string escapedScenarioId ) returns string {
		
		string s;
		for s in escapedScenarioIdDictionary.keys() {
			if (escapedScenarioIdDictionary[s] = escapedScenarioId) then {
				return s;
			}
		}
		
		// not found, just return original string
		return escapedScenarioId;
	}
	
	action convertToScenarioId( string in_dvName ) returns string {
		string PREFIX := "DV_";
		string out_scenarioId;
		
		// for now just prepend the prefix, but in future do more checks and escaping of the name
		out_scenarioId:=PREFIX+in_dvName;
		
		return out_scenarioId;
	}
	
	
	action onload() {
		log "DataViewService implementation loaded. MetaData: "+implementationMetaData.toString() at INFO;
		
		log "onload() - entered." at DEBUG;

		// Look for the event to define a new DataView
		on all DataViewAddDefinition():dvAddDefinition {
			string escapedDvName := escapeDvName(dvAddDefinition.dvName);
			
			// name must not be ""
			if (dvAddDefinition.dvName.length()=0) then {
				string msg := "Invalid name for new DataView - length must be greater than zero.";
				log "onload().on_all_DataViewAddDefinition() - "+msg at WARN;
				route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string> );
				return;
			}
			
			//name can not start with MEMST (clashes with memorystore plugin)
			string MEMORY_STORE_PREFIX := "MEMST_";
			if (dvAddDefinition.dvName.find(MEMORY_STORE_PREFIX)=0) then {
				string msg := "Invalid name for new DataView - Must not start with " + MEMORY_STORE_PREFIX;
				log "onload().on_all_DataViewAddDefinition() - "+msg at WARN;
				route DataViewException(dvAddDefinition.msgId, dvAddDefinition.dvName, msg, new dictionary<string,string> );
				return;
			}
			
			
			// displayName must not be ""
			if (dvAddDefinition.dvDisplayName.length()=0) then {
				string msg := "Invalid displayName for new DataView - length must be greater than zero.";
				log "onload().on_all_DataViewAddDefinition() - "+msg at WARN;
				route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string> );
				return;
			}
			
			// use the escaped dvName here.  Will save to the escapedDvNamesDictionary
			// when validation is passed

			// both dvName and scenarioId are escaped
			dvName:=escapedDvName;
			scenarioId:=convertToScenarioId(dvName);
			string rawScenarioId:=convertToScenarioId(dvAddDefinition.dvName);
			
			// Validate the params we were given...
			
			// Make sure this is not already present as a DataView or Scenario
			if dataViewNamesDictionary.hasKey(scenarioId) then {
				string msg := "DataView name already exists";
				log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
				route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
				return;
			}
			
				
			// Make sure we were given more than zero field names and types
			if (0=dvAddDefinition.fieldNames.size() or 0=dvAddDefinition.fieldTypes.size()) then {
				string msg := "There must be 1 or more fields. DataViewAddDefinition event supplied "+dvAddDefinition.fieldNames.size().toString() +" names, and "+dvAddDefinition.fieldTypes.size().toString()+" types.";
				log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
				route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
				return;
			}
			
			// Make sure we were not given any duplicate field names or emptystring field names
			dictionary<string,boolean> validNames := new dictionary<string,boolean>;
			string fName;
			for fName in dvAddDefinition.fieldNames {
				// check for name=""
				if (0=fName.length()) then {
					string msg := "One of the supplied field names in a DataViewAddDefinition event is invalid. Field names must have a length greater than zero. The field names supplied were: "+dvAddDefinition.fieldNames.toString();
					log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
					route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
					return;					
				}
				// check for duplicates
				if (validNames.hasKey(fName)) then {
					string msg := "One of the supplied field names in a DataViewAddDefinition event is a duplicate. Duplicate field name: \""+fName+"\". The field names supplied were: "+dvAddDefinition.fieldNames.toString();
					log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
					route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
					return;
				}
				validNames[fName] := true;
			}
			
			// Make sure we were given enough types for the field names
			if (dvAddDefinition.fieldNames.size() != dvAddDefinition.fieldTypes.size()) then {
				string msg := "Each field name must have a corresponding type. DataViewAddDefinition event supplied "+dvAddDefinition.fieldNames.size().toString() +" names, and "+dvAddDefinition.fieldTypes.size().toString()+" types.";
				log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
				route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
				return;
			}
			
			// Make sure the type strings we were given represent valid scenario variable types
			dictionary<string,boolean> validTypes := {"string":true, "float":true, "integer":true, "boolean":true, "enumeration":true};			
			integer i := 0;
			string t;
			for t in dvAddDefinition.fieldTypes {
				if not validTypes.hasKey(t) then {
					string msg := "One of the supplied field types in a DataViewAddDefinition event is invalid. The type supplied at index "+i.toString()+" was: \""+t+"\"";
					log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
					route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
					return;
				}
				i:=i+1;
			}

			compoundKeyIndexes := new sequence<integer>; // ensure it is ALWAYS reset
			compoundKeyNames := new sequence<string>; // ensure it is ALWAYS reset
			// if we've been given some optional key fields, then validate these
			if (dvAddDefinition.keyFields.size() > 0) then {
				string k;
				for k in dvAddDefinition.keyFields {
					integer idx := dvAddDefinition.fieldNames.indexOf(k);
					if (-1 = idx) then {
						string msg := "One of the supplied keyField field names in a DataViewAddDefinition event is invalid. The invalid keyField name supplied was: \""+k+"\". The keyField names supplied were: "+dvAddDefinition.keyFields.toString();
						log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
						route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
						return;						
					}
					if (-1 < compoundKeyIndexes.indexOf(idx)) then {
						string msg := "A duplicate keyField field name was given in a DataViewAddDefinition event. Duplicate keyField name: \""+k+"\". The keyField names supplied were: "+dvAddDefinition.keyFields.toString();
						log "onload().on_all_DataViewAddDefinition() - \""+escapedDvName+"\" - "+msg at WARN;
						route DataViewException(dvAddDefinition.msgId, escapedDvName, msg, new dictionary<string,string>);
						return;												
					}
					compoundKeyIndexes.append(idx); // since it is valid, add the index to the set.
				}
				compoundKeyNames := dvAddDefinition.keyFields;
			}

			// all fields are validated, save the escapedDvName and associate the scenarioId with the escapedName (dvName is escaped already)
			escapedDvNamesDictionary.add(dvAddDefinition.dvName, dvName);
			escapedScenarioIdDictionary.add(rawScenarioId, scenarioId);
			
			dataViewNamesDictionary.add(scenarioId, dvName);
			log "onload().on_all_DataViewAddDefinition() - \""+dvAddDefinition.dvName+"\" - adding new DataView definition with name: \""+dvAddDefinition.dvName+"\", and exposed externally with scenarioId: \""+scenarioId+"\"" at INFO;
			log "onload().on_all_DataViewAddDefinition() - \""+dvAddDefinition.dvName+"\" - spawning initializeScenario()..." at DEBUG;
			spawn initializeScenario();

		}

		// look for requests to unload definitions and tidy up our records.
		// The actual (spawned) DataView *also* looks for this event and kills itself
		DataViewDeleteDefinition dvDeleteDefinition;
		on all DataViewDeleteDefinition():dvDeleteDefinition {
			string rawScenarioId := convertToScenarioId(dvDeleteDefinition.dvName);
			string escapedScenarioId := getEscapedScenarioId(rawScenarioId);
			if dataViewNamesDictionary.hasKey(escapedScenarioId) then {
				log "onload().on_all_DataViewDeleteDefinition() - \""+dvDeleteDefinition.dvName+"\" - removing DataView from dictionary with DataView name: \""+dvDeleteDefinition.dvName+"\", and exposed externally with scenarioId: \""+rawScenarioId+"\"" at INFO;
				dataViewNamesDictionary.remove(escapedScenarioId);
				escapedDvNamesDictionary.remove(dvDeleteDefinition.dvName);  // key is unescapedDvName
				escapedScenarioIdDictionary.remove(rawScenarioId);  // key is unescapedScenarioId
			}
		}

		
		// Look for external (i.e. com.apama.scenario API events) to remove
		// existing scenarios from our list to ensure we're up to date.
		ScenarioUnloaded scenScenarioUnloaded;
		on all ScenarioUnloaded():scenScenarioUnloaded {
			// the following if statement should never now eval to true, as the entry in the dictionary 
			// should always have been previously deleted by the DataViewDeleteDefinition (see above). Done for completeness.
			if dataViewNamesDictionary.hasKey(scenScenarioUnloaded.scenarioId) then {
				string dvName := dataViewNamesDictionary[scenScenarioUnloaded.scenarioId];
				log "onload().on_all_ScenarioUnloaded() - \""+scenScenarioUnloaded.scenarioId+"\" - removing DataView from dictionary with DataView name: \""+dvName+"\", and exposed externally with scenarioId: \""+scenScenarioUnloaded.scenarioId+"\"" at INFO;
				dataViewNamesDictionary.remove(scenScenarioUnloaded.scenarioId);
				escapedDvNamesDictionary.remove(getUnescapedDvName(dvName));  // key is unescapedDvName
				escapedScenarioIdDictionary.remove(getUnescapedScenarioId(scenScenarioUnloaded.scenarioId));  // key is unescapedScenarioId
			}			
		}

		
		// Look for DataViewItem* events for DataView names that do not exist and route exception events
		DataViewAddItem unmatchedAddItem;
		on all unmatched DataViewAddItem():unmatchedAddItem {
			string msg := "Unknown DataView name in a DataViewAddItem event";
			log "unmatched DataViewAddItem() - "+msg at WARN;
			route DataViewException(unmatchedAddItem.msgId, escapeDvName(unmatchedAddItem.dvName), msg, new dictionary<string,string>);
		}
		DataViewAddOrUpdateItem unmatchedAddOrUpdateItem;
		on all unmatched DataViewAddOrUpdateItem():unmatchedAddOrUpdateItem {
			string msg := "Unknown DataView name in a DataViewAddOrUpdateItem event";
			log "unmatched DataViewAddOrUpdateItem() - "+msg at WARN;
			route DataViewException(unmatchedAddOrUpdateItem.msgId, escapeDvName(unmatchedAddOrUpdateItem.dvName), msg, new dictionary<string,string>);
		}
		DataViewUpdateItem unmatchedUpdateItem;
		on all unmatched DataViewUpdateItem():unmatchedUpdateItem {
			string msg := "Unknown DataView name in a DataViewUpdateItem event";
			log "unmatched DataViewUpdateItem() - "+msg at WARN;
			route DataViewItemException(unmatchedUpdateItem.msgId, escapeDvName(unmatchedUpdateItem.dvName), unmatchedUpdateItem.dvItemId, msg, new dictionary<string,string>);
		}
		DataViewUpdateItemDelta unmatchedUpdateItemDelta;
		on all unmatched DataViewUpdateItemDelta():unmatchedUpdateItemDelta {
			string msg := "Unknown DataView name in a DataViewUpdateItemDelta event";
			log "unmatched DataViewUpdateItemDelta() - "+msg at WARN;
			route DataViewItemException(unmatchedUpdateItemDelta.msgId, escapeDvName(unmatchedUpdateItemDelta.dvName), unmatchedUpdateItemDelta.dvItemId, msg, new dictionary<string,string>);
		}
		DataViewDeleteItem unmatchedDeleteItem;
		on all unmatched DataViewDeleteItem():unmatchedDeleteItem {
			string msg := "Unknown DataView name in a DataViewDeleteItem event";
			log "unmatched DataViewDeleteItem() - "+msg at WARN;
			route DataViewItemException(unmatchedDeleteItem.msgId, escapeDvName(unmatchedDeleteItem.dvName), unmatchedDeleteItem.dvItemId, msg, new dictionary<string,string>);
		}
		DataViewDeleteAllItems unmatchedDeleteAllItems;
		on all unmatched DataViewDeleteAllItems():unmatchedDeleteAllItems {
			string msg := "Unknown DataView name in a DataViewDeleteAllItems event";
			log "unmatched DataViewDeleteAllItems() - "+msg at WARN;
			route DataViewException(unmatchedDeleteAllItems.msgId, escapeDvName(unmatchedDeleteAllItems.dvName), msg, new dictionary<string,string>);
		}
		DataViewGetFieldLookup unmatchedLookup;
		on all unmatched DataViewGetFieldLookup():unmatchedLookup {
			string msg := "Unknown DataView name in a DataViewGetFieldLookup event";
			log "unmatched DataViewGetFieldLookup() - "+msg at WARN;
			route DataViewException(unmatchedLookup.msgId, escapeDvName(unmatchedLookup.dvName), msg, new dictionary<string,string>);
		}
		
		log "onload() - complete." at DEBUG;
	}


	//===================================================================================

	
	// This defines the behaviour for each DataView bridge scenario type
	action initializeScenario() {
		log "initializeScenario() - \""+dvName+"\" - entered (spawned with this action)." at DEBUG;
		updater.init(scenarioId, context.current());

		string SCENARIO_SERVICE_CHANNEL := "com.apama.scenario";
		Scenario scenario := new Scenario; // store the schema of the DataView

		// Build up the scenario definition event
		scenario.scenarioId := scenarioId;
		scenario.displayName := dvAddDefinition.dvDisplayName;
		scenario.description := dvAddDefinition.dvDescription;
		scenario.inputNames := [];
		scenario.inputTypes := [];
		scenario.inputConstraints := [];
		scenario.inputDefaults := [];
		scenario.outputNames := dvAddDefinition.fieldNames;
		scenario.outputTypes := dvAddDefinition.fieldTypes;
		scenario.executionMode := 0;
		scenario.extraParams := dvAddDefinition.extraParams;
		scenario.extraParams.add("isReadOnly", "true");
		scenario.extraParams.add("type", "dataview");
		NUM_FIELDS := scenario.outputTypes.size();
	
		// If we have dont have enough constraints then set default constraints for ALL inputs (ignore any we were given)
		// - no - don't need to do that - always ZERO INPUTS
		
		// If we have dont have enough default initial inputs set basic defaults for ALL inputs (ignore any we were given)
		// - no - don't need to do that - always ZERO INPUTS

		
		// Notify any client listeners when scenario loads
		route scenario;
		send scenario to SCENARIO_SERVICE_CHANNEL;
		route DataViewDefinition(dvAddDefinition.msgId, dvName, dvAddDefinition.dvDisplayName, dvAddDefinition.dvDescription, dvAddDefinition.fieldNames, dvAddDefinition.fieldTypes, dvAddDefinition.keyFields, new dictionary<string,string>);

		// Build the helper dictionaries
		initialiseFieldLookup(scenario, dvName);
		
		// Look for the request to delete this bridge type
		DataViewDeleteDefinition delDef;
		on DataViewDeleteDefinition(dvName = getUnescapedDvName(dvName)):delDef {
			//Remove this bridge type
			// the "on unload" listener will generate the response.
			log "initializeScenario().on_DataViewDeleteDefinition() - \""+dvName+"\" - routing DataViewDefinitionDeleted/ScenarioUnloaded events..." at INFO;
			route DataViewDefinitionDeleted(delDef.msgId, dvName, new dictionary<string,string>);
			route ScenarioUnloaded(scenarioId);
			log "initializeScenario().on_DataViewDeleteDefinition() - \""+dvName+"\" - Killing the mthread for this DataView." at INFO;
			die;
		}

		// RequestInstances handling (dumps current state of all instances by sending Instance events)
		RequestInstancesInternal scenRequestInstances;
		on all RequestInstancesInternal(scenarioId = scenarioId): scenRequestInstances {
			log "initializeScenario().on_all_RequestInstances() - \""+scenarioId+"\" - received request for instances. Initiating recovery on for request messageId: "+scenRequestInstances.messageId.toString() at DEBUG;
			
			integer k;
			_Item item;
			for k in instancesDictionary.keys() {
				item := instancesDictionary[k];

				log "dvItemMThread().on_all_RequestInstances() - \""+scenarioId+":"+k.toString()+"\" - routing an Instance event for this DataView Item..." at DEBUG;

				if(scenRequestInstances.ownerFilter = false or 
				   scenRequestInstances.owner = item.owner or 
				   item.owner = "*") then {
					updater.emitInstance(scenRequestInstances, item.scenarioInstanceId, item.owner, "RUNNING", new sequence<string>, item.outputFields);
				}
			}
			route RequestInstancesDone(scenarioId, scenRequestInstances.messageId);
		}
		
		// Look for the scenario recovery event and send out the definition of this Scenario/DataView. (remember we have spawned per Scenario)
		on all StartScenarioRecovery() {
			route scenario;
		}

		
		
		
		// ========= Listeners for Create/Delete/Edit/Update ==================
		
		
		// ####################################################################
		// ####################################################################	
		// CREATE
		// First if an instance is created via the scenario API (e.g. a dashboard)...
		Create scenCreate;
		on all Create(scenarioId = scenarioId): scenCreate {
			log "initializeScenario().on_all_Create() - \""+scenarioId+"\" - Scenario instance create ignored - DataViewService does not handle incoming CREATE events from the ScenarioService." at WARN;
			emitNack(scenCreate.messageId, -1);
			return;
		}
		// ... secondly if it is created from the MonitorScript side of things
		DataViewAddItem dvAddItem;
		on all DataViewAddItem(dvName = getUnescapedDvName(dvName)): dvAddItem {
			log "initializeScenario().on_all_DataViewAddItem() - \""+dvName+"\" - " at DEBUG;
			addDataViewItem(dvAddItem);
		}
		
		
		// #########################################################
		// #########################################################
		// DELETE
		// From Scenario API - Ignore/Nack
		Delete scenDelete;
		on all Delete(scenarioId=scenarioId):scenDelete {
			log "initializeScenario().on_all_Delete() - \""+scenarioId+":"+scenDelete.scenarioInstanceId.toString()+"\" - Scenario instance delete ignored - DataViewService does not handle incoming DELETE events from the ScenarioService." at WARN;
			emitNack(scenDelete.messageId, scenDelete.scenarioInstanceId);
			return;
		}
		// From DataViewService API
		DataViewDeleteItem dvDeleteItem;
		on all DataViewDeleteItem(dvName=getUnescapedDvName(dvName)):dvDeleteItem {
			log "initializeScenario().on_all_DataViewDeleteItem() - \""+dvName+":"+dvDeleteItem.dvItemId.toString()+"\" - " at DEBUG;
			deleteDataViewItem(dvDeleteItem);
		}
		// From DataViewService API
		DataViewDeleteAllItems dvDeleteAllItems;
		on all DataViewDeleteAllItems(dvName=getUnescapedDvName(dvName)):dvDeleteAllItems {
			log "initializeScenario().on_all_DataViewDeleteAllItems() - \""+dvName+"\" - " at DEBUG;
			deleteAllDataViewItems(dvDeleteAllItems);
		}
		
		
		// #########################################################
		// #########################################################
		// EDIT
		// From Scenario API - Ignore/Nack
		Edit scenEdit;
		on all Edit(scenarioId = scenarioId):scenEdit {
			log "initializeScenario().on_all_Edit() - \""+scenarioId+":"+scenEdit.scenarioInstanceId.toString()+"\" - Scenario instance edit ignored - DataViewService does not handle incoming EDIT events from the ScenarioService." at WARN;
			emitNack(scenEdit.messageId, scenEdit.scenarioInstanceId);
			return;
		}		

		
		// #########################################################
		// #########################################################
		// ADD OR UPDATE (full)
		// Forward the AddOrUpdates from the Monitor via this DataViewService to the Scenario API
		DataViewAddOrUpdateItem dvAddOrUpdateItem;
		on all DataViewAddOrUpdateItem(dvName = getUnescapedDvName(dvName)):dvAddOrUpdateItem {
			addOrUpdateDataViewItem(dvAddOrUpdateItem);
		}

		// #########################################################
		// #########################################################
		// UPDATE (full)
		// Forward the Updates from the Monitor via this DataViewService to the Scenario API
		DataViewUpdateItem dvUpdateItem;
		on all DataViewUpdateItem(dvName = getUnescapedDvName(dvName)):dvUpdateItem {
			updateFullDataViewItem(dvUpdateItem);
		}

		// #########################################################
		// #########################################################
		// UPDATE (delta) - an update containing only deltas
		// Forward the Updates from the Monitor via this DataViewService to the Scenario API
		DataViewUpdateItemDelta dvUpdateItemDelta;
		on all DataViewUpdateItemDelta(dvName = getUnescapedDvName(dvName)):dvUpdateItemDelta {
			updateDeltaDataViewItem(dvUpdateItemDelta);
		}
		
		// ####################################################################
		// ####################################################################
		log "initializeScenario() - \""+dvName+"\" - complete." at DEBUG;
	}
	
	
	//===================================================================================

	
	// Management Actions

	/** Build the fieldname-index lookup dictionary, and add a the DataViewGetFieldLookup listener. */
	action initialiseFieldLookup(Scenario theScenario, string dataViewName) {
		log "initialiseFieldLookup() - building the dictionary, and adding a the DataViewGetFieldLookup listener" at DEBUG;
		string s;
		integer count := 0;
		dictionary <string, integer> fieldLookupDictionary := new dictionary<string, integer>; // fieldName : index in sequence
		for s in theScenario.outputNames {
			fieldLookupDictionary.add(s, count);
			count := count + 1;
		}		


		// Look for requests for the helper lookup dictionaries and reply
		DataViewGetFieldLookup lookupReq;
		on all DataViewGetFieldLookup(dvName = getUnescapedDvName(dataViewName)):lookupReq {
			log "initialiseFieldLookup().on_all_DataViewGetFieldLookup() - \""+dataViewName+"\" - routing DataViewFieldLookup event..." at DEBUG;
			route DataViewFieldLookup(lookupReq.msgId, dataViewName, fieldLookupDictionary, new dictionary<string,string>);
		}		
	}
	

	action generateInstanceDied(_Item item) {
		// this should only ever be called once, from deleteDataViewItem() or deleteAllDataViewItems(), so we no longer need a flag to says its been called.
		log "generateInstanceDied() - \""+scenarioId+":"+item.scenarioInstanceId.toString()+"\" - routing Died events." at DEBUG;
		updater.emitInstanceDied(item.scenarioInstanceId, item.owner);
	}
	
	
	//===================================================================================

	
	// Management Actions
	action onunload() {
		route DataViewServiceUnloaded( new dictionary<string,string> );
		log "DataViewService implementation unloaded." at INFO;
	}
	
	
	//===================================================================================

	/* Build the compound key from a COMPLETE sequence of values (i.e. not just the keyFields).
	 * The key will ALWAYS be > "".  "" indicates an error.
	 * For a single-field (non-compound) key, whose value is "", just return the prefix.
	*/
	action buildCompoundKeyFromSeq(sequence<string> fieldValues) returns string {
		string key := "#";
		integer keyIndex;
		integer i:=0;
		if not (fieldValues.size() = NUM_FIELDS) then {
			log "buildCompoundKeyfromSeq() - the given sequence of fields is the wrong size. Required: "+NUM_FIELDS.toString()+", Found: "+fieldValues.size().toString() at WARN;
			return "";
		}
		for keyIndex in compoundKeyIndexes {
			if (fieldValues.size() <= keyIndex) then {
				log "buildCompoundKeyfromSeq() - the given sequence of fields does not contain a required keyField: "+compoundKeyNames[i]+", index: "+keyIndex.toString() at WARN;
				return "";
			}
			key := key + fieldValues[keyIndex]; 
			i:=i+1;
			if (compoundKeyIndexes.size() > i) then {
				key := key +":";
			}
		}
		return key;
	}

	/* Build the compound key from a sequence of keyField values ONLY (i.e. just the keyFields).
	 * The key will ALWAYS be > "".  "" indicates an error.
	 * For a single-field (non-compound) key, whose value is "", just return the prefix.
	*/
	action buildCompoundKeyFromKeySeq(sequence<string> keyFieldValues) returns string {
		string key := "#";
		string keyValue;
		integer i:=0;
		if not (keyFieldValues.size() = compoundKeyIndexes.size()) then {
			log "buildCompoundKeyfromKeySeq() - the given sequence of keyFields is the wrong size. Required: "+compoundKeyIndexes.size().toString()+", Found: "+keyFieldValues.size().toString() at WARN;
			return "";
		}
		for keyValue in keyFieldValues {
			key := key + keyValue; 
			i:=i+1;
			if (compoundKeyIndexes.size() > i) then {
				key := key +":";
			}
		}
		return key;
	}

	/* Build the compound key from a dictionary of fieldNAME<->fieldvalue. 
	 * The dictionary does not need to contain a complete set of fields, but must contain at least the keyFields.
	 * The key will ALWAYS be > "".  "" indicates an error.
	 * For a single-field (non-compound) key, whose value is "", just return the prefix.
	*/
	action buildCompoundKeyFromDict(dictionary<string,string> fields) returns string {
		string key := "#"; // all keys will have this prefix
		string keyName;
		integer i:=0;
		for keyName in compoundKeyNames {
			if not (fields.hasKey(keyName)) then {
				log "buildCompoundKeyfromDict() - the given dictionary does not contain a required keyField: "+keyName at WARN;
				return "";
			}
			key := key + fields[keyName];
			i:=i+1;
			if (compoundKeyIndexes.size() > i) then {
				key := key +":";
			}
		}
		return key;
	}

	/* Build the compound key from a dictionary of fieldINDEX<->fieldvalue. 
	 * The dictionary does not need to contain a complete set of fields, but must contain at least the keyFields.
	 * The key will ALWAYS be > "".  "" indicates an error.
	 * For a single-field (non-compound) key, whose value is "", just return the prefix.
	*/
	action buildCompoundKeyFromIndexDict(dictionary<integer,string> fields) returns string {
		string key := "#"; // all keys will have this prefix
		integer keyIndex;
		integer i:=0;
		for keyIndex in compoundKeyIndexes {
			if not (fields.hasKey(keyIndex)) then {
				log "buildCompoundKeyfromIndexDict() - the given dictionary does not contain a required keyField: "+compoundKeyNames[i]+", index: "+keyIndex.toString() at WARN;
				return "";
			}
			key := key + fields[keyIndex];
			i:=i+1;
			if (compoundKeyIndexes.size() > i) then {
				key := key +":";
			}
		}
		return key;
	}
	
	
	/** Add a new DataViewItem to the current (spawned) DataView.
	 * This action is called as a result of matching a DataViewAddItem.
	 * 
	 * @exception Routes a DataViewException event if the new Item cannot be added.
	 */
	action addDataViewItem(DataViewAddItem dvAddItem) {
		log "addDataViewItem() - \""+dvName+"\" - entered." at DEBUG;
		
		if (dvAddItem.fieldValues.size() != NUM_FIELDS) then {
			string msg := "DataView addItem ignored - " + NUM_FIELDS.toString() + " output field(s) required but " + dvAddItem.fieldValues.size().toString() + " provided.";
			
			// dvName is escaped already
			log "addDataViewItem() - \""+dvName+"\" - " + msg at WARN;
			route DataViewException(dvAddItem.msgId, dvName, msg, new dictionary<string,string>);
			return;
		}

		_Item newItem := createDefaultItem();
		newItem.owner := dvAddItem.owner;
		newItem.outputFields := dvAddItem.fieldValues;
		
		// are we using the automatic compound key feature? If so we must generate the key and store it in a dictionary
		if (compoundKeyIndexes.size()>0) then {
			// build the key
			string compoundKey := buildCompoundKeyFromSeq(newItem.outputFields);
			
			// dvName is escaped already
			log "addDataViewItem() - \""+dvName+"\" - compoundKey is: "+compoundKey at DEBUG;
			
			// do we already have an item with that key?
			if (compoundKeyInstanceDictionary.hasKey(compoundKey)) then {
				// reject the new item - duplicate key
				string msg := "DataView addItem ignored - the DataView uses the keyFields feature and the new item clashes with an existing item.  The supplied values were: "+dvAddItem.fieldValues.toString();
				log "addDataViewItem() - \""+dvName+"\" - " + msg at WARN;
				route DataViewException(dvAddItem.msgId, dvName, msg, new dictionary<string,string>);
				return;
			}
			
			// add the new key to the dictionary to map it to the scenarioInstanceId(=dvItemId)
			compoundKeyInstanceDictionary.add(compoundKey, scenarioInstanceId);
		}
		
		//copy then increment the main instanceId counter
		newItem.scenarioInstanceId := scenarioInstanceId;
		scenarioInstanceId := scenarioInstanceId + 1;

		// store the newly created Item
		log "addDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - storing Item..." at INFO;
		instancesDictionary.add(newItem.scenarioInstanceId, newItem);
	
		// Send Created/Added events to both the Scenario and DataViewService APIs
		// This ensures that the Application MonitorScript and the Components such as Dashboards see the new instance
		log "addDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - routing Created/Added events..." at DEBUG;
		updater.emitCreated(-1, newItem.scenarioInstanceId, newItem.owner, "RUNNING", new sequence<string>, newItem.outputFields);
		
		log "addDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - routing initial Update event for this Item..." at DEBUG;
		float timeStamp := dvAddItem.timeStamp;
		if (-1.0=timeStamp) then {
			timeStamp:=currentTime;
		}
		updater.emitUpdate_time(newItem.scenarioInstanceId, timeStamp, newItem.outputFields, newItem.owner);
		
		route DataViewItem(dvAddItem.msgId, dvName, newItem.scenarioInstanceId, newItem.owner, newItem.outputFields, new dictionary<string,string>);

		log "addDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - complete." at DEBUG;
	}
	
	
	/* Add a new DataViewItem to the current (spawned) DataView if it does not already exist, 
	 * or update it when it does exist. This will ONLY work when keyFields are used.
	 * Attempts to change the owner of an existing item will be rejected with a DataViewItemException.
	 *
	 * This action is called as a result of matching a DataViewAddOrUpdateItem.
	 * 
	 * @exception Routes a DataViewException event if the new Item cannot be added.
	 * @exception Routes a DataViewItemException event if the new Item attempts to change the owner of an existing item.
	 */
	action addOrUpdateDataViewItem(DataViewAddOrUpdateItem dvAddOrUpdateItem) {
		log "addOrUpdateDataViewItem() - \""+dvName+"\" - entered." at DEBUG;
		
		if (0=compoundKeyIndexes.size()) then { 
			// this DataView does not use keyFields - error - The DataViewAddOrUpdateItem event is only valid for DataViews which use keyFields
			string msg := "DataViewAddOrUpdateItem event is only valid for DataViews which use keyFields";
			log "addOrUpdateDataViewItem() - \""+dvName+"\" - routing DataViewException event - "+msg at WARN;
			route DataViewException(dvAddOrUpdateItem.msgId, dvName, msg, new dictionary<string,string>);
			return;
		}

		if (dvAddOrUpdateItem.fieldValues.size() != NUM_FIELDS) then {
			string msg := "DataView addOrUpdateItem ignored - " + NUM_FIELDS.toString() + " output field(s) required but " + dvAddOrUpdateItem.fieldValues.size().toString() + " provided.";
			log "addOrUpdateDataViewItem() - \""+dvName+"\" - " + msg at WARN;
			route DataViewException(dvAddOrUpdateItem.msgId, dvName, msg, new dictionary<string,string>);
			return;
		}

		_Item newItem := createDefaultItem();
		newItem.owner := dvAddOrUpdateItem.owner;
		newItem.outputFields := dvAddOrUpdateItem.fieldValues;
		
		// build the key
	
		string compoundKey := buildCompoundKeyFromSeq(newItem.outputFields);
		log "addOrUpdateDataViewItem() - \""+dvName+"\" - compoundKey is: "+compoundKey at DEBUG;
		
		// do we already have an item with that key?
		boolean alreadyExists := compoundKeyInstanceDictionary.hasKey(compoundKey);
		if (alreadyExists) then {
			// Yes we found the key - so we're going to get the existing item and update it (after a couple of checks)
			integer itemId := compoundKeyInstanceDictionary[compoundKey];
			newItem.scenarioInstanceId := itemId; // copy the ID for use when we send out the update
			log "addOrUpdateDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - found dvItemId for keyFields" at DEBUG;
			
			// Get the existing item
			_Item item := instancesDictionary[itemId];
			
			// Check the owner is not changed
			if (item.owner != newItem.owner) then {
				// reject the new item - attempted to change OWNER
				string msg := "DataView addOrUpdateItem ignored - detected attempt to change the owner of an existing item.  Current owner: \""+item.owner+"\", attempted owner: \""+newItem.owner+"\"";
				log "addOrUpdateDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - " + msg at WARN;
				route DataViewItemException(dvAddOrUpdateItem.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;
			}
			
			// Now we can make the change to the actual stored values
			item.outputFields := newItem.outputFields;
		}
		else {
			// No didn't find the key, so it must be new - add the new key to the dictionary to map it to the scenarioInstanceId(=dvItemId)
			compoundKeyInstanceDictionary.add(compoundKey, scenarioInstanceId);
			
			//copy then increment the main instanceId counter
			newItem.scenarioInstanceId := scenarioInstanceId;
			scenarioInstanceId := scenarioInstanceId + 1;

			// store the newly created Item
			log "addOrUpdateDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - storing Item..." at INFO;
			instancesDictionary.add(newItem.scenarioInstanceId, newItem);
		
			// Send Created/Added events to both the Scenario and DataViewService APIs
			// This ensures that the Application MonitorScript and the Components such as Dashboards see the new instance
			log "addOrUpdateDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - routing Created/Added events..." at DEBUG;
			updater.emitCreated(-1, newItem.scenarioInstanceId, newItem.owner, "RUNNING", new sequence<string>, newItem.outputFields);
		}

		// In either case (add or update), we now need to route the Update event
		log "addOrUpdateDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - routing an Update event with the following field values:"+newItem.outputFields.toString() at DEBUG;
		float timeStamp := dvAddOrUpdateItem.timeStamp;
		if (-1.0=timeStamp) then {
			timeStamp:=currentTime;
		}
		updater.emitUpdate_time(newItem.scenarioInstanceId, timeStamp, newItem.outputFields, newItem.owner);
		
		if (not alreadyExists) then {
		
			route DataViewItem(dvAddOrUpdateItem.msgId, dvName, newItem.scenarioInstanceId, newItem.owner, newItem.outputFields, new dictionary<string,string>);
		}

		// All done
		log "addOrUpdateDataViewItem() - \""+dvName+":"+newItem.scenarioInstanceId.toString()+"\" - complete." at DEBUG;
	}


	/** Delete an existing DataViewItem from the current (spawned) DataView.
	 * This action is called as a result of matching a DataViewDeleteItem.
	 * 
	 * @exception Routes a DataViewItemException event if the Item cannot be deleted (not found, etc).
	 */
	action deleteDataViewItem(DataViewDeleteItem dvDeleteItem) {
		integer itemId := dvDeleteItem.dvItemId;
		string compoundKey;
		if (0<=itemId) then { // an itemId was supplied
			if (not instancesDictionary.hasKey(itemId)) then {
				string msg := "Unknown dvItemId: "+itemId.toString();
				log "deleteDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvDeleteItem.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;
			}
		}
		else { // no itemId supplied
			if (0=compoundKeyIndexes.size()) then { 
				// this DataView does not use keyFields - error - must supply a dvItemId
				string msg := "A valid dvItemId must be supplied (this DataView does not use keyFields)";
				log "deleteDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvDeleteItem.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;				
			}
			if (dvDeleteItem.keyFields.size() != compoundKeyIndexes.size()) then {
				// invalid number of keys supplied
				string msg := "Incorrect number of key fields supplied. Found "+dvDeleteItem.keyFields.size().toString()+", require "+compoundKeyIndexes.size().toString();
				log "deleteDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvDeleteItem.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;				
			}

			// build the key
			compoundKey := buildCompoundKeyFromKeySeq(dvDeleteItem.keyFields);
			if (0=compoundKey.length()) then {
				// we get an error back from buildCompoundKey - most likely the key fields were not supplied
				string msg := "Invalid set of key fields supplied: "+dvDeleteItem.keyFields.toString();
				log "deleteDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvDeleteItem.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;								
			}
			
			
			// all valid - get the itemId from the dictionary
			if (not compoundKeyInstanceDictionary.hasKey(compoundKey)) then {
				// we built a valid key, but no dvItem is currently known with that key
				string msg := "No DataViewItem could be found for the keyFields supplied: "+dvDeleteItem.keyFields.toString();
				log "deleteDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvDeleteItem.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;
			}
			itemId := compoundKeyInstanceDictionary[compoundKey];
			log "deleteDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - found dvItemId for keyFields" at DEBUG;
			
			// remove that compound key
			compoundKeyInstanceDictionary.remove(compoundKey);
		}
		_Item item := instancesDictionary[itemId];

		// Tell everyone about Deletion
		generateInstanceDied(item);
		log "deleteDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing Deleted events..." at DEBUG;
		updater.emitDeleted(-1, itemId, item.owner);
		if (dvDeleteItem.keyFields.size() != compoundKeyIndexes.size()) then {
			// deleted using dvItemId, but this DataView is using keyFields, so we need to build the key to pass out in the Deleted event
			integer i := 0;
			while i < compoundKeyIndexes.size() {
				dvDeleteItem.keyFields.append( item.outputFields[compoundKeyIndexes[i]] );
				i := i + 1;
			}
		}
		route DataViewItemDeleted(dvDeleteItem.msgId, dvName, itemId, dvDeleteItem.keyFields, new dictionary<string,string>);
		log "deleteDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - removing Item..." at INFO;
		instancesDictionary.remove(itemId);
		
		//log "deleteDataViewItem() - instancesDictionary content after removal: "+instancesDictionary.toString() at DEBUG;
	}


	
	/** Delete all existing DataViewItems from the current (spawned) DataView.
	 * This action is called as a result of matching a DataViewDeleteAllItems.
	 * 
	 * @exception Routes a DataViewItemException event if the Item cannot be deleted (not found, etc).
	 */
	action deleteAllDataViewItems(DataViewDeleteAllItems dvDeleteAllItems) {
		
		// Iterate all the items an delete each one
		integer itemId;
		_Item item;
		for itemId in instancesDictionary.keys() {
			item := instancesDictionary[itemId];
			// Tell everyone about Deletion
			generateInstanceDied(item);
			log "deleteAllDataViewItems() - \""+dvName+":"+itemId.toString()+"\" - routing Deleted events..." at DEBUG;
			updater.emitDeleted(-1, itemId, item.owner);
		}
		
		// Clear the dictionaries
		log "deleteAllDataViewItems() - \""+dvName+"\" - removing ALL Items..." at INFO;
		compoundKeyInstanceDictionary.clear();
		instancesDictionary.clear();

		route DataViewAllItemsDeleted(dvDeleteAllItems.msgId, dvName, new dictionary<string,string>);
		
		//log "deleteAllDataViewItems() - instancesDictionary content after removal: "+instancesDictionary.toString() at DEBUG;
	}


	/** Update ALL the field values of an existing DataViewItem in the current (spawned) DataView.
	 * This action is called as a result of matching a DataViewUpdateItem.
	 * 
	 * @exception Routes a DataViewItemException event if the Item cannot be updated (not found, keyField clash, etc).
	 */
	action updateFullDataViewItem(DataViewUpdateItem dvupdate) {
		integer itemId := dvupdate.dvItemId;
		
		// first check the number of fields (this is required before some of the other checks)
		if (dvupdate.fieldValues.size()!= NUM_FIELDS) then {
			string msg := "DataView updateItem ignored - " + NUM_FIELDS.toString() + " output field(s) required but " + dvupdate.fieldValues.size().toString() + " provided.";
			log "updateFullDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - " + msg at WARN;
			route DataViewItemException(dvupdate.msgId, dvName, itemId, msg, new dictionary<string,string>);
			return;
		}

		string compoundKey := "";
		if (0>itemId) then { // no itemId supplied
			if (0=compoundKeyIndexes.size()) then { 
				// this DataView does not use keyFields - error - must supply a dvItemId
				string msg := "A valid dvItemId must be supplied (this DataView does not use keyFields)";
				log "updateFullDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvupdate.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;				
			}
			// build the key
			compoundKey := buildCompoundKeyFromSeq(dvupdate.fieldValues);
			log "updateFullDataViewItem() - \""+dvName+"\" - compoundKey: "+compoundKey at DEBUG;
			if (0=compoundKey.length()) then {
				// we get an error back from buildCompoundKey - most likely the key fields were not supplied?
				string msg := "Unable to construct compound key from field values: "+dvupdate.fieldValues.toString();
				log "updateFullDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvupdate.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;								
			}
			
			// all valid - get the itemId from the dictionary
			if (not compoundKeyInstanceDictionary.hasKey(compoundKey)) then {
				// we built a valid key, but no dvItem is currently known with that key
				string msg := "No DataViewItem could be found using the keyFields from the field values supplied: "+dvupdate.fieldValues.toString();
				log "updateFullDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvupdate.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;
			}
			itemId := compoundKeyInstanceDictionary[compoundKey];
			log "updateFullDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - found dvItemId for keyFields" at DEBUG;
		}
		
		// check if we have an Item with the (supplied or calculated) itemId
		if (not instancesDictionary.hasKey(itemId)) then {
			string msg := "Unknown dvItemId: "+itemId.toString();
			log "updateFullDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing DataViewItemException event - "+msg at WARN;
			route DataViewItemException(dvupdate.msgId, dvName, itemId, msg, new dictionary<string,string>);
			return;
		}
				
		_Item item := instancesDictionary[itemId];
		
		// IMPORTANT
		// check we have not altered a keyField value if this DataView is configured for their use!!
		if (0<compoundKeyIndexes.size()) then {
			string existingCompoundKey := buildCompoundKeyFromSeq(item.outputFields);
			if (0=compoundKey.length()) then {
				compoundKey := buildCompoundKeyFromSeq(dvupdate.fieldValues);
			}
			if not (compoundKey=existingCompoundKey) then {
				string msg := "DataView updateItem ignored - It is not permitted to change the value of a keyField. The supplied values were: "+dvupdate.fieldValues.toString();
				log "updateFullDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvupdate.msgId, dvName, itemId, msg, new dictionary<string,string>);

				log "updateFullDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - existing compoundKey: \""+existingCompoundKey+"\", new compoundKey: \""+compoundKey+"\"" at DEBUG;
				
				return;				
			}
		}

		// Now we can make the change to the actual stored values
		integer i:=0;
		while(i<NUM_FIELDS) {
			item.outputFields[i] := dvupdate.fieldValues[i].clone();
			i:=i+1;
		}

		log "updateFullDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing an Update event with the following field values:"+item.outputFields.toString() at DEBUG;
		float timeStamp := dvupdate.timeStamp;
		if (-1.0=timeStamp) then {
			timeStamp:=currentTime;
		}
		updater.emitUpdate_time(item.scenarioInstanceId, timeStamp, item.outputFields, item.owner);
	}
	
	
	/** Update a SUBSET of the field values of an existing DataViewItem in the current (spawned) DataView.
	 * This action is called as a result of matching a DataViewUpdateItemDelta.
	 * 
	 * @exception Routes a DataViewItemException event if the Item cannot be updated (not found, keyField clash, etc).
	 */
	action updateDeltaDataViewItem(DataViewUpdateItemDelta dvdelta) {

		integer numberOfUpdatedFields := dvdelta.fieldValues.size() - compoundKeyIndexes.size();
		if (numberOfUpdatedFields=0) or (dvdelta.fieldValues.size()=0) then {
			// empty delta
			log "updateDeltaDataViewItem() - \""+dvName+":"+dvdelta.dvItemId.toString()+"\" - empty delta - ignored. Delta field values: "+dvdelta.fieldValues.toString() at DEBUG;
			//route DataViewItemException(dvName, dvdelta.dvItemId, "Empty delta - ignored. Delta field values: "+dvdelta.fieldValues.toString(), new dictionary<string,string>);
			return;
		}
		
		integer itemId := dvdelta.dvItemId;
		string compoundKey := "";
		
		if (0>itemId) then { // no itemId supplied
			if (0=compoundKeyIndexes.size()) then { 
				// this DataView does not use keyFields - error - must supply a dvItemId
				string msg := "A valid dvItemId must be supplied (this DataView does not use keyFields)";
				log "updateDeltaDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvdelta.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;				
			}
			if (numberOfUpdatedFields<0) then {
				// not enough dictionary items provided - must be at least the number of keyFields+1
				string msg := "Not enough delta field values provided. Must be at least number of keyFields +1 (="+ (compoundKeyIndexes.size()+1).toString() +"). Supplied delta field values: "+dvdelta.fieldValues.toString();
				log "updateDeltaDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvdelta.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;				
			}
			// build the key
			compoundKey := buildCompoundKeyFromIndexDict(dvdelta.fieldValues);
			log "updateDeltaDataViewItem() - \""+dvName+"\" - compoundKey: "+compoundKey at DEBUG;
			
			if (0=compoundKey.length()) then {
				// we get an error back from buildCompoundKey - most likely the key fields were not supplied?
				string msg := "Unable to construct compound key from delta field values: "+dvdelta.fieldValues.toString();
				log "updateDeltaDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvdelta.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;								
			}
			
			// all valid - get the itemId from the dictionary
			if (not compoundKeyInstanceDictionary.hasKey(compoundKey)) then {
				// we built a valid key, but no dvItem is currently known with that key
				string msg := "No DataViewItem could be found for the keyFields supplied: "+dvdelta.fieldValues.toString();
				log "updateDeltaDataViewItem() - \""+dvName+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvdelta.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;
			}
			itemId := compoundKeyInstanceDictionary[compoundKey];
			log "updateDeltaDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - found dvItemId for keyFields" at DEBUG;
		}

		// check if we have an Item with the (supplied or calculated) itemId
		if (not instancesDictionary.hasKey(itemId)) then {
			string msg := "Unknown dvItemId: "+itemId.toString();
			log "updateDeltaDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing DataViewItemException event - "+msg at WARN;
			route DataViewItemException(dvdelta.msgId, dvName, itemId, msg, new dictionary<string,string>);
			return;
		}
		
		//log "updateDeltaDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - content of delta: "+dvdelta.fieldValues.toString() at DEBUG;
		_Item item := instancesDictionary[itemId];
		integer k;
		sequence<string> tmpFields := item.outputFields.clone(); // take a copy of the current values
		for k in dvdelta.fieldValues.keys() {
			if (k<0 or k>=NUM_FIELDS) then {
				string msg := "DataView updateItemDelta ignored - Invalid field index provided ("+k.toString()+") in the delta field values: "+dvdelta.fieldValues.toString();
				log "updateDeltaDataViewItem() - \""+dvName+"\" - " + msg at WARN;					
				route DataViewItemException(dvdelta.msgId, dvName, itemId, msg, new dictionary<string,string>);
				return;
			}
			// no need to check for duplicate indexs, as we're getting them out of a dictionary - by definition there can't be duplicate keys!

			tmpFields[k] := dvdelta.fieldValues[k]; // overwrite a specific value with a new value
		}
		
		// IMPORTANT
		// check we have not altered a keyField value if this DataView is configured for their use!!
		if (0<compoundKeyIndexes.size()) then {
			string existingCompoundKey := buildCompoundKeyFromSeq(item.outputFields);
			if (0=compoundKey.length()) then {
				compoundKey := buildCompoundKeyFromSeq(tmpFields);
			}
			if not (compoundKey=existingCompoundKey) then {
				string msg := "DataView updateItemDelta ignored - It is not permitted to change the value of a keyField. The supplied values were: "+dvdelta.fieldValues.toString();
				log "updateDeltaDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing DataViewItemException event - "+msg at WARN;
				route DataViewItemException(dvdelta.msgId, dvName, itemId, msg, new dictionary<string,string>);

				log "updateDeltaDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - existing compoundKey: \""+existingCompoundKey+"\", new compoundKey: \""+compoundKey+"\"" at DEBUG;

				return;				
			}
		}

		// Now we can make the change to the actual stored values
		item.outputFields := tmpFields; // replace with the new updated set
		
		log "updateDeltaDataViewItem() - \""+dvName+":"+itemId.toString()+"\" - routing an Update event with the following field values: "+item.outputFields.toString() at DEBUG;
		float timeStamp := dvdelta.timeStamp;
		if (-1.0=timeStamp) then {
			timeStamp:=currentTime;
		}
		updater.emitUpdate_time(item.scenarioInstanceId, timeStamp, item.outputFields, item.owner);
	}

	
	//===================================================================================

	
	
	// The ACKs and NACKs for the Scenario API and the Scenario Bridge API.	
	action emitAcknowledgement(integer messageId, _Item item) {
		updater.emitAcknowledgement(messageId, item.scenarioInstanceId, item.outputFields);
	}

	action emitNack(integer messageId, integer scenarioInstanceId) {
		updater.emitNack(messageId, scenarioInstanceId);
	}

}
 0000003a C:\SoftwareAG\Apama\monitors\DataViewService_Impl_Dict.mon
DISC 0000003c 6330853567799787854:6330572092823077198 from 127.0.0.1:53172
CONN 0000003c 6330853572165271886:6330572097188561230 from 127.0.0.1:53175
TIME 0000000e 1474016712.1,1
MONF 00003027 //*****************************************************************************
// Title:       StatusSupport
// Description: Provides event definitions for generic status reporting from  
//              service monitors.
//
// Revision:    $Revision: 261655 $
//
// $Copyright(c) 2006-2007, 2008-2009, 2011-2012 Progress Software Corporation (PSC). All rights reserved.$
// $Copyright (c) 2013-2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
// Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
//
//*****************************************************************************

package com.apama.statusreport;

/*
	The events defined in this file act as an API between applications
	or blocks and service monitors.  They provide functionality to 
	Subscribe to status messages.
	
	The aim of this API is to provide an abstraction over any adapter
	specific details - for example, some adapters may require an explicit 
	call to subscribe to such data, some may be sent it anyway.
	
	Any adapter specific information that the application needs to supply
	or be supplied can be passed in the extraParams dictionary - these 
	are free-form (though there are conventions on the keys, see below).
	
	The service monitor also needs to handle any session initiation that
	may be required.
	
	A Status event does not denote a change of state, merely what the current
	state is - in particular, one will be sent out after every 
	SubscribeStatus request.

	All operations and responses are keyed on serviceId (if non-blank), 
	object,	connection and subServiceID. Every event starts with these 4 
	fields.
	
*/


/**
 *	Sent to the SubscribeStatus chanenl to subscribe to status.
 *	
 */
event SubscribeStatus {
	constant string CHANNEL := "SubscribeStatus";
	/**
	 *	service ID to subscribe to - blank will target all services.
	 */
	string serviceID;
	
	/**
	 *	object to request status of - this may include:
	 *	"Connection" - whether connected or not
	 *	"Market" - a market may be "Open", "Closed", or other states
	 */
	string object;
	
	/**
	 *	subService ID to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string subServiceID;
	
	/**
	 *	connection to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string connection;
}

/**
 *	Sent to the service monitor to unsubscribe from status.
 *	
 */
event UnsubscribeStatus {	 
	constant string CHANNEL := "SubscribeStatus";
	/**
	 *	service ID to subscribe to - blank will target all services
	 */
	string serviceID;
	
	/**
	 *	object to request status of - this may include:
	 *	"Connection" - whether connected or not
	 *	"Market" - a market may be "Open", "Closed", or other states
	 */
	string object;
	
	/**
	 *	subService ID to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string subServiceID;
	
	/**
	 *	connection to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string connection;
}


/**
 *	Sent from the service monitor to the StatusReport channel to notify the application of status for a 
 *	subscribed item.
 *	
 */
event Status {	
	constant string CHANNEL := "StatusReport";
	/**
	 *	service ID to subscribe to - blank will target all services
	 */
	string serviceID;
	
	/**
	 *	object to request status of - this may include:
	 *	"Connection" - whether connected or not
	 *	"MarketState" - a market may be "Open", "Closed", or other states
	 */
	string object;
	
	/**
	 *	subService ID to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string subServiceID;
	
	/**
	 *	connection to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string connection;

	/**
	 *	status description.
	 *	A free-form text string giving a description of the status.
	 */
	string description;
	
	/**
	 *	A sequence of summary strings specifying information about the 
	 *	status of the specified object.  This will be a well recognized
	 *	sequence of words - for example, a financial market's
	 *	"MarketState" may be "Open", "Closed", "PreOpen", etc.  A Connection 
	 *	may be "Connected", "Disconnected", "Disconnected LoginFailed", 
	 *	"Disconnected TimedOut", etc. 
	 */
	sequence<string> summaries;

	/**
	 *	available.
	 *	True if the object is "available" - the exact meaning is adapter 
	 * 	specific; for example, connected, open for general orders, etc.
	 */
	 boolean available;
	 
	 /** 
	 *	extra parameters that do not map into any of the above.  Convention
	 *	is that keys are in TitleCase.  e.g. "Username", "CloseTime", etc.
	 */
	wildcard dictionary <string, string> extraParams;
}


event StatusError {
	constant string CHANNEL := "StatusReport";
	/**
	 *	service ID to subscribe to - blank will target all services
	 */
	string serviceID;
	
	/**
	 *	object to request status of - this may include:
	 *	"Connection" - whether connected or not
	 *	"MarketState" - a market may be "Open", "Closed", or other states
	 */
	string object;
	
	/**
	 *	subService ID to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string subServiceID;
	
	/**
	 *	connection to subscribe to.
	 *	Some services may expose several services.  The interpretation of
	 *	this string is adapter-specific.
	 */
	string connection;

	/**
	 *	status description.
	 *	A free-form text string giving a description of the status.
	 */
	string description;
	
	/**
	 *	Whether the subscription has been terminated.  Any subscribers will 
	 *	need to send a new SubscribeStatus request after this.
	 */
	boolean failed;
}
	
//*****************************************************************************
// Title:         ParallelStatusSupport
// Description:   ParallelStatusSupport description
// Dependencies:  None
// Author:        arrustem
//
//*****************************************************************************

/*
	Event definitions for block subject event wrappers;
	These are provided for backwards compatibility.  Monitors listening to status should instead subscribe to the "StatusReport" channel.
*/
event SubscribeStatusToContext {
	context instanceContext;
	SubscribeStatus subscribe;
}

event UnsubscribeStatusToContext {
	context instanceContext;
	UnsubscribeStatus unsubscribe;
}

monitor ParallelStatusSupport {
	/*	dictionary < [serviceId] , sequence of contexts > eventRouter;
	 - eventRouter will keep track of all contexts that are listening to a particular symbol
	 - The same d.s. will be used for both, Status and StatusError
	   since they are added/removed precisely at the same time. 
	*/
	dictionary < string , sequence<context> > eventRouter;
	
	/*	dictionary < [serviceId], dictionary < [contextId], [key ctr] > > statusToContextCtr;
	 - Keeps count of number of listeners for each symbol on each context
	*/
	dictionary < string, dictionary < integer, integer > > statusToContextCtr;

	/*	dictionary < [serviceId], listener > statusListeners;
	 - keeps track of the local listeners for Status
		dictionary < [serviceId], listener > errorListeners;
	 - keeps track of the local listeners for StatusError
	*/
	dictionary < string, listener > statusListeners;
	dictionary < string, listener > errorListeners;
	
	action onload() {
		monitor.subscribe("StatusReport");
		// set up listeners for the wrapped events coming from subjects
		SubscribeStatusToContext subscribe;
		on all SubscribeStatusToContext():subscribe subscribeEventHandler(subscribe);
		
		UnsubscribeStatusToContext unsubscribe;
		on all UnsubscribeStatusToContext():unsubscribe unsubscribeEventHandler(unsubscribe);
	}
	
	action ondie() {
                // print out an error if statusToContextCtr is not empty
                if (statusToContextCtr.size() != 0) then {
                        log "ParallelStatusSupport is terminating but there may still be contexts subscribed to Status events" at ERROR;
                }
                else {
                        log "ParallelStatusSupport is terminating" at INFO;
                }
	}
	
	action onunload() {
		// not doing anything here since it is assumed that only one instance of this monitor is executing 
	}
	
	action subscribeEventHandler(SubscribeStatusToContext evt) {
		
		string key := evt.subscribe.serviceID;
		integer cId := evt.instanceContext.getId();
		
		// manage statusToContextCtr
		if not statusToContextCtr.hasKey(key) then {
			dictionary <integer, integer> entry := {cId:0};
			statusToContextCtr.add(key, entry);
		}
		else {
			if not statusToContextCtr[key].hasKey(cId) then {
				statusToContextCtr[key].add(cId, 0);
				
			}
		}
		statusToContextCtr[key][cId] := statusToContextCtr[key][cId] + 1;
		
		// manage eventRouter
		if not eventRouter.hasKey(key) then {
			sequence < context > entry := [];
			eventRouter.add(key, entry);
			addListener(key);
		}
		// look up context in the symbol
		if (eventRouter[key].indexOf(evt.instanceContext) = -1) then {
			eventRouter[key].append(evt.instanceContext);
		}

		route evt.subscribe;
	}

	action unsubscribeEventHandler(UnsubscribeStatusToContext evt) {
		string key := evt.unsubscribe.serviceID;
		
		integer cId := evt.instanceContext.getId();
		
		// manage statusToContextCtr
		if ((not statusToContextCtr.hasKey(key)) or 
			(not statusToContextCtr[key].hasKey(cId))) then {
			log "unsubscribeEventHandler: unexpected UnsubscribeStatusToContext received for (key=" + 
				key.toString() + ", contextId=" + cId.toString() + ")" at WARN;
		}
		else {
			statusToContextCtr[key][cId] := statusToContextCtr[key][cId] - 1;
			if (statusToContextCtr[key][cId] = 0) then {
				statusToContextCtr[key].remove(cId);
				integer i := eventRouter[key].indexOf(evt.instanceContext);
				if (i != -1) then {
					eventRouter[key].remove(i);
				}
			}
			if (statusToContextCtr[key].size() = 0) then {
				eventRouter.remove(key);
				removeListener(key);
			}
		}

		route evt.unsubscribe;
	}
	
	action addListener(string serviceID) {
		listener dl, el;
		context c;
		Status d;
		dl := on all Status(serviceID=serviceID):d {
			//send d to eventRouter[serviceID];
			for c in eventRouter[serviceID] {
				if c.getId() != context.current().getId() then {
					send d to c;
				}
			}			
		}
		if statusListeners.hasKey(serviceID) then {
			log "[action addListener] statusListeners already contains a listener for entry " + serviceID at WARN;
		}
		else {
			statusListeners.add(serviceID, dl);
		}
		
		StatusError e;
		el := on all StatusError(serviceID=serviceID):e {
			//send e to eventRouter[serviceID];
			for c in eventRouter[serviceID] {
				if c.getId() != context.current().getId() then {
					send e to c;
				}
			}
		}
		if errorListeners.hasKey(serviceID) then {
			log "[action addListener] errorListeners already contains a listener for entry " + serviceID at WARN;
		}
		else {
			errorListeners.add(serviceID, el);
		}
	}
	
	action removeListener(string serviceID) {

		if statusListeners.hasKey(serviceID) then {
			statusListeners[serviceID].quit();
			statusListeners.remove(serviceID);
		}
		else {
			log "[action removeListener] statusListeners does not contain a listener for entry " + serviceID at WARN;			
		}
		
		if errorListeners.hasKey(serviceID) then {
			errorListeners[serviceID].quit();
			errorListeners.remove(serviceID);
		}
		else {
			log "[action removeListener] errorListeners does not contain a listener for entry " + serviceID at WARN;			
		}
	}
}

 0000002e C:\SoftwareAG\Apama\monitors\StatusSupport.mon
DISC 0000003c 6330853572165271886:6330572097188561230 from 127.0.0.1:53175
CONN 0000003c 6330853572159242574:6330572097182531918 from 127.0.0.1:53178
TIME 0000000e 1474016712.2,1
MONF 000089d2 package com.apama.adapters;

//*****************************************************************************
// Apama IAF Status Manager service.
//
// The IAF Status Manager provides a factory and event interface for services 
// that monitor connectivity with an external system, routing events 
// when the system becomes unresponsive.
//
// $Copyright(c) 2005-2013 Progress Software Corporation (PSC). All rights reserved.$
// $Copyright (c) 2013-2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
// Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
//
// Version:  1.0
// Author:   Prashant Thumma
// Requires: None.
// Revision: $Revision: 268343 $
//
//*****************************************************************************

// -----------------------------------------------------------------------------
// Public Application Interface - between the IAFStatusManager service and EPL 
// clients of this service.
// -----------------------------------------------------------------------------

// Input events

/**
 * Sent from a client monitor to the IAFStatusManager, to register interest in 
 * receiving status events from the specified codec and transport. The fields 
 * of this event uniquely identify a subscription. 
 */
event AdapterStatusRegister {
	/** An identifier that will be used to refer to this transport and codec 
	 * pair when registering, deregistering or monitoring adapter status. 
	 */
	string adapterName;
	
	/** The name of the adapter's codec as it is specified in the adapter
	 * configuration file.
	 */
	string codec;

	/** The name of the adapter's transport as it is specified in the adapter
	 * configuration file.
	 */
	string transport;
	
	/** Codec version filter: in most cases, specify empty string ("") for 
	 * this field. 
	 *
	 * If set to a non-empty string, this field indicates the precise 
	 * codec version that the client depends on, which will lead to a 
	 * failed subscription attempt and an AdapterError event being sent to the 
	 * client if the versions do not match exactly. 
	 *
	 * The reason that Apama now recommends this field should usually be empty 
	 * is that most applications should implement a less simplistic policy 
	 * for handling version mismatch by specifying an empty string for this 
	 * value when subscribing, and then testing the version in an 
	 * application-specific when the AdapterUp event is received, e.g. 
	 * logging a WARN rather than failing entirely when there's a version 
	 * mismatch, or checking the major.minor version only (to avoid failing 
	 * when the fix version number is changed). 
	 *
	 * It is particularly important not to set the codecVersion filter 
	 * for applications in a persistent Correlator, since a product or adapter 
	 * upgrade that changes the fix version number is likely to prevent the 
	 * application from working correctly (unless specific steps are taken to 
	 * delete and reinject associated EPL as part of the upgrade). 
	 *
	 */
	string codecVersion;

	/** Transport version filter: in most cases, specify empty string ("") for 
	 * this field. 
	 *
	 * If set to a non-empty string, this field indicates the precise 
	 * transport version that the client depends on, which will lead to a 
	 * failed subscription attempt and an AdapterError event being sent to the 
	 * client if the versions do not match exactly. 
	 *
	 * The reason that Apama now recommends this field should usually be empty 
	 * is that most applications should implement a less simplistic policy 
	 * for handling version mismatch by specifying an empty string for this 
	 * value when subscribing, and then testing the version in an 
	 * application-specific when the AdapterUp event is received, e.g. 
	 * logging a WARN rather than failing entirely when there's a version 
	 * mismatch, or checking the major.minor version only (to avoid failing 
	 * when the fix version number is changed). 
	 *
	 * It is particularly important not to set the transportVersion filter 
	 * for applications in a persistent Correlator, since a product or adapter 
	 * upgrade that changes the fix version number is likely to prevent the 
	 * application from working correctly (unless specific steps are taken to 
	 * delete and reinject associated EPL as part of the upgrade). 
	 *
	 */
	string transportVersion;
	
	/** Configuration version filter: in most cases, specify empty string ("") 
	 * for this field. 
	 *
	 * If set to a non-empty string, this field indicates the precise 
	 * configuration version that the client depends on, which will lead to a 
	 * failed subscription attempt and an AdapterError event being sent to the 
	 * client if the versions do not match exactly. 
	 *
	 * If the adapter specifies a CONFIG_VERSION in the status 
	 * dictionary returned by getStatus(), the value of this field must either 
	 * be the same or be an empty string (""); if the adapter transport does 
	 * not specify a CONFIG_VERSION, this value must be an empty string.
	 *
	 * The reason that Apama now recommends this field should usually be empty 
	 * is that most applications should implement a less simplistic policy 
	 * for handling version mismatch by specifying an empty string for this 
	 * value when subscribing, and then testing the version in an 
	 * application-specific when the AdapterUp event is received, e.g. 
	 * logging a WARN rather than failing entirely when there's a version 
	 * mismatch, or checking the major.minor version only (to avoid failing 
	 * when the fix version number is changed). 
	 *
	 * It is particularly important not to set the configVersion filter 
	 * for applications in a persistent Correlator, since a product or adapter 
	 * upgrade that changes the fix version number is likely to prevent the 
	 * application from working correctly (unless specific steps are taken to 
	 * delete and reinject associated EPL as part of the upgrade). 
	 */
	string configVersion;
	
	/** The name of the channel the IAF adapter is receiving events from. 
	 */
	string channel;
}

/**
 * Sent by a client monitor to the IAFStatusManager, to remove its 
 * subscription to status events for a particular adapter.
 */
event AdapterStatusDeregister {
	/** The identifier used to refer to this transport and codec 
	 * pair when registering, deregistering or monitoring adapter status. 
	 */
	string adapterName;
}

// Output events

/** 
 * Notifies registered clients that the specified adapter process is 
 * running.
 */
event AdapterUp {
	/** The identifier used to refer to this transport and codec 
	 * pair when registering, deregistering or monitoring adapter status. 
	 */
	string adapterName;
	
	/** The difference (in seconds) between the time a request for status was 
	 * sent from the IAFStatusManager to the IAF and the time the response 
	 * was received.
	 */
	float latency;
	
	/** Information about the codec's status, which may including version  
	 * and connection information. Standard keys are VERSION, CONFIG_VERSION, 
	 * and CONNECTION (or in the case of multiple connections, keys of the 
	 * form CONNECTION_connectionName).
	 */
	dictionary<string, string> codecStatus;

	/** Information about the transport's status, which may including version 
	 * and connection information. Standard keys are VERSION, CONFIG_VERSION, 
	 * and CONNECTION (or in the case of multiple connections, keys of the 
	 * form CONNECTION_connectionName).
	 */
	dictionary<string, string> transportStatus;
}

/** 
 * Notifies registered clients that there is a problem with this subscription, 
 * such as a mismatch between the subscribed version and the version reported 
 * by the running transport/codec, or a timeout waiting for status information 
 * from the adapter. 
 */
event AdapterError {
	/** The identifier used to refer to this transport and codec 
	 * pair when registering, deregistering or monitoring adapter status. 
	 */
	string adapterName;
	
	/** A free-form string describing the problem.
	 */
	string description;
}

/** 
 * Notifies registered clients that a connection between the adapter and the 
 * external system it communicates with has been closed.  
 */
event ConnectionClosed {
	/** The identifier used to refer to this transport and codec 
	 * pair when registering, deregistering or monitoring adapter status. 
	 */
	string adapterName;
	
	/**
	 * A unique identifier for the connection.
	 * 
	 * If the adapter manages more than 
	 * one connection, this will be the connection name returned by the adapter 
	 * as a "CONNECTION_connectionName" getStatus() key (but without the 
	 * "CONNECTION_" prefix), or "" if the adapter only manages one connection. 
	 * The connectionName is often a number but could be a string. 
	 * One event will be sent for each connection the adapter manages.
	 */
	string connectionName;
	
	/** 
	  * Connection generation identifier.
	  * 
	  * This uniquely identifies a successful  
	  * connection attempt for the connectionName connection. 
	  * If the connection fails, and then is successfully connected again, this
	  * will change.  The connectionGeneration is often a number that is
	  * initialized with a timestamp when the adapter is created, then
	  * incremented every time it reconnects.
	  *
	  */
	string connectionGeneration;
}

/** 
* Notifies registered clients that a connection between the adapter and the 
* external system it communicates with has been successfully established.  
*/
event ConnectionOpened {
	/** The identifier used to refer to this transport and codec 
	 * pair when registering, deregistering or monitoring adapter status. 
	 */
	string adapterName;
	
	/**
	 * A unique identifier for the connection. 
	 * 
	 * If the adapter manages more than 
	 * one connection, this will be the connection name returned by the adapter 
	 * as a "CONNECTION_connectionName" getStatus() key (but without the 
	 * "CONNECTION_" prefix), or "" if the adapter only manages one connection. 
	 * The connectionName is often a number but could be a string. 
	 * One event will be sent for each connection the adapter manages.
	 */
	string connectionName;
	
	/** 
	  * Connection generation identifier.
	  *
	  * This uniquely identifies a successful  
	  * connection attempt for the connectionName connection. If the connection 
	  * fails, and then is successfully connected again, this will change. 
	  * The connectionGeneration is often a number that is initialized with a 
	  * timestamp when the adapter is created, then incremented every time it 
	  * reconnects.
	  */
	string connectionGeneration;
}

// -----------------------------------------------------------------------------
// Internal Interface - The following events are used for communication between 
// the IAFStatusManager and the IAF. Not for use by adapter or application 
// developers. 
// -----------------------------------------------------------------------------

/** An internal event sent from the IAFStatusManager to the IAF.
 * @private
 */
event IAFStatusRequest_1 {
	integer id;
	string codec;
	string transport;
}

/** An internal event sent from the IAF to the IAFStatusManager. 
 * @private
 */
event IAFStatusResponse_1 {
	integer id;
	integer maxIAFStatusVersion;
	integer iafPort;
	string configName;
	string codecName;
	string transportName;
	boolean running;
	boolean restarted;
	dictionary<string, string> codecStatus;
	dictionary<string, string> transportStatus;
}


//-----------------------------------------------------------------------------
// Adapter Notification Interface - The following events are used for 
// communication between the IAFStatusManager and a transport and codec running 
// inside the IAF.
//-----------------------------------------------------------------------------

// Transport/Codec -> IAFStatusManager

/** 
* Sent by a transport/codec running inside the IAF to notify the 
* IAFStatusManager that a connection to the external system the adapter 
* communicates with has been closed. 
*
* This is optional, and may be used to provide a way for IAFStatusManager 
* clients to be notified immediately about connectivity changes, without 
* waiting for the IAFStatusManager to poll the adapter for status.
* If the adapter sends this event, all fields must be consistent with the 
* values provided in the adapter's getStatus() implementations.
*/
event AdapterConnectionClosed {
	/** The name of the adapter's codec as it is specified in the adapter
	 * configuration file.
	 */
	string codecName;
	
	/** The name of the adapter's transport as it is specified in the adapter
	 * configuration file.
	 */
	string transportName;
	
	/**
	 * A unique identifier for the connection.
	 * 
	 * If the adapter manages more than 
	 * one connection, this should be the same as the connection name returned 
	 * by the adapter as a "CONNECTION_connectionName" key from getStatus() 
	 * (but without the "CONNECTION_" prefix), or "" if the adapter only 
	 * manages one connection. 
	 *
	 * The connectionName is often a number but could be a string. 
	 * One event should be sent for each connection the adapter manages.
	 */
	string connectionName;
	
	/** 
	  * Connection generation identifier.
	  *
	  * This uniquely identifies a successful  
	  * connection attempt for the connectionName connection. If the connection 
	  * fails, and then is successfully connected again, this will change. 
	  * The connectionGeneration is often a number that is initialized with a 
	  * timestamp when the adapter is created, then incremented every time it 
	  * reconnects.
	  */
	string connectionGeneration;
}

/** 
* Sent by a transport/codec running inside the IAF to notify the 
* IAFStatusManager that a connection to the external system the adapter 
* communicates with has been successfully established. 
*
* This is optional, and may be used to provide a way for IAFStatusManager 
* clients to be notified immediately about connectivity changes, without 
* waiting for the IAFStatusManager to poll the adapter for status. 
* If the adapter sends this event, all fields must be consistent with the 
* values provided in the adapter's getStatus() implementations.
*/
event AdapterConnectionOpened {
	/** The name of the adapter's codec as it is specified in the adapter
	 * configuration file.
	 */
	string codecName;
	
	/** The name of the adapter's transport as it is specified in the adapter
	 * configuration file.
	 */
	string transportName;

	/**
	 * A unique identifier for the connection.
	 *
	 * If the adapter manages more than 
	 * one connection, this should be the same as the connection name returned 
	 * by the adapter as a "CONNECTION_connectionName" key from getStatus() 
	 * (but without the "CONNECTION_" prefix), or "" if the adapter only 
	 * manages one connection. 
	 *
	 * The connectionName is often a number but could be a string. 
	 * One event should be sent for each connection the adapter manages.
	 */
	string connectionName;
	
	/** 
	  * Connection generation identifier. 
	  *
	  * This uniquely identifies a successful  
	  * connection attempt for the connectionName connection. If the connection 
	  * fails, and then is successfully connected again, this will change. 
	  * The connectionGeneration is often a number that is initialized with a 
	  * timestamp when the adapter is created, then incremented every time it 
	  * reconnects.
	  */
	string connectionGeneration;
}

/**
 * The IAFStatusManager allows interested monitors to register for 
 * status updates from adapters. 
 *
 * All registered adapters are polled for status information every few seconds, 
 * and in addition adapters may also send asynchronous notifications of 
 * connection changes to the IAFStatusManager if desired. The 
 * IAFStatusManager maintains status information for all registered 
 * adapters, and passes this on to registered clients whenever an adapter or 
 * any of its connections changes state.
 *
 * @routes AdapterUp, AdapterError, ConnectionClosed, ConnectionOpened
 * @listens AdapterStatusRegister, AdapterStatusDeregister
 *   IAFStatusResponse_1, AdapterConnectionClosed, AdapterConnectionOpened
 * @sends IAFStatusRequest_1
 */
monitor IAFStatusManager
{
	event ProcessConnectionsOpened {
		string codec;
		string transport;
	}
	AdapterStatusRegister subscribe;
	AdapterStatusRegister firstSubscription;
	AdapterStatusDeregister unsubscribe;
	AdapterConnectionClosed adapterConnClosed;
	AdapterConnectionOpened adapterConnOpened;
	dictionary<string, IAFStatusResponse_1> cachedAdapterStatus;
	IAFStatusResponse_1 adapterStatus;
	dictionary<string, ConnectionClosed> connClosed;
	dictionary<string, ConnectionOpened> connOpened;
	dictionary<string, integer> subscriptions;
	dictionary<string, listener> unsubscriptionListeners;
	dictionary<string, integer> channels;
	string codec;
	string transport;
	AdapterUp lastUpStatus;
	float IAF_STATUS_REQUEST_TIMEOUT := 5.0;
	integer lastResponseId := 0;
	integer currentRequestId := 0;
	integer currentResponseId := 0;
	float currentRequestTime := 0.0;
	float currentResponseTime := 0.0;
	boolean currentlyRunning:=false;
	boolean statusKnown:=false;
	string status:="";

	action onload() {
		// spawn this monitor to start handling status information for this adapter
		on all unmatched AdapterStatusRegister() : subscribe spawn startStatusHandler();
	}
	
	// main entry point 	
	action startStatusHandler()
	{
		codec := subscribe.codec;
		transport := subscribe.transport;
		firstSubscription := subscribe;		
		connClosed := new dictionary<string, ConnectionClosed>;
		connOpened := new dictionary<string, ConnectionOpened>;
		
		// add the subscription
		addSubscription();
					
		// listen for any additional subscriptions for this adapter
		on all AdapterStatusRegister(codec=subscribe.codec, transport=subscribe.transport) : subscribe
		{
			addSubscription();
			rerouteStatus(subscribe.adapterName);
		}
				
		// listen for IAFStatusResponse_1 for this adapter
		on all IAFStatusResponse_1(codecName=codec, transportName=transport) : adapterStatus
		{
			currentResponseTime := currentTime;
			currentResponseId := adapterStatus.id;

			processStatusUpdate();
		}
			
		on all AdapterConnectionClosed(codecName=codec, transportName=transport) : adapterConnClosed
		{
			processClosedConnection();
		}

		on all AdapterConnectionOpened(codecName=codec, transportName=transport) : adapterConnOpened
		{
			processOpenedConnection();
		}
		
		on all wait(IAF_STATUS_REQUEST_TIMEOUT)
		{
			sendStatusRequest();
		}
		
		sendStatusRequest();	
	}
		
	action addSubscription()
	{
		// add it to the list of subscription for this adapter
		if not subscriptions.hasKey(subscribe.adapterName) then {
			subscriptions.add(subscribe.adapterName, 1);
			// listen for RemoveAdapterSubscription
			listener l:=on all AdapterStatusDeregister(adapterName=subscribe.adapterName) : unsubscribe
			{
				removeSubscription();
			}
			unsubscriptionListeners.add(subscribe.adapterName, l);
		} else {
			subscriptions[subscribe.adapterName] := subscriptions[subscribe.adapterName] + 1;
		}
		if not channels.hasKey(subscribe.channel) then {
			channels.add(subscribe.channel, 1);
		} else {
			channels[subscribe.channel]:=channels[subscribe.channel] +1;
		}
		firstSubscription.codecVersion := handleSubscribeVersion(subscribe.codecVersion, firstSubscription.codecVersion);
		firstSubscription.transportVersion := handleSubscribeVersion(subscribe.transportVersion, firstSubscription.transportVersion);
		firstSubscription.configVersion := handleSubscribeVersion(subscribe.configVersion, firstSubscription.configVersion);
		if (cachedAdapterStatus.hasKey(subscribe.adapterName)) then 
		{ 
			sendAdapterStatus(subscribe.adapterName, cachedAdapterStatus[subscribe.adapterName]); 
		}
	}

	action handleSubscribeVersion(string newVersion, string oldVersion) returns string {
		// handle subscription where old and new may differ. The idea is 
		// to use the version that has a value, unless both do, in which
		// case it is an error to not use the latest version.
		if newVersion = "" then {
			return oldVersion;
		}
		if oldVersion = "" then {
			return newVersion;
		}
		if oldVersion != newVersion then {
			string msg := "Incompatible version number - subscription with " + subscribe.transportVersion + " and previously subscribed with " + firstSubscription.transportVersion + " for Adapter: " + subscribe.adapterName;
			log msg at ERROR;
			route AdapterError(subscribe.adapterName, msg);
			if statusKnown and currentlyRunning then {
				lastUpStatus.adapterName:=subscribe.adapterName;
				route lastUpStatus;
			}
		}			
		return oldVersion;
	}

	action removeSubscription()
	{
		if subscriptions.hasKey(unsubscribe.adapterName) then {
			if subscriptions[unsubscribe.adapterName]=1 then {
				subscriptions.remove(unsubscribe.adapterName);
				unsubscriptionListeners[unsubscribe.adapterName].quit();
				unsubscriptionListeners.remove(unsubscribe.adapterName);
			} else {
				subscriptions[unsubscribe.adapterName] := subscriptions[unsubscribe.adapterName] - 1;
			}   			
		} else {
			log "Internal error: Received an unsubscribe for an unsubscribed adapter name" at ERROR;
		}
		// if the number of subscriptions is zero then terminate this monitor
		if (subscriptions.size() = 0) then
		{
			die;
		}
	}

	action processStatusUpdate()
	{
		string adapterName;
		boolean errorOccured := false;

		for adapterName in subscriptions.keys()
		{
			// check the codec status versions
			if (adapterStatus.codecStatus.hasKey("VERSION") and
			    firstSubscription.codecVersion != "" and 
			    firstSubscription.codecVersion != adapterStatus.codecStatus["VERSION"]) then 
			{
				status:="Adapter " + adapterStatus.codecStatus["VERSION"] 
					+ " and subscription codec " + firstSubscription.codecVersion 
					+ " version mismatch for Adapter: " + adapterName;
				log status at ERROR;
				route AdapterError(adapterName, status);
				errorOccured := true;
			}

			// check the transport status versions
			if (adapterStatus.transportStatus.hasKey("VERSION") and
			    firstSubscription.transportVersion != "" and
			    firstSubscription.transportVersion != adapterStatus.transportStatus["VERSION"]) then
			{
				status:="Adapter " + adapterStatus.transportStatus["VERSION"] 
					+ " and subscription transport " + firstSubscription.transportVersion
					+ " version mismatch for Adapter: " + adapterName;
				log status at ERROR;
				route AdapterError(adapterName, status);
				errorOccured := true;
			}

			// check the codec config version
			if (adapterStatus.codecStatus.hasKey("CONFIG_VERSION") and
			    firstSubscription.configVersion != "" and 
			    firstSubscription.configVersion != adapterStatus.codecStatus["CONFIG_VERSION"]) then
			{
				status:="Adapter " + adapterStatus.codecStatus["CONFIG_VERSION"]
						 + " and subscription config " + firstSubscription.configVersion
						 + " version mismatch for Adapter: " + adapterName;
				log status at ERROR;
				route AdapterError(adapterName, status);
				errorOccured := true;
			}
				
			// check the transport config version
			if (adapterStatus.transportStatus.hasKey("CONFIG_VERSION") and
			    firstSubscription.configVersion != "" and 
			    firstSubscription.configVersion != adapterStatus.transportStatus["CONFIG_VERSION"]) then
			{
				status:="Adapter " + adapterStatus.transportStatus["CONFIG_VERSION"]
						 + " and subscription config " + firstSubscription.configVersion
						 + " version mismatch for Adapter: " + adapterName;
				log status at ERROR;
				route AdapterError(adapterName, status);
				errorOccured := true;
			}

			currentlyRunning:=false;
			statusKnown := true;
			if not errorOccured then
			{
				currentlyRunning:=adapterStatus.running;
				// check to see if the adapter is running
				if (adapterStatus.running) then
				{
					if not cachedAdapterStatus.hasKey(adapterName) then
					{
						cachedAdapterStatus[adapterName] := adapterStatus;
						sendAdapterStatus(adapterName, adapterStatus);
					}
					else if not checkStatus(cachedAdapterStatus[adapterName], adapterStatus) then
					{
						cachedAdapterStatus[adapterName] := adapterStatus;
						sendAdapterStatus(adapterName, adapterStatus);
					}
				}
				else
				{
					status:="Adapter is not running";
					route AdapterError(adapterName, status);
				}
			}
			
		}
		updateConnectionStatus();   			
	}

	action checkStatus(IAFStatusResponse_1 a, IAFStatusResponse_1 b) returns boolean
	{
		string k;
		if not (a.id = b.id) then { return false; }
		if not (a.maxIAFStatusVersion = b.maxIAFStatusVersion) then { return false; }
		if not (a.iafPort = b.iafPort) then { return false; }
		if not (a.configName = b.configName) then { return false; }
		if not (a.codecName = b.codecName) then { return false; }
		if not (a.transportName = b.transportName) then { return false; }
		if not (a.running = b.running) then { return false; }
		if not (a.restarted = b.restarted) then { return false; }
		for k in a.codecStatus.keys() {
			if not b.codecStatus.hasKey(k) then { return false; }
			if not b.codecStatus[k] = a.codecStatus[k] then { return false; }
		}
		for k in a.transportStatus.keys() {
			if not b.transportStatus.hasKey(k) then { return false; }
			if not b.transportStatus[k] = a.transportStatus[k] then { return false; }
		}
		return true;
	}
	
	action sendAdapterStatus(string name, IAFStatusResponse_1 status)
	{
		float latency := currentResponseTime - currentRequestTime;
		lastUpStatus:=AdapterUp(name,latency, 
						 status.codecStatus, 
						 status.transportStatus);
		route lastUpStatus;
	}
	
	action processClosedConnection()
	{
		boolean ignoreThisEvent := false;
		
		if not currentlyRunning then {
			// If adapter is not running i.e. AdapterUp event was not sent 
			// previously then ignore this event.
			// Connection status will be updated when sending next AdapterUp event
			ignoreThisEvent := true;
		}
		// check to see if this is was a new connection
		else if (not connClosed.hasKey(adapterConnClosed.connectionName)) then
		{
			// ok, this is a new event so we can bypass all the other checks
			// The code to send the ConnectionClosed is sent at the end
			// of this action block.
		}
		// check to see if we already handled this connection closed event
		else if (connClosed[adapterConnClosed.connectionName].connectionGeneration = 
			     adapterConnClosed.connectionGeneration) then
		{
			// do nothing; we already handled this event
			ignoreThisEvent := true;
		}
		// check to see if there is a corresponding open event for this close
		else if (not connOpened.hasKey(adapterConnClosed.connectionName)) then
		{
			string adapterName;
			for adapterName in subscriptions.keys()
			{
				ConnectionOpened co := ConnectionOpened(adapterName, 
														 adapterConnClosed.connectionName, 
													     adapterConnClosed.connectionGeneration);
				route co;
				connOpened[co.connectionName] := co;
			}
		}
		// check to see if there was this close matches the previous open
		else if (connOpened[adapterConnClosed.connectionName].connectionGeneration != 
				 adapterConnClosed.connectionGeneration) then
		{
			string adapterName;
			for adapterName in subscriptions.keys()
			{
				// first notify the close of the previous open connection
				route ConnectionClosed(adapterName, 
									   adapterConnClosed.connectionName, 
									   connOpened[adapterConnClosed.connectionName].connectionGeneration);
				
				// now notify the open of the currently closing connection
				ConnectionOpened co := ConnectionOpened(adapterName, 
														adapterConnClosed.connectionName, 
														adapterConnClosed.connectionGeneration);
				route co;
				connOpened[co.connectionName] := co;
			}
		}
		
		// Send the ConnectionClosed event
		if (not ignoreThisEvent) then
		{
			string adapterName;
			for adapterName in subscriptions.keys()
			{
				ConnectionClosed cc := ConnectionClosed(adapterName, 
														adapterConnClosed.connectionName, 
														adapterConnClosed.connectionGeneration);
				route cc;
				connClosed[cc.connectionName] := cc;  // we dont care about the adapterName
			}
		}
		if connOpened.hasKey(adapterConnClosed.connectionName) then {
			connOpened.remove(adapterConnClosed.connectionName);
		}
	}	   		
	
	action processOpenedConnection()
	{
		boolean ignoreThisEvent := false;
		
		if not currentlyRunning then {
			// If adapter is not running i.e. AdapterUp event was not sent 
			// previously then ignore this event.
			// Connection status will be updated when sending next AdapterUp event
			ignoreThisEvent := true;
		}
		// check to see if this is a a new connection
		else if (not connOpened.hasKey(adapterConnOpened.connectionName)) then
		{
			// ok, this is a new event so we can bypass all the other checks
			// The code to send the ConnectionOpened is sent at the end
			// of this action block.
		}
		// check to see if we already handled this connection open event
		else if (connOpened[adapterConnOpened.connectionName].connectionGeneration = 
			     adapterConnOpened.connectionGeneration) then
		{
			// do nothing; we already handled this event
			ignoreThisEvent := true;
		}
		// check to see if there is not a corresponding connection 
		// closed for the previously opened connection - i.e. there
		// is either no connection closed record, or the connection 
		// closed record is of a different connectionGeneration.
		else if ( not connClosed.hasKey(adapterConnOpened.connectionName) or
			      (connOpened[adapterConnOpened.connectionName].connectionGeneration != 
				   connClosed[adapterConnOpened.connectionName].connectionGeneration) ) then
		{
			string adapterName;
			for adapterName in subscriptions.keys()
			{
				ConnectionClosed cc := ConnectionClosed(adapterName, 
									adapterConnOpened.connectionName, 
									connOpened[adapterConnOpened.connectionName].connectionGeneration);
				route cc;
				connClosed[cc.connectionName] := cc;
			}
		}
		
		// Send the AdapterConnectionOpened event
		if (not ignoreThisEvent) then
		{
			string adapterName;
			for adapterName in subscriptions.keys()
			{
				ConnectionOpened co := ConnectionOpened(adapterName, 
														adapterConnOpened.connectionName, 
														adapterConnOpened.connectionGeneration);
				route co;
				connOpened[co.connectionName] := co;  // we dont care about the adapterName
			}
		}
		if connClosed.hasKey(adapterConnOpened.connectionName) then {
			connClosed.remove(adapterConnOpened.connectionName);
		}
	}	   		

	action updateConnectionStatus()
	{
		// Check each connection we know about, and close
		// any that aren't mentioned in this status message.
		string conn;
		boolean connectionsClosed:=false;
		for conn in connOpened.keys() {
			string tsKey;
			if conn="" then {
				tsKey:="CONNECTION";
			} else {
				tsKey:="CONNECTION_"+conn;
			}
			if (not adapterStatus.transportStatus.hasKey(tsKey) or adapterStatus.transportStatus[tsKey] != connOpened[conn].connectionGeneration) and
				not connClosed.hasKey(conn) then {
				route AdapterConnectionClosed(adapterStatus.codecName,
							      adapterStatus.transportName,
							      conn,
							      connOpened[conn].connectionGeneration);
				connectionsClosed:=true;
			}
		}
		if connectionsClosed then {
			route ProcessConnectionsOpened(codec, transport);
			IAFStatusResponse_1 localStatus:=adapterStatus;
			on ProcessConnectionsOpened(codec, transport) {
				updateConnectionsOpened(localStatus);
			}				
			return;
		} else {
			updateConnectionsOpened(adapterStatus);
		}
	}
	
	action updateConnectionsOpened(IAFStatusResponse_1 adapterStatus) {
		// check to see if the "CONNECTION" exists in the transportStatus of the
		// IAFStatusResponse_1 event
		if (adapterStatus.transportStatus.hasKey("CONNECTION")) then
		{
			// if this is a new connection or the connection has changed then route
			// an AdapterConnectionOpened event.  This event reuses the logic in processOpenedConnection
			string connection:="";
			if ( (not connOpened.hasKey(connection)) ) then
			{
				route AdapterConnectionOpened(adapterStatus.codecName,
											  adapterStatus.transportName,
											  "",
											  adapterStatus.transportStatus["CONNECTION"]);
			}
		}
		else
		{
			// check to see if this is an adapter that supports multiple connections
			// i.e. the connection names start with "CONNECTION_"
			string tsKey;
			integer connKeyLength := "CONNECTION_".length();
			for tsKey in adapterStatus.transportStatus.keys()
			{
				if (tsKey.length()> connKeyLength and tsKey.substring(0, connKeyLength) = "CONNECTION_") then
				{
					string connection:=tsKey.substring(connKeyLength, tsKey.length());
					// if this is a new connection or the connection has changed then route
					// an AdapterConnectionOpened event.  This event reuses the logic in processOpenedConnection
					if ( (not connOpened.hasKey(connection)) ) then
					{
						route AdapterConnectionOpened(adapterStatus.codecName,
													  adapterStatus.transportName,
													  connection,
													  adapterStatus.transportStatus[tsKey]);
					}
				}
			}
		}
	}

	action sendStatusRequest()
	{
		currentRequestTime := currentTime;
		// check for timeouts
		// also consider currentResponseTime to determine timeout to avoid state fluctuations 
		if (currentRequestId - currentResponseId) > 3 and (currentRequestTime - currentResponseTime) > 15.0 then
		{
			status:="Status request timed out";
			currentlyRunning:=false;
			statusKnown:=true;
			string adapterName;
			for adapterName in subscriptions.keys() {
				route AdapterError(adapterName, status);
			}
		}
	
		// increment the request id
		currentRequestId := currentRequestId + 1;
				
		// send the status request to all the collected channels
		string channel;
		for channel in channels.keys()
		{			
			send IAFStatusRequest_1(currentRequestId, codec, transport) to channel;
		}
	}

	action rerouteStatus(string adapterName)
	{
		if statusKnown then {
			if currentlyRunning then {
				lastUpStatus.adapterName:=adapterName;
				route lastUpStatus;
			} else {
				route AdapterError(adapterName, status);
			}
		}
	}
}
 0000003a C:\SoftwareAG\Apama\adapters\monitors\IAFStatusManager.mon
MONF 00009c0c package com.apama.database;

//*****************************************************************************
// ADBC (Apama Database Connector) Adapter Event Definitions.
//
// Provides definitions for all events sent to or from the ADBC Adapter. 
//
// $Copyright(c) 2009-2012 Progress Software Corporation (PSC). All rights reserved.$
// $Copyright (c) 2013, 2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
// Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
//
// Version:  1.0
// Author:   Frank Stoessel, Fred Cohen, Aston Chan
// Requires: None.
//
//*****************************************************************************

////////////////////////////////////////////////////////////////////////////////
//
// Database management events
//
////////////////////////////////////////////////////////////////////////////////

/**
* Request a connection to an existing Database (ODBC, JDBC, etc.).
*
* See also: DatabaseOperationAck()
*
* Response event: DatabaseOperationAck(); errorMessage is blank on success,
* "Database does not exist.", or otherwise a description of the error.
*
* Dataplayer: <br>
*     - Direction: From client to the correlator.
*/ 
event OpenDatabase
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service Id this database is on. */
	string serviceId;
	/** The name of the datbaase to open. */
	string databaseName;
	/** The username to the database. */
	string userName;
	/** The password to the database. */
	string password;
	/** Auto-commit mode: (case insensitive values).
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.
	*/
	string autoCommit;	
	/** If true, open db connection in read only mode(default is fale). */
	boolean readOnly; 
	/** Any additional parameters. */
	dictionary <string,string> extraParams;
}

/**
* Request an open Database be closed, freeing up resources.
* CloseDatabase will succeed if the database is not currently open.
*
* See also: DatabaseOperationAck()
*
* Response event: DatabaseOperationAck(); errorMessage is blank on success.
*
* Dataplayer: <br>
*     - Direction: From client to the correlator.
*/
event CloseDatabase
{
	/**  Unique request-response matching number. */
	integer messageId;	
	/** The service Id this database is on. */
	string serviceId;
	/** Database connection to close. */
	integer databaseId;	
	/** Force the adapter to close even if the reference count is not zero. */
	boolean force;		
	/** Any additional parameters. */
	dictionary<string,string> extraParams;
}

/**
* The acknowledgement event for OpenDatabase() or CloseDatabase().
*
* See also: OpenDatabase(), CloseDatabase()
*
* Dataplayer:<br>
*     - Direction: From correlator to the client.
*
*     Channel: <verbatim><</verbatim>serviceId<verbatim>></verbatim>.Control
*/
event DatabaseOperationAck
{
	/** Unique request-response matching number. */
	integer messageId;
	/** The service Id this database is on. */
	string serviceId;
	/** Blank on success, otherwise the error message. */
	string errorMessage;
	/** The Database connection. */ 
	integer databaseId;	
}		

/**
* Request a list of all available databases.
*
* Response event: Databases()
*
* Dataplayer: <br>
*    - Direction: From client to the correlator.
*/
event RequestDatabases
{
	/** Unique request-response matching number. */
	integer messageId;
	/** The service Id to request databases on. */
	string serviceId;
	/** Location of database (DataSource specific) directory, connection string,etc. */
	string databaseLocation;
	/** User name (if required). */
	string userName;	
	/** Password (if required). */
	string password;	
	/** Any additional parameters. */
	dictionary<string,string> extraParams;
}

/**
* Included in the response event for getDatabasesFull as a sequence 
* of Database events, one for each available database.
*
* See also: getDatbasesFull()
*
* Dataplayer: <br>
*    - Direction: From correlator to client.
*/
event Database
{
	/** A short display name. */
	string shortName;
	/** A connection URL (this value is passed to openDatabase). */
	string dbUrl;
	/** A detailed decription, likely empty for most dbs. */
	string description;
	/** Any additional parameters. */
	dictionary<string,string> extraParams;
}

/**
* Response event for RequestDatabases. A sequence of Database events
* is returned, one for each available database.
*
* See also: RequestDatabases
*
* Dataplayer: <br>
*    - Direction: From correlator to client.
*/
event Databases
{
	/** Unique request-response matching number. */
	integer messageId;
	/** The service Id the databases are on. */
	string serviceId;
	/** Blank on success, otherwise some error message. */
	string errorMessage;
	/** A list of databases available. */
	sequence<Database> databases;
	/** Any additional parameters. */
	dictionary<string,string> extraParams;
}


/**
* Request DataSources available for a given ADBC adapter.
*
* Response event: DataSources
*    
*     Channel: None
*/
event RequestDataSources
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** Any additional parameters. */
	dictionary<string,string> extraParams;
}

/**
* Included in the response event for RequestDataSources(). 
* A sequence of DataSource events is returned, one for 
* each available dataSource.
*
* See also: RequestDataSources()
*
* Dataplayer: <br>
*     - Direction: From correlator to client <br>
*     - Direction: From iaf to client
*
*     Channel: None
*/
event DataSource
{
	/** Service Ids (channel names) to talk to this datasource. */
	string serviceId;
	/** Name of DataSource (e.g. Sim, ODBC, SQLite, etc.). */
	string name;
	/** Any additional parameters. */
	dictionary<string,string> extraParams;
}

/**
* Response event for RequestDataSources(), with a sequence
* of available dataSources.
*
* See also: RequestDataSources()
*
* Dataplayer: <br>
*     - Direction: From correlator to client <br>
*     - Direction: From iaf to client
*
*     Channel: None
*/
event DataSources
{
	/** Unique request-response matching number. */
	integer messageId; 
	/** Blank on success, otherwise some error message. */
	string errorMessage;
	/** A list of dataSources available. */
	sequence<DataSource> datasources;
	/** Any additional parameters. */
	dictionary<string,string> extraParams;
}


/**
* Request all named queries from a data source.
*/
event RequestNamedQueries {
	/** Unique request-response matching number. */
	integer messageId;
	/** The service Id the databases are on. */
	string serviceId;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* The parameter data for a parameter of a named query.
*/
event ParameterData {
	/** Name of the parameter. */
	string name;
	/** The type of the parameter : Integer, Float, String, Time, Boolean, ApamaEvent. */
	string type;
	/** A description of the parameter. */
	string description;	
	/** The default value for the parameter. */
	string defaultValue;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* The named query.
*
* See also RequestNamedQueries
*
*/
event NamedQuery {
	/** Name of the NamedQuery. */
	string name;
	/** A description of the NamedQuery. */
	string description;
	/** The parameters associated with this NamedQuery. */
	sequence<ParameterData> parameters;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* All named query information available on this serviceId.
*
* See also RequestNamedQueries
*
*/
event NamedQueries {
	/** Unique request-response matching number. */
	integer messageId;
	/** The service Id the databases are on. */
	string serviceId;
	/** Blank on success, otherwise the error message. */
	string errorMessage;
	/** The sequence of available NamedQueries. */
	sequence<NamedQuery> namedQueries;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}



////////////////////////////////////////////////////////////////////////////////
//
// Query events
//
////////////////////////////////////////////////////////////////////////////////
/**
* Sent to adapter to create a prepared (reusable) database query.
*
* See also: PreparedQueryAck()
*
* Response event: PreparedQueryAck(); errorMessage is blank on success,
* or otherwise a description of the error.
*/
event CreatePreparedQuery {
	/** Unique id for this query. */
	integer messageId;
	/** The serviceId this prepared query is to be created on. */
	string serviceId;
	/** The database connection id. */
	integer databaseId;
	/** Prepared query to create. */
	string query;
	/** Optional input parameter types for prepared queries or stored procedures. */
	sequence<string> inputParamTypes;
	/** Optional output parameter types for stored procedures. */
	sequence<string> outputParamTypes;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Sent to adapter to delete a prepared (reusable) database query.
*
* See also: PreparedQueryAck()
*
* Response event: PreparedQueryAck(); errorMessage is blank on success,
* or otherwise a description of the error.
*/
event DeletePreparedQuery {
	/** Unique id for this query. */
	integer messageId;
	/** The serviceId this prepared query is on. */
	string serviceId;
	/** The database connection id. */
	integer databaseId;
	/** Prepared query to delete. */
	integer queryId;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* The acknowledgement event for CreatePreparedQuery or DeletePreparedQuery.
*
* See also: CreatePreparedQuery(), DeletePreparedQuery()
*/
event PreparedQueryAck
{
	/** Unique id for this query. */
	integer messageId;
	/** The serviceId of the prepared query. */
	string serviceId;
	/** Blank on success, otherwise the error message. */
	string errorMessage;
	/** Prepared query id. */
	integer queryId;
}		

/**
* Sent to adapter to start a database query.
*
* See also: QueryStatus(), ResultSchema(), ResultEvent()
*
* Response: QueryStatus(), ResultSchema(), ResultEvent()
*
* Dataplayer: <br>
*     - Direction: Client to correlator.
*/
event StartQuery {
	/** Unique id for this query. */
	integer messageId;
	/** The service id, Adapter channel name. */
	string serviceId;
	/** Database connection id. */
	integer databaseId;	
	/** Query to run (either query string or name). */
	string query;		
	/** True if this is a named query.*/
	boolean	namedQuery;
	/** Parameters for a named query. */
	dictionary<string,string> namedQueryParameters;
	/** Return Event Type should be one of: Native, Historical, ResultEvent, ResultEventHetero. */
	string returnType;	
	/** Event type (name) for use in mapping rules, needed when using Native returnType. */
	string eventType;
	/**
	* Indicates Query is being used for backtest.
	*
	* <pre>
	* This requires:<br>
	*	1. Only one can be running per database per adapter. <br>
	*	2. All queries must be time sorted. <br>
	*	3. The IAF must be able to identify the time attribute <br>
	*		for all types and possible tie breaker attribute.</pre>
	*/
	boolean backtest;
	/** Generate a status event every n seconds. If n <= 0.0 no status event is generated. */
	float statusFrequency;
	/** Max events to send before waiting for GetNextBatch. */
	integer batchSize;
	/** If true the batch size will count &Time events, if false it will not. */
	boolean batchSizeIncludesTimeEvents;
	/** Speed for back testing: <= 0.0, as fast as possible, > 0.0 -> some multiple of playback speed. */
	float backtestSpeed;
	/** Column to use for backtest time attribute, not needed if column name is "time". */
	string timeColumn;
	/** Stop the query and send a BatchDone if the event time > runUntilTime. */
	float runUntilTime;
	/** PreparedQuery to run. */
	integer preparedQueryId;
	/** Input parameters for preparedQuery. */
	sequence<string> inputParameters; 
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Status event in response to the StartQuery event.
* QueryStatus() contains the status of a currently active query request.
*
* See also: StartQuery()
*
* Dataplayer: <br>
*     - Direction: Correlator to client
*
*     Channel: <verbatim><</verbatim>serviceId<verbatim>></verbatim>.Control
*/
event QueryStatus {
	/** Unique id for this query. */
	integer messageId;
	/** The service id, Adapter channel name. */
	string serviceId;
	/** Number of events sent so far. */
	integer eventCount;
	/** Timestamp of last event sent. */
	float lastEventTime;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Event in response to the StartQuery event to indicate the result's schema.
*
* Note: This event will be sent before the first ResultEvent and only if 
* ReturnType is set to ResultEvent.
*
* See also: StartQuery()
*
* Dataplayer: <br>
*     - Direction: Correlator to client
*
*     Channel: <verbatim><</verbatim>serviceId<verbatim>></verbatim>.Control
*/
event ResultSchema {
	/** Unique id for this query. */
	integer messageId;
	/** The service id, Adapter channel name. */
	string serviceId;
	/** The id of the schema. */
	integer schemaId; 
	/** The order of the fields in the schema. */
	sequence <string> fieldOrder;
	/** The types of the fields in the schema. */
	dictionary <string, string> fieldTypes;
	/** The index field(s) of this schema. */
	sequence <string> indexFields;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Response to the StartQuery or GetNextBatch event.
*
* These will be sent in order until the number requested, in the rowLimit
* specified, in either the StartQuery or GetNextBatch, has been sent.
*
* No additional ResultEvents will be sent until the next GetNextBatch.
* If rowLimit=0, events will be streamed as fast as correlator can accept.
*
* See also: StartQuery(), GetNextBatch()
*
* Dataplayer: <br>
*     - Direction: Correlator to client
*
*     Channel: None.
*/
event ResultEvent {
	/** Unique id for this query. */
	integer messageId;
	/** The service id, Adapter channel name. */
	string serviceId;
	/** The id of the schema. */
	integer schemaId;
	/** The result data. */
	dictionary <string, string> row;
}


// Template style definition (for reference only)
// Response to the StartQuery or GetNextBatch event.
// These will be sent in order until the number requested in the rowLimit
// specified in either the StartQuery or GetNextBatch has been sent.
// No additional ResultEvents will be sent until the next GetNextBatch.
// If rowLimit=0, events will be streamed as fast as correlator can accept.
//
// event Historical<T>Event {
//	/** Unique id for this query. */
//	integer messageId;
//	/** The service id, Adapter channel name. */
//	string serviceId;
// 	float timestamp;	* Time stamp for all events in eventData (milliseconds)
// 	<T> event;			* Event requested
// }
//


/**
* Request that a running query be stopped.
*
* StopQuery will succeed if the query operation has already been stopped.
*
* See also: StartQuery(), QueryDone()
*
* Dataplayer: <br>
*     - Direction: Client to correlator
*/
event StopQuery {
	/** Unique id of the query to stop. */
	integer messageId;
	/** The service id, Adapter channel name. */
	string serviceId;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Request that a running query be paused.
*
* When the query is paused, a BatchDone is returned.  To restart a query
* send get next batch.  Note, it is possible to send a PauseQuery and get
* a QueryDone if the query is completed before the PauseQuery arrives.
*
* See also: StartQuery(), StopQuery(), QueryDone()
*
* Dataplayer: <br>
*     - Direction: Client to correlator
*/
event PauseQuery {
	/** Unique id for this query. */
	integer messageId;
	/** The service id, Adapter channel name. */
	string serviceId;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Sent to the adapter to request the next rowLimit set of events from
* a running query.
*
* Must be sent after a BatchDone message to get additional events from 
* the query. If conflicting settings are used (both batchSize and
* runUntilTime non-zero) a BatchDone with reason invalidGetNextBatch will be
* returned.
*
* See also: StartQuery(), QueryDone(), BatchDone()
*
* Dataplayer: <br>
*     - Direction: Client to correlator
*/
event GetNextBatch {
	/** Unique id for this query. */
	integer messageId;
	/** The service id, Adapter channel name. */
	string serviceId;
	/** Generate a status event every n seconds. If n <= 0.0 no status event is generated. */
	float statusFrequency;
	/** Max events to send before waiting for GetNextBatch. */
	integer batchSize;
	/** If true the batch size will count &Time events, if false it will not. */
	boolean batchSizeIncludesTimeEvents;
	/** Speed for back testing: <= 0.0, as fast as possible, > 0.0 -> some multiple of playback speed. */
	float backtestSpeed;
	/** Stop the query and send a BatchDone in the event time >= runUntilTime. */
	float runUntilTime;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Returned when the batch (specified by rowLimit) is completed.
*
* See also: StartQuery(), QueryDone(), GetNextBatch()
*
* Dataplayer: <br>
*     - Direction: Correlator to client
*
*     Channel: <verbatim><</verbatim>serviceId<verbatim>></verbatim>.Control
*/
event BatchDone
{
	/** Unique id for this query. */
	integer messageId;
	/** The service id. */
	string serviceId;
	/** Reason the batch is done (e.g. eventLimit, timeLimit, paused, invalidGetNextBatch). */
	string reason;
	/** Number of events sent so far. */
	integer eventCount;
	/** Timestamp of last event sent. */
	float lastEventTime;
	/** Last event sent. */
	string lastEvent; 
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Query is done.  Message is NULL if the query has succeeded, not NULL if an
* error has occurred.
* 
* See also: StartQuery(), BatchDone(), GetNextBatch()
*
* Dataplayer: <br>
*     - Direction: Correlator to client
*
*     Channel: <verbatim><</verbatim>serviceId<verbatim>></verbatim>.Control
*/
event QueryDone {
	/** Unique id for this query. */
	integer messageId;
	/** The service id. */
	string serviceId;
	/** Blank on success, otherwise the error message. */
	string errorMessage;
	/** Total number of events sent. */
	integer eventCount;
	/** Timestamp of last event. */
	float lastEventTime;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Request that all running queries on all databases be stopped.
* StopAllQueries will succeed if all queries have already been stopped.
*
* Response event: StopAllQueriesAck; errorMessage is blank on success, or
* otherwise a description of the error.
*
* See also: StartQuery(), PauseDone(), StopAllQueriesAck()
*
* Dataplayer: <br>
*     - Direction: Client to correlator
*/
event StopAllQueries
{
	/** Unique id for this query. */
	integer messageId;
	/** The service id. */
	string serviceId;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* The acknowledgement event for StopAllQueries.
* 
* See also: StopAllQueries()
*
* Dataplayer: <br>
*     - Direction: Correlator to client
*
*     Channel: <verbatim><</verbatim>serviceId<verbatim>></verbatim>.Control
*/
event StopAllQueriesAck
{
	/** Unique id for this query. */
	integer messageId;
	/** The service id. */
	string serviceId;
	/** Blank on success, otherwise the error message. */
	string errorMessage;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}


////////////////////////////////////////////////////////////////////////////////
//
// Store events
//
////////////////////////////////////////////////////////////////////////////////

/**
* Sent to the adapter to store the event contained in the eventData field.
*
* The acknowledge field indicates if a StoreOperationAck should be sent. 
* If true any errors will be returned in the errorMessage
* field of StoreOperationAck, a StoreOperationError will not be sent.
*
* Error only response event: StoreOperationError; errorMessage is equal
* to "Invalid database id". or otherwise a description of the error.
*
* Optional response event: StoreOperationAck; errorMessage is blank on success,
* equal to "Invalid database id" or otherwise a description of the error.
*
*/
event StoreEvent {
	/** Storage message id. The messages next message id must be increasing. */
	integer messageId;	
	/** The service id. */
	string serviceId;
	/** Id of database to store into. */
	integer databaseId;
	/** Used as store time (seconds). */
	float timestamp;
	/** Raw event (stringified). */
	string eventData;
	/** If true a StoreOperationAck should be sent, therefore any errors will be returned in the errorMessage. */
	boolean acknowledge;
	/** The table to store the event in. */
	string tableName;
	/** Optional user defined statement to use. */
	string statementName;	
	/** Column for event timestamp, not needed if column name is "time". */
	string timeColumn;
	/** Any additional settings. */
	dictionary<string,string> extraParams;
 
}

/**
* Sent to adapter to store the data in the fieldValues field.
*
* The acknowledge field indicates if a StoreOperationAck should be sent.
* If true any errors will be returned in the errorMessage
* field of StoreOperationAck, a StoreOperationError will not be sent.
*
* Error only response event: StoreOperationError; errorMessage is equal
* to "Invalid database id". or otherwise a description of the error
*
* Optional response event: StoreOperationAck; errorMessage is blank on success,
* equal to "Invalid database id" or otherwise a description of the error
*/
event StoreData {
	/** Storage message id. */
	integer messageId;	
	/** The service id. */
	string serviceId;
	/** Id of database to store into. */
	integer databaseId;
	/** If true a StoreOperationAck should be sent, therefore any errors will be returned in the errorMessage. */
	boolean acknowledge;
	/** The table to store the data in. */
	string tableName;
	/** Optional user defined statement to use. */
	string statementName;
	/** The column values to be stored. */
	dictionary<string,string> fieldValues;
	/** Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* Request that the IAF commit all outstanding event store or updates
* return CommitAck when the commit is completed.
*/
event CommitRequest
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service Id. */
	string serviceId;
	/** The database Id to commit to. */
	integer databaseId;	
	/** This is currently a container to allow additional information in the future. */
	dictionary<string, string> extraParams;
}

/**
* The acknowledgement event for Commit.
*/
event CommitAck
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service Id. */
	string serviceId;
	/** The database Id to commit to. */
	integer databaseId;
	/** Populated with an errorMessage if there is one. */
	string	errorMessage;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* Request that the IAF rollback all outstanding event store or updates
* return RollbackAck when the rollback is completed.
*/
event RollbackRequest
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service id. */
	string serviceId;
	/** The database Id to rollback data on. */
	integer databaseId;	
	/** This is currently a container to allow additional information in the future. */
	dictionary<string, string> extraParams;
}


/**
* The acknowledgement event for Rollback.
*/
event RollbackAck
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service id. */
	string serviceId;
	/** The database Id to rollback data on. */
	integer databaseId;
	/** Populated with an errorMessage if there is one. */
	string	errorMessage;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* The error notification event for store operations.
*
* The event is send whenever an event cannot be stored. If a large number of
* events cannot be stored, the IAF can send back a set of failures in a single
* message by setting the lowest and highest message id.
*/
event StoreOperationError
{	
	/** The service Id. */
	string serviceId;
	/** The database id for operation. */
	integer databaseId;
	/** Populated with an errorMessage if there is one. */
	string errorMessage; 
	/** Start of failed store event set. */
	integer lowestMessageId;
	/** End of failed store event set. */
	integer highestMessageId;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* The acknowledgement event for StoreEvent or StoreData.
*
* A StoreOperationError will not be sent on error when acknowledge=true.
*
*/
event StoreOperationAck
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service Id. */
	string serviceId;
	/** The database id for operation. */
	integer databaseId;
	/** Populated with an errorMessage if there is one. */
	string errorMessage;	
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* Sent to adapter to create a custom insert/update statement for use
* with the store events.
*
* See also: StoreStatementAck()
*
* Response event: StoreStatementAck(); errorMessage is blank on success,
* or otherwise a description of the error
*/
event CreateStoreStatement
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service Id. */
	string serviceId;
	/** The database id for operation. */
	integer databaseId;
	/** Optional name, generated if not provided. */
	string name;
	/** The table to create the statement against. */
	string tableName;
	/** SQL insert/update statement. */
	string statementString;
	/** Optional input parameter types. */
	sequence<string> inputParamTypes;
	/** Parameter number to event field/data dictionary name mapping. */
	dictionary<integer,string> 	inputToNameMap;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
*Sent to adapter to delete a custom insert/update statement for use
*with the store events.
*
*See also: StoreStatementAck()
*
*Response event: StoreStatementAck(); errorMessage is blank on success,
*or otherwise a description of the error
*/
event DeleteStoreStatement
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service Id. */
	string serviceId;
	/** The database id. */
	integer databaseId;
	/** The statement to be deleted. */
	string statementName;
	/** The name of the table the statement was created against. */
	string tableName;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* The acknowledgement event for CreateStoreStatement or DeleteStoreStatement.
*
* See also: CreateStoreStatement and DeleteStoreStatement
*/
event StoreStatementAck
{
	/** Unique request-response matching number. */
	integer messageId;	
	/** The service Id. */
	string serviceId;
	/** The database id. */
	integer databaseId;
	/** Name of insert/update statement. */
	string statementName;
	/** Blank on success, otherwise the error message. */
	string errorMessage;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

////////////////////////////////////////////////////////////////////////////////
//
// Database command events
//
////////////////////////////////////////////////////////////////////////////////

/**
* Sent to adapter to perform maintenance operations such as update or delete.
*
* Response event: CommandAck; errorMessage is blank on success, or
* otherwise a description of the error
*/
event Command {
	/** Unique request-response matching number. */
	integer messageId;
	/** The service id. */
	string serviceId;
	/** The database id. */
	integer databaseId;
	/** Command string (Update, delete, etc.). */
	string operationString;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

/**
* The acknowledgement event for Command.
*/
event CommandAck
{
	/** Unique request-response matching number. */
	integer messageId;
	/** The service id. */
	string serviceId;
	/** Populated with an errorMessage if there is one. */
	string errorMessage;
	/** This is currently a container to allow additional information in the future. */
	dictionary<string,string> extraParams;
}

// Query flow control event
/** @private */
event FlowControl {
	string channel;
	integer value;
}

//
//Parallel execution support events 
//

/** @private */
event RequestDatabasesToContext {
	context instanceContext;
	RequestDatabases reqDbs;
}

/** @private */
event RequestDataSourcesToContext {
	context instanceContext;
	RequestDataSources reqDss;
}

/** @private */
event RequestNamedQueriesToContext {
	context instanceContext;
	RequestNamedQueries reqNqs;
}

/** @private */
event OpenDatabaseToContext {
	context instanceContext;
	OpenDatabase open;
}

/** @private */
event CloseDatabaseToContext {
	context instanceContext;
	CloseDatabase close;
}

/** @private */
event CreateStoreStatementToContext {
	context instanceContext;
	 CreateStoreStatement createStoreStatement;
}

/** @private */
event DeleteStoreStatementToContext {
	context instanceContext;
	DeleteStoreStatement deleteStoreStatement;
}

/** @private */
event StoreEventToContext {
	context instanceContext;
	StoreEvent storeEvent;
}

/** @private */
event StoreDataToContext {
	context instanceContext;
	StoreData storeData;
}

/** @private */
event CommandToContext {
	context instanceContext;
	Command command;
}

/** @private */
event CommitRequestToContext {
	context instanceContext;
	CommitRequest commit;
}

/** @private */
event RollbackRequestToContext {
	context instanceContext;
	RollbackRequest rollback;
}

/** @private */
event StartQueryToContext {
	context instanceContext;
	StartQuery query;
}

/** @private */
event GetNextBatchToContext {
	context instanceContext;
	GetNextBatch next;
}

/** @private */
event PauseQueryToContext {
	context instanceContext;
	PauseQuery pause;
}

/** @private */
event StopQueryToContext {
	context instanceContext;
	StopQuery stop;
}

/** @private */
event StopAllQueriesToContext {
	context instanceContext;
	StopAllQueries stopAll;
}

/** @private */
event CreatePreparedQueryToContext {
	context instanceContext;
	CreatePreparedQuery createPreparedQuery;
}

/** @private */
event DeletePreparedQueryToContext {
	context instanceContext;
	DeletePreparedQuery deletePreparedQuery;
}


//
// Configuration events for scenario update and event persistence 
//

/**
* Request to create an instance of a configuration defining how an event
* or the updates to a scenario will be stored to a database. 
*
* See also: CreateStoreConfigurationInContext, StoreConfigurationAck()
*
* Response event: StoreConfigurationAck; errorMessage is blank on success,
* or otherwise a description of the error.
*/
event CreateStoreConfiguration {	
	/** Unique request-response matching number. */
	integer messageId;
	/** The service name. */
	string serviceName;
	/**  Valid types: event, scenario, or dataview. */
	string eventSourceType;	
	/** Event, Scenario, or Dataview name/Id. */
	string name; 		
	/** The database to create it on. */
	string DbUrl;
	/** The user Id for the database (if required). */
	string userName;
	/** The password to the database (if required). */
	string password;
	/** The table to create the store configuration against. */
	string tableName;
	/** The name of the database table's column that contains the event's time stamp. */
	string timeColumn;
	/** Auto-commit mode.
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.
	*/
	string autoCommit;
	/** Only store updates on this frequency (seconds).*/
	float throttlePeriod;
	/** If parallelexecution is allowed set to true. */
	boolean parallelExecution;
	/** For scenarios only. */
	dictionary<string,string> varToColumnName;
	/** For scenarios only. */
	dictionary<string,integer> columnToOutputIndex;
	/** Automatically start the store configuration. */
	boolean autoStart;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* Request to create an instance of a configuration defining how an event
* or the updates to a scenario will be stored to a database.
* 
* This event allows the requesting context and the context for
* the monitor instance persisting events to be specified.
*
* See also: CreateStoreConfiguration, StoreConfigurationAck
*
* Response event: StoreConfigurationAck; errorMessage is blank on success,
* or otherwise a description of the error.
*
* @private 
*/
event CreateStoreConfigurationInContext {	
	/** Unique request-response matching number. */
	integer messageId;
	/** The service name. */
	string serviceName;
	/**  Valid types: event, scenario, or dataview. */
	string eventSourceType;	
	/** Event, Scenario, or Dataview name. */
	string name; 
	/** The database to create it on. */
	string DbUrl;
	/** The user Id for the database (if required). */
	string userName;
	/** The password to the database (if required). */
	string password;
	/** The table to create the store configuration against. */
	string tableName;
	/** The name of the database table's column that contains the event's time stamp. */
	string timeColumn;
	/** Auto-commit mode: (case insensitive values).
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.
	*/
	string autoCommit;
	/** Only store updates on this frequency (seconds).*/
	float throttlePeriod;
	/** A handle to this context for parallel execution. */
	context thisContext;
	/** A handle to the store context for parallel execution. */
	context storeContext;
	/** For scenarios only. */
	dictionary<string,string> varToColumnNames;
	/** For scenarios only. */
	dictionary<string,integer> columnToOutputIndex;
	/** Automatically start the store configuration. */
	boolean autoStart;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* Request to start an instance of a store configuration.
*
* Once the configuration is ready a StoreConfigurationReady
* will be sent.  On recovery from an adapter down condition a
* StoreConfigurationReady will be sent again when the configuration
* has reconnected and is again ready.
*
* See also: StoreConfigurationAck, StoreConfigurationReady
*
* Response event: StoreConfigurationAck; errorMessage is blank on success,
* or otherwise a description of the error.
*/
event StartStoreConfiguration {	
	/** Unique request-response matching number. */
	integer messageId;
	/** Configuration instance. */
	integer storeConfigId;
	/** Event, Scenario, or Dataview name. */
	string name;
	/** Seconds to wait for operation to complete. */
	float  timeOut;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* Notification that the store configuration is ready 
* to accept events for storage.
*
* See also: StartStoreConfiguration
*/
event StoreConfigurationReady {	
	/** Unique request-response matching number. */
	integer messageId;
	/** Configuration instance. */
	integer storeConfigId;
	/** Event, Scenario, or Dataview name. */
	string name;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* Request to stop an instance of a store configuration.
*
* See also: StoreConfigurationAck
*
* Response event: StoreConfigurationAck; errorMessage is blank on success,
* or otherwise a description of the error
*/
event StopStoreConfiguration {	
	/** Unique request-response matching number. */
	integer messageId;
	/** Configuration instance. */
	integer storeConfigId;
	/** Event, Scenario, or Dataview name. */
	string name;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
*Request to stop an instance of a store configuration.
*
* See also: StoreConfigurationAck
*
* Response event: StoreConfigurationAck(); errorMessage is blank on success,
* or otherwise a description of the error
*/
event DeleteStoreConfiguration {
	/** Unique request-response matching number. */	
	integer messageId;
	/** Configuration instance. */
	integer storeConfigId;
	/** Event, Scenario, or Dataview name. */
	string name;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}

/**
* Response to store configuration request.
*
* See also: CreateStoreConfiguration, StartStoreConfiguration,
* StopStoreConfiguration, DeleteStoreConfiguration.
*
* ErrorMessage is blank on success, or otherwise a description of the error.
*/
event StoreConfigurationAck {	
	/** Unique request-response matching number. */
	integer messageId;
	/** Configuration instance. */
	integer storeConfigId;
	/** Context used if parallelExecution requested. */
	context storeContext;
	/** Event, Scenario, or Dataview name. */
	string name;
	/** Blank on success, otherwise the error message. */
	string errorMessage;
	/**  Any additional settings. */
	dictionary<string,string> extraParams;
}
 0000003b C:\SoftwareAG\Apama\adapters\monitors\ADBCAdapterEvents.mon
MONF 00008458 package com.apama.database;

//*****************************************************************************
// ADBC (Apama Database Connector) Adapter Service.
//
// For use with the ADBC adapter:
//    1) Forwards Database request events to the adapter.
//    2) Forwards Database response events to the Java ADBC Service API layer.
//    3) Provides support for parallel execution of blocks and event actions.
//
// $Copyright(c) 2009-2012 Progress Software Corporation (PSC). All rights reserved.$
// $Copyright (c) 2013-2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
// Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
//
// Author:   Frank Stoessel, Aston Chan
// Version:  1.0
// Requires: ADBCAdapterEvents.mon.
//
//*****************************************************************************

/**
* For use with the ADBC adapter.
* 
*    1) Forwards Database request events to the adapter.<br>
*    2) Forwards Database response events to the Java ADBC Service API layer.<br>
*    3) Provides support for parallel execution of blocks and event actions.
*
* @routes RequestDatabases, RequestDataSources, RequestNamedQueries, OpenDatabase, 
*		CloseDatabase, CreateStoreStatement, DeleteStoreStatement, StoreEvent, StoreData, 
*		CommitRequest, RollbackRequest, Command, StartQuery, GetNextBatch, PauseQuery, 
*		StopQuery, StopAll, CreatePreparedQuery, DeletePreparedQuery.
*
* @listens StopAllQueriesAck, StoreOperationError, StoreOperationAck, CommandAck, CommitAck,
*		RequestDataSources, RequestNamedQueries, RequestDatabases, OpenDatabase, CloseDatabase,
*		StartQuery, StopQuery, PauseQuery, GetNextBatch, StopAllQueries, CreatePreparedQuery,
*		DeletePreparedQuery, StoreEvent, StoreData, CreateStoreStatement, DeleteStoreStatement,
*		CommitRequest, RollbackRequest, Command, DataSources, NamedQueries, Databases, 
*		DatabaseOperationAck, QueryStatus, BatchDone, QueryDone, FlowControl, RequestDatabasesToContext,
*		RequestDataSourcesToContext, RequestNamedQueriesToContext, OpenDatabaseToContext, 
*		CloseDatabaseToContext, StoreEventToContext, StoreDataToContext, CreateStoreStatementToContext, 
*		DeleteStoreStatementToContext, CommandToContext, CommitRequestToContext, RollbackRequestToContext,
*		StartQueryToContext, GetNextBatchToContext, PauseQueryToContext, StopQueryToContext, 
*		StopAllQueriesToContext, CreatePreparedQueryToContext, DeletePreparedQueryToContext,
*		PreparedQueryAck, ResultSchema, ResultEvent.
*
*/
monitor ADBCAdapterService
{
	string ADBC_CHANNEL := "com.apama.adbc";

	// Used by the Java ADBC Service API layer
	string ADBC_SERVICE_CHANNEL := ADBC_CHANNEL + ".service";
	string ADBC_SERVICE_CONTROL_CHANNEL_SUFFIX := ".Control";
	string ADBC_SERVICE_STATUS_CHANNEL_SUFFIX := ".Status";

	// Used to persist parallel execution listeners
	/** @private */
	dictionary < integer, context > eventRouter;
	/** @private */
	dictionary < integer, listener > opAckListeners;
	/** @private */
	dictionary < context, listener > storeErrorListeners;
	/** @private */
	dictionary < integer, sequence<listener > > queryListeners;

	// Request events that are forwarded to the adapter
	//

	/** @private */
	RequestDataSources		requestDatasources;
	/** @private */
	RequestNamedQueries		requestNamedQueries;
	/** @private */
	RequestDatabases		requestDatabases;

	/** @private */
	OpenDatabase			openDatabase;
	/** @private */
	CloseDatabase			closeDatabase;

	/** @private */
	StartQuery				startQuery;
	/** @private */
	StopQuery				stopQuery;
	/** @private */
	PauseQuery				pauseQuery;
	/** @private */
	GetNextBatch			getNextBatch;
	/** @private */
	StopAllQueries			stopAllQueries;

	/** @private */
	CreatePreparedQuery		createPreparedQuery;
	/** @private */
	DeletePreparedQuery		deletePreparedQuery;
	
	/** @private */
	StoreEvent				storeEvent;
	/** @private */
	StoreData				storeData;
	/** @private */
	CreateStoreStatement	createStoreStatement;
	/** @private */
	DeleteStoreStatement	deleteStoreStatement;
	/** @private */
	CommitRequest			commitRequest;
	/** @private */
	RollbackRequest			rollbackRequest;

	/** @private */
	Command			 		command;


	// Response Events from adapter that get logged
	// and/or forwarded to the Java ADBC Service API layer
	/** @private */
	DataSources				datasources;
	/** @private */
	NamedQueries			namedQueries;
	/** @private */
	Databases				databases;

	/** @private */
	DatabaseOperationAck	databaseOperationAck;

	/** @private */
	QueryStatus				queryStatus;
	/** @private */
	BatchDone				batchDone;
	/** @private */
	QueryDone				queryDone;
	/** @private */
	StopAllQueriesAck		stopAllQueriesAck;

	/** @private */
	PreparedQueryAck		preparedQueryAck;

	/** @private */
	StoreOperationAck		storeOperationAck;
	/** @private */
	StoreOperationError		storeOperationError;
	/** @private */
	CommitAck				commitAck;

	/** @private */
	CommandAck				commandAck;

	/** @private */
	FlowControl 			c;


	/**
	* Sets up the listeners and the forwarders.
	*
	* @listens StopAllQueriesAck, StoreOperationError, StoreOperationAck, CommandAck, CommitAck
	*/
	action onload()
	{
		log "Loaded ADBC adapter service" at INFO;
		
		// Forwarders to the adapter and Java service layer
		setupForwarders();

		// Forwarders to support parallel execution
		setupContextForwarders();

		// Log certain critical errors, just in case they're not
		// handled by the user

		on all StopAllQueriesAck():stopAllQueriesAck {
			if stopAllQueriesAck.errorMessage != "" then {
				log ("Service '" + stopAllQueriesAck.serviceId +
				     "' - Failed to stop running queries: " +
				     stopAllQueriesAck.errorMessage +
					 " (message id "+stopAllQueriesAck.messageId.toString()+")") at WARN;
			}
		}

		on all StoreOperationError():storeOperationError {
			log ("Service '" + storeOperationError.serviceId +
				 "' database id" + storeOperationError.databaseId.toString() +
				 " - Store error: " + storeOperationError.errorMessage +
				 " (message ids " + storeOperationError.lowestMessageId.toString() +
				 " to " + storeOperationError.highestMessageId.toString() + ")") at WARN;
		}

		on all StoreOperationAck():storeOperationAck {
			if storeOperationAck.errorMessage != "" then {
				log ("Service '" + storeOperationAck.serviceId +
					 "' - Store error: " + storeOperationAck.errorMessage +
					 " (message id "+storeOperationAck.messageId.toString()+")") at WARN;
			}
		}

		on all CommandAck():commandAck {
			if commandAck.errorMessage != "" then {
				log ("Service '" + commandAck.serviceId +
				     "' - Command error: " + commandAck.errorMessage +
					 " (message id "+commandAck.messageId.toString()+")") at WARN;
			}
		}

		on all CommitAck():commitAck {
			if commitAck.errorMessage != "" then {
				log ("Service '" + commitAck.serviceId +
				     "' - Commit error: " +	commitAck.errorMessage +
					 " (message id "+commitAck.messageId.toString()+")") at WARN;
			}
		}
	}

	/**
	* Set up all listeners to forward things onto the 
	* appropriate channels.
	*/
	action setupForwarders()
	{
		// Request events sent to ADBC adapter

		on all RequestDataSources(): requestDatasources
		{ send requestDatasources to ADBC_CHANNEL; }

		on all RequestNamedQueries(): requestNamedQueries
		{ send requestNamedQueries to requestNamedQueries.serviceId; }

		on all RequestDatabases(): requestDatabases
		{ send requestDatabases to requestDatabases.serviceId; }

		on all OpenDatabase(): openDatabase
		{ send openDatabase to openDatabase.serviceId; }

		on all CloseDatabase(): closeDatabase
		{ send closeDatabase to closeDatabase.serviceId; }

		on all StartQuery(): startQuery
		{ send startQuery to startQuery.serviceId; }

		on all StopQuery(): stopQuery
		{ send stopQuery to stopQuery.serviceId; }

		on all PauseQuery(): pauseQuery
		{ send pauseQuery to pauseQuery.serviceId; }

		on all GetNextBatch(): getNextBatch
		{ send getNextBatch to getNextBatch.serviceId; }

		on all StopAllQueries(): stopAllQueries
		{ send stopAllQueries to stopAllQueries.serviceId; }

		on all CreatePreparedQuery(): createPreparedQuery
		{ send createPreparedQuery to createPreparedQuery.serviceId; }

		on all DeletePreparedQuery(): deletePreparedQuery
		{ send deletePreparedQuery to deletePreparedQuery.serviceId; }

		on all StoreEvent(): storeEvent
		{ send storeEvent to storeEvent.serviceId; }

		on all StoreData(): storeData
		{ send storeData to storeData.serviceId; }

		on all CreateStoreStatement(): createStoreStatement
		{ send createStoreStatement to createStoreStatement.serviceId; }

		on all DeleteStoreStatement(): deleteStoreStatement
		{ send deleteStoreStatement to deleteStoreStatement.serviceId; }

		on all CommitRequest(): commitRequest
		{ send commitRequest to commitRequest.serviceId; }

		on all RollbackRequest(): rollbackRequest
		{ send rollbackRequest to rollbackRequest.serviceId; }

		on all Command(): command
		{ send command to command.serviceId; }


		// Response events sendted to Java ADBC Service API layer 

		on all DataSources(): datasources
		{ send datasources to ADBC_SERVICE_CHANNEL; }

		on all NamedQueries(): namedQueries
		{ send namedQueries to namedQueries.serviceId + ADBC_SERVICE_CONTROL_CHANNEL_SUFFIX; }

		on all Databases(): databases
		{ send databases to databases.serviceId + ADBC_SERVICE_CONTROL_CHANNEL_SUFFIX; }

		on all DatabaseOperationAck(): databaseOperationAck
		{
			if databaseOperationAck.errorMessage != "" then
			{
				log ("Service '" + databaseOperationAck.serviceId +
				     "' - Database error: " + databaseOperationAck.errorMessage +
					 " (message id "+databaseOperationAck.messageId.toString()+")") at ERROR;
			}
			send databaseOperationAck to databaseOperationAck.serviceId + ADBC_SERVICE_CONTROL_CHANNEL_SUFFIX;
		}

		on all QueryStatus() : queryStatus
		{ send queryStatus to queryStatus.serviceId + ADBC_SERVICE_STATUS_CHANNEL_SUFFIX; }

		on all BatchDone():batchDone
		{ send batchDone to batchDone.serviceId + ADBC_SERVICE_CONTROL_CHANNEL_SUFFIX; }

		on all QueryDone():queryDone
		{
			if queryDone.errorMessage != "" and queryDone.errorMessage.find("Query incomplete") < 0 then
			{
				log ("Service '" + queryDone.serviceId +
					 "' - Query error: " + queryDone.errorMessage +
					 " (message id "+queryDone.messageId.toString()+")") at ERROR;
			}
			send queryDone to queryDone.serviceId + ADBC_SERVICE_CONTROL_CHANNEL_SUFFIX;
		}

		on all FlowControl() : c 
		{ send c to c.channel; }
	}
	
	/**
	* Set up all listeners to forward things onto the 
	* appropriate channels in the correct context.
	*
	* For use with parallel processing.
	*/	
	action setupContextForwarders()
	{
		RequestDatabasesToContext reqDbs;
		on all RequestDatabasesToContext():reqDbs requestDbsEventHandler(reqDbs);
	
		RequestDataSourcesToContext reqDss;
		on all RequestDataSourcesToContext():reqDss requestDssEventHandler(reqDss);
	
		RequestNamedQueriesToContext reqNqs;
		on all RequestNamedQueriesToContext():reqNqs requestNqsEventHandler(reqNqs);
	
		OpenDatabaseToContext open;
		on all OpenDatabaseToContext():open openDbEventHandler(open);
	
		CloseDatabaseToContext close;
		on all CloseDatabaseToContext():close closeDbEventHandler(close);
	
		StoreEventToContext storeEvent;
		on all StoreEventToContext():storeEvent storeEventHandler(storeEvent);
		
		StoreDataToContext store;
		on all StoreDataToContext():store storeDataEventHandler(store);
		
		CreateStoreStatementToContext createStatement;
		on all CreateStoreStatementToContext():createStatement createStatementEventHandler(createStatement);

		DeleteStoreStatementToContext deleteStatement;
		on all DeleteStoreStatementToContext():deleteStatement deleteStatementEventHandler(deleteStatement);

		CommandToContext command;
		on all CommandToContext():command commandEventHandler(command);
	
		CommitRequestToContext commit;
		on all CommitRequestToContext():commit commitEventHandler(commit);
	
		RollbackRequestToContext rollback;
		on all RollbackRequestToContext():rollback rollbackEventHandler(rollback);
	
		StartQueryToContext query;
		on all StartQueryToContext():query startQueryEventHandler(query);
	
		GetNextBatchToContext next;
		on all GetNextBatchToContext():next getNextBatchEventHandler(next);
	
		PauseQueryToContext pause;
		on all PauseQueryToContext():pause pauseQueryEventHandler(pause);
	
		StopQueryToContext stop;
		on all StopQueryToContext():stop stopQueryEventHandler(stop);
	
		StopAllQueriesToContext stopAll;
		on all StopAllQueriesToContext():stopAll stopAllQueriesEventHandler(stopAll);

		CreatePreparedQueryToContext createPQuery;
		on all CreatePreparedQueryToContext():createPQuery createPreparedQueryEventHandler(createPQuery);

		DeletePreparedQueryToContext deletePQuery;
		on all DeletePreparedQueryToContext():deletePQuery deletePreparedQueryEventHandler(deletePQuery);
	}

	/**
	* Handle any cleanup.
	*/
	action ondie() {
		if (eventRouter.size() != 0) then {
			log "ondie: ADBC adapter service is terminating but there may still be contexts subscribed to response events" at ERROR;
		}
		else {
			log "ondie: ADBC adapter service is terminating" at INFO;
		}
	}

	action onunload() {
		log "Unloaded ADBC adapter service" at INFO;
	}


	/**
	* Request databases with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The RequestDatabasesToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action requestDbsEventHandler(RequestDatabasesToContext evt)
	{
		listener l;
		Databases dbs;
		
		eventRouter.add(evt.reqDbs.messageId, evt.instanceContext);
	
		l := on Databases(messageId=evt.reqDbs.messageId):dbs {

			if eventRouter.hasKey(evt.reqDbs.messageId) then {
				send dbs to eventRouter[evt.reqDbs.messageId];
			}
			
			if opAckListeners.hasKey(evt.reqDbs.messageId) then {
				opAckListeners[evt.reqDbs.messageId].quit();
				opAckListeners.remove(evt.reqDbs.messageId);
			}
			if eventRouter.hasKey(evt.reqDbs.messageId) then {
				eventRouter.remove(evt.reqDbs.messageId);
			}
		}
		
		opAckListeners.add(evt.reqDbs.messageId, l);

		route evt.reqDbs;
	}

	/**
	* Request database sources with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The RequestDataSourcesToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action requestDssEventHandler(RequestDataSourcesToContext evt)
	{
		listener l;
		DataSources dss;
		
		eventRouter.add(evt.reqDss.messageId, evt.instanceContext);
	
		l := on all DataSources(messageId=evt.reqDss.messageId):dss {

			if eventRouter.hasKey(evt.reqDss.messageId) then {
				send dss to eventRouter[evt.reqDss.messageId];
			}
		}

		// Wait 5 seconds for all data sources to respond
		on wait(5.0) {				
			if opAckListeners.hasKey(evt.reqDss.messageId) then {
				opAckListeners[evt.reqDss.messageId].quit();
				opAckListeners.remove(evt.reqDss.messageId);
			}
			if eventRouter.hasKey(evt.reqDss.messageId) then {
				eventRouter.remove(evt.reqDss.messageId);
			}
		}
		
		opAckListeners.add(evt.reqDss.messageId, l);

		route evt.reqDss;
	}

	/**
	* Request names queries with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The RequestNamedQueriesToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action requestNqsEventHandler(RequestNamedQueriesToContext evt)
	{
		listener l;
		NamedQueries nqs;
		
		eventRouter.add(evt.reqNqs.messageId, evt.instanceContext);
	
		l := on NamedQueries(messageId=evt.reqNqs.messageId):nqs {

			if eventRouter.hasKey(evt.reqNqs.messageId) then {
				send nqs to eventRouter[evt.reqNqs.messageId];
			}
			
			if opAckListeners.hasKey(evt.reqNqs.messageId) then {
				opAckListeners[evt.reqNqs.messageId].quit();
				opAckListeners.remove(evt.reqNqs.messageId);
			}
			if eventRouter.hasKey(evt.reqNqs.messageId) then {
				eventRouter.remove(evt.reqNqs.messageId);
			}
		}
		
		opAckListeners.add(evt.reqNqs.messageId, l);

		route evt.reqNqs;
	}

	/**
	* Request to open a database with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The OpenDatabaseToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action openDbEventHandler(OpenDatabaseToContext evt)
	{
		listener l;
		DatabaseOperationAck openAck;
		
		eventRouter.add(evt.open.messageId, evt.instanceContext);
	
		l := on DatabaseOperationAck(messageId=evt.open.messageId):openAck {

			if eventRouter.hasKey(evt.open.messageId) then {
				send openAck to eventRouter[evt.open.messageId];
			}
			
			if opAckListeners.hasKey(evt.open.messageId) then {
				opAckListeners[evt.open.messageId].quit();
				opAckListeners.remove(evt.open.messageId);
			}
			if eventRouter.hasKey(evt.open.messageId) then {
				eventRouter.remove(evt.open.messageId);
			}
		}
		
		opAckListeners.add(evt.open.messageId, l);

		route evt.open;
	}

	/**
	* Request to close a database with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The CloseDatabaseToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action closeDbEventHandler(CloseDatabaseToContext evt)
	{
		listener l;
		DatabaseOperationAck closeAck;
		
		eventRouter.add(evt.close.messageId, evt.instanceContext);
	
		l := on DatabaseOperationAck(messageId=evt.close.messageId):closeAck {

			if eventRouter.hasKey(evt.close.messageId) then {
				send closeAck to eventRouter[evt.close.messageId];
			}
			
			if opAckListeners.hasKey(evt.close.messageId) then {
				opAckListeners[evt.close.messageId].quit();
				opAckListeners.remove(evt.close.messageId);
			}
			if eventRouter.hasKey(evt.close.messageId) then {
				eventRouter.remove(evt.close.messageId);
			}
		}
		
		opAckListeners.add(evt.close.messageId, l);

		route evt.close;
	}

	/**
	* Request to create a store statement with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The CreateStoreStatementToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action createStatementEventHandler(CreateStoreStatementToContext evt)
	{
		listener l;
		StoreStatementAck createStatementAck;
		
		eventRouter.add(evt.createStoreStatement.messageId, evt.instanceContext);
	
		l := on StoreStatementAck(messageId=evt.createStoreStatement.messageId):createStatementAck {

			if eventRouter.hasKey(evt.createStoreStatement.messageId) then {
				send createStatementAck to eventRouter[evt.createStoreStatement.messageId];
			}
			
			if opAckListeners.hasKey(evt.createStoreStatement.messageId) then {
				opAckListeners[evt.createStoreStatement.messageId].quit();
				opAckListeners.remove(evt.createStoreStatement.messageId);
			}
			if eventRouter.hasKey(evt.createStoreStatement.messageId) then {
				eventRouter.remove(evt.createStoreStatement.messageId);
			}
		}
		
		opAckListeners.add(evt.createStoreStatement.messageId, l);

		route evt.createStoreStatement;
	}

	/**
	* Request to delete a store statement with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The DeleteStoreStatementToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action deleteStatementEventHandler(DeleteStoreStatementToContext evt)
	{
		listener l;
		StoreStatementAck deleteStatementAck;
		
		eventRouter.add(evt.deleteStoreStatement.messageId, evt.instanceContext);
	
		l := on StoreStatementAck(messageId=evt.deleteStoreStatement.messageId):deleteStatementAck {

			if eventRouter.hasKey(evt.deleteStoreStatement.messageId) then {
				send deleteStatementAck to eventRouter[evt.deleteStoreStatement.messageId];
			}
			
			if opAckListeners.hasKey(evt.deleteStoreStatement.messageId) then {
				opAckListeners[evt.deleteStoreStatement.messageId].quit();
				opAckListeners.remove(evt.deleteStoreStatement.messageId);
			}
			if eventRouter.hasKey(evt.deleteStoreStatement.messageId) then {
				eventRouter.remove(evt.deleteStoreStatement.messageId);
			}
		}
		
		opAckListeners.add(evt.deleteStoreStatement.messageId, l);

		route evt.deleteStoreStatement;
	}

	/**
	* Request to store an event with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The StoreEventToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action storeEventHandler(StoreEventToContext evt)
	{
		listener l;
		listener m;
		StoreOperationAck storeAck;
		StoreOperationError storeError;
		
		if evt.storeEvent.acknowledge then {
			eventRouter.add(evt.storeEvent.messageId, evt.instanceContext);
			
			l := on StoreOperationAck(messageId=evt.storeEvent.messageId):storeAck {

				if eventRouter.hasKey(evt.storeEvent.messageId) then {
					send storeAck to eventRouter[evt.storeEvent.messageId];
				}
				
				if opAckListeners.hasKey(evt.storeEvent.messageId) then {
					opAckListeners[evt.storeEvent.messageId].quit();
					opAckListeners.remove(evt.storeEvent.messageId);
				}
				if eventRouter.hasKey(evt.storeEvent.messageId) then {
					eventRouter.remove(evt.storeEvent.messageId);
				}
			}
			
			opAckListeners.add(evt.storeEvent.messageId, l);
		}
		if not storeErrorListeners.hasKey(evt.instanceContext) then {
			m := on all StoreOperationError(serviceId=evt.storeEvent.serviceId):storeError {
	
				if storeErrorListeners.hasKey(evt.instanceContext) then {
					send storeError to evt.instanceContext;
				}
			}
			// Listener will not be deleted until monitor unload
			storeErrorListeners.add(evt.instanceContext, m);
		}

		route evt.storeEvent;
	}

	/**
	* Request to store data with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The StoreDataToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action storeDataEventHandler(StoreDataToContext evt)
	{
		listener l;
		listener m;
		StoreOperationAck storeAck;
		StoreOperationError storeError;
		
		if evt.storeData.acknowledge then {
			eventRouter.add(evt.storeData.messageId, evt.instanceContext);
			
			l := on StoreOperationAck(messageId=evt.storeData.messageId):storeAck {
	
				if eventRouter.hasKey(evt.storeData.messageId) then {
					send storeAck to eventRouter[evt.storeData.messageId];
				}
				
				if opAckListeners.hasKey(evt.storeData.messageId) then {
					opAckListeners[evt.storeData.messageId].quit();
					opAckListeners.remove(evt.storeData.messageId);
				}
				if eventRouter.hasKey(evt.storeData.messageId) then {
					eventRouter.remove(evt.storeData.messageId);
				}
			}
			
			opAckListeners.add(evt.storeData.messageId, l);
		}
		if not storeErrorListeners.hasKey(evt.instanceContext) then {
			m := on all StoreOperationError(serviceId=evt.storeData.serviceId):storeError {
	
				if storeErrorListeners.hasKey(evt.instanceContext) then {
					send storeError to evt.instanceContext;
				}
			}
			// Listener will not be deleted until monitor unload
			storeErrorListeners.add(evt.instanceContext, m);
		}

		route evt.storeData;
	}

	/**
	* Request to commit with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The CommitRequestToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action commitEventHandler(CommitRequestToContext evt)
	{
		listener l;
		CommitAck commitAck;
		
		eventRouter.add(evt.commit.messageId, evt.instanceContext);
	
		l := on CommitAck(messageId=evt.commit.messageId):commitAck {

			if eventRouter.hasKey(evt.commit.messageId) then {
				send commitAck to eventRouter[evt.commit.messageId];
			}
			
			if opAckListeners.hasKey(evt.commit.messageId) then {
				opAckListeners[evt.commit.messageId].quit();
				opAckListeners.remove(evt.commit.messageId);
			}
			if eventRouter.hasKey(evt.commit.messageId) then {
				eventRouter.remove(evt.commit.messageId);
			}
		}
		
		opAckListeners.add(evt.commit.messageId, l);

		route evt.commit;
	}	

	/**
	* Request to rollback with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The RollbackRequestToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action rollbackEventHandler(RollbackRequestToContext evt)
	{
		listener l;
		RollbackAck rollbackAck;
		
		eventRouter.add(evt.rollback.messageId, evt.instanceContext);
	
		l := on RollbackAck(messageId=evt.rollback.messageId):rollbackAck {

			if eventRouter.hasKey(evt.rollback.messageId) then {
				send rollbackAck to eventRouter[evt.rollback.messageId];
			}
			
			if opAckListeners.hasKey(evt.rollback.messageId) then {
				opAckListeners[evt.rollback.messageId].quit();
				opAckListeners.remove(evt.rollback.messageId);
			}
			if eventRouter.hasKey(evt.rollback.messageId) then {
				eventRouter.remove(evt.rollback.messageId);
			}
		}
		
		opAckListeners.add(evt.rollback.messageId, l);

		route evt.rollback;
	}	

	/**
	* Request a command against a database with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The CommandToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action commandEventHandler(CommandToContext evt)
	{
		listener l;
		CommandAck commandAck;
		
		eventRouter.add(evt.command.messageId, evt.instanceContext);
	
		l := on CommandAck(messageId=evt.command.messageId):commandAck {

			if eventRouter.hasKey(evt.command.messageId) then {
				send commandAck to eventRouter[evt.command.messageId];
			}
			
			if opAckListeners.hasKey(evt.command.messageId) then {
				opAckListeners[evt.command.messageId].quit();
				opAckListeners.remove(evt.command.messageId);
			}
			if eventRouter.hasKey(evt.command.messageId) then {
				eventRouter.remove(evt.command.messageId);
			}
		}
		
		opAckListeners.add(evt.command.messageId, l);

		route evt.command;
	}	

	/**
	* Request to start a query with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The StartQueryToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action startQueryEventHandler(StartQueryToContext evt)
	{
		listener l,m,n,o,p;
		ResultSchema schema;
		ResultEvent result;
		BatchDone batchDone;
		QueryStatus queryStatus;
		QueryDone queryDone;
		
		
		eventRouter.add(evt.query.messageId, evt.instanceContext);
	
		l := on all ResultSchema(messageId=evt.query.messageId):schema {

			if eventRouter.hasKey(evt.query.messageId) then {
				send schema to eventRouter[evt.query.messageId];
			}
		}
		m := on all ResultEvent(messageId=evt.query.messageId):result {

			if eventRouter.hasKey(evt.query.messageId) then {
				send result to eventRouter[evt.query.messageId];
			}
		}
		n := on all BatchDone(messageId=evt.query.messageId):batchDone {

			if eventRouter.hasKey(evt.query.messageId) then {
				send batchDone to eventRouter[evt.query.messageId];
			}
		}
		o := on all QueryStatus(messageId=evt.query.messageId):queryStatus {

			if eventRouter.hasKey(evt.query.messageId) then {
				send queryStatus to eventRouter[evt.query.messageId];
			}
		}
		p := on QueryDone(messageId=evt.query.messageId):queryDone {

			if eventRouter.hasKey(evt.query.messageId) then {
				send queryDone to eventRouter[evt.query.messageId];
			}
			
			if queryListeners.hasKey(evt.query.messageId) then {
				queryListeners[evt.query.messageId][0].quit();
				queryListeners[evt.query.messageId][1].quit();
				queryListeners[evt.query.messageId][2].quit();
				queryListeners[evt.query.messageId][3].quit();
				queryListeners[evt.query.messageId][4].quit();
				queryListeners.remove(evt.query.messageId);
			}
			if eventRouter.hasKey(evt.query.messageId) then {
				eventRouter.remove(evt.query.messageId);
			}
		}
		sequence<listener> q := [l,m,n,o,p];
		queryListeners.add(evt.query.messageId, q);

		route evt.query;
	}	

	/**
	* Request to get the next batch with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The GetNextBatchToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action getNextBatchEventHandler(GetNextBatchToContext evt)
	{
		// StartQuery already added router entry and listener
		route evt.next;
	}	

	/**
	* Request to pause a query with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The PauseQueryToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action pauseQueryEventHandler(PauseQueryToContext evt)
	{
		// StartQuery already added router entry and listener
		route evt.pause;
	}	

	/**
	* Request to stop a query with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The StopQueryToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action stopQueryEventHandler(StopQueryToContext evt)
	{
		// StartQuery already added router entry and listener
		route evt.stop;
	}	

	/**
	* Request to stop all queries with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The StopAllQueriesToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action stopAllQueriesEventHandler(StopAllQueriesToContext evt)
	{
		listener l;
		StopAllQueriesAck stopAllAck;
		
		eventRouter.add(evt.stopAll.messageId, evt.instanceContext);
	
		l := on StopAllQueriesAck(messageId=evt.stopAll.messageId):stopAllAck {

			if eventRouter.hasKey(evt.stopAll.messageId) then {
				send stopAllAck to eventRouter[evt.stopAll.messageId];
			}
			
			if opAckListeners.hasKey(evt.stopAll.messageId) then {
				opAckListeners[evt.stopAll.messageId].quit();
				opAckListeners.remove(evt.stopAll.messageId);
			}
			if eventRouter.hasKey(evt.stopAll.messageId) then {
				eventRouter.remove(evt.stopAll.messageId);
			}
		}
		
		opAckListeners.add(evt.stopAll.messageId, l);

		route evt.stopAll;
	}

	/**
	* Request to create a prepared queries with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The CreatePreparedQueryToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action createPreparedQueryEventHandler(CreatePreparedQueryToContext evt)
	{
		listener l;
		PreparedQueryAck pqueryAck;
		
		eventRouter.add(evt.createPreparedQuery.messageId, evt.instanceContext);
	
		l := on PreparedQueryAck(messageId=evt.createPreparedQuery.messageId):pqueryAck {

			if eventRouter.hasKey(evt.createPreparedQuery.messageId) then {
				send pqueryAck to eventRouter[evt.createPreparedQuery.messageId];
			}
			
			if opAckListeners.hasKey(evt.createPreparedQuery.messageId) then {
				opAckListeners[evt.createPreparedQuery.messageId].quit();
				opAckListeners.remove(evt.createPreparedQuery.messageId);
			}
			if eventRouter.hasKey(evt.createPreparedQuery.messageId) then {
				eventRouter.remove(evt.createPreparedQuery.messageId);
			}
		}
		
		opAckListeners.add(evt.createPreparedQuery.messageId, l);

		route evt.createPreparedQuery;
	}	

	/**
	* Request to delete a prepared queries with the response being sent to the correct context.
	*
	* Action to support parallel execution.
	*
	* @param evt The DeletePreparedQueryToContext request with information 
	*				where to send the response.
	*
	* @private
	*/
	action deletePreparedQueryEventHandler(DeletePreparedQueryToContext evt)
	{
		listener l;
		PreparedQueryAck pqueryAck;
		
		eventRouter.add(evt.deletePreparedQuery.messageId, evt.instanceContext);
	
		l := on PreparedQueryAck(messageId=evt.deletePreparedQuery.messageId):pqueryAck {

			if eventRouter.hasKey(evt.deletePreparedQuery.messageId) then {
				send pqueryAck to eventRouter[evt.deletePreparedQuery.messageId];
			}
			
			if opAckListeners.hasKey(evt.deletePreparedQuery.messageId) then {
				opAckListeners[evt.deletePreparedQuery.messageId].quit();
				opAckListeners.remove(evt.deletePreparedQuery.messageId);
			}
			if eventRouter.hasKey(evt.deletePreparedQuery.messageId) then {
				eventRouter.remove(evt.deletePreparedQuery.messageId);
			}
		}
		
		opAckListeners.add(evt.deletePreparedQuery.messageId, l);

		route evt.deletePreparedQuery;
	}	

}
 0000003c C:\SoftwareAG\Apama\adapters\monitors\ADBCAdapterService.mon
MONF 00011ff2 package com.apama.database;

//*****************************************************************************
// ADBC (Apama Database Connector) Events.
//
// Public event API for use with the ADBC adapter.
//
// $Copyright(c) 2009-2012 Progress Software Corporation (PSC). All rights reserved.$
// $Copyright (c) 2013-2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
// Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
//
// Author:   Frank Stoessel, Fred Cohen
// Version:  1.0
// Requires: ADBCAdapterEvents.mon, ADBCAdapterService.mon
//
//*****************************************************************************

using com.apama.database.GetNextBatchToContext;
using com.apama.database.RequestDataSourcesToContext;

/** Event containing the actions to perform discovery of ADBC
 * adapter resources.  This is used to find the available data
 * sources (JDBC, Sim, etc.) and the default databases and
 * query templates (named queries) configured for those data sources.
 *
 * See findAvailableDataSources(), getDatabases() and getQueryTemplates()
 * 
 * Also see findAvailableDataSourcesFull(), getDatabasesFull() and 
 * getQueryTemplatesFull() for versions of the calls that accept the complete
 * set of parameters.
 */
event Discovery{
	
	/**
	 * Set context that is current prior to spawning.
	 *
	 * Optional: Only needed when using parallel processing.
	 *
	 * @param  preSpawnContext Current context prior to spawning.
	 *
	 */
	action initPreSpawnContext(context preSpawnContext) {
		_preSpawnContext := preSpawnContext;
	}

	/**
	* Search for available DataSources.
	*
	* @param  timeout The find Datasources timesout.
	* @param  callback Callback action.
	*
	* See findAvailableDataSourcesFull()
	*/
	action findAvailableDataSources(
			float timeout, 
			action < string, sequence<DataSource> > callback )
	{
		findAvailableDataSourcesFull(timeout, new dictionary<string,string>, callback);
	}

	/**
	* Find the availale DataSources.
	*
	* @deprecated [This action has been deprecated due to the misleading name.
	* Please use findAvailableDataSources instead as it is identical in functionality
	* and a more acurately named action.]
	*/
	action findAvailableServers(
			float timeout, 
			action < string, sequence<DataSource> > callback )
	{
		log "findAvailableServers has now been deprecated, Please see ADBCEvents.mon for more details" at WARN;
		findAvailableDataSourcesFull(timeout, new dictionary<string,string>, callback);
	}

	/**
	* Get the available Databases on a serviceId, using the service Id(s) 
	* returned from findAvailableDataSources.
	*
	* @param  serviceId The serviceId to search on.
	* @param  user The user Id for the database (if required).
	* @param  password The password to the database (if required).
	* @param  callback Callback action.
	*
	* See getDatabasesFull()
	*/
	action getDatabases(
			string serviceId,
			string user, 
			string password,
			action< string, sequence<Database> > callback)
	{
		getDatabasesFull(serviceId, "", user, password, new dictionary<string,string>, callback);
	}

	/**
	* Provides details of all the loaded named queries.
	*
	* @param  serviceId The serviceId to search on.
	* @param  callback Callback action.
	*
	* see getNamedQueriesFull()
	*/
	action getNamedQueries(
			string serviceId, 
			action<	string,	sequence<NamedQuery> > callback)
	{
		getNamedQueriesFull(serviceId, new dictionary<string,string>, callback);
	}

	/**
	* Search for available DataSources, This action allows you to 
	* pass in any extra parameters.
	*
	* @param  timeout The find Datasources timesout.
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*/
	action findAvailableDataSourcesFull(
			float timeout, 
			dictionary<string,string> extraParams, 
			action < string, sequence<DataSource> > callback )
	{
		_setExecutionMode();

		sequence<DataSource> result := new sequence<DataSource>;
		integer id := integer.getUnique();
		DataSources ds;

		listener l := on all DataSources(messageId = id) : ds {
			if ds.errorMessage.length() != 0 then {
				result.clear();
				l.quit();
				callback(ds.errorMessage, result);
			}
			else {
				result.appendSequence(ds.datasources);
				if timeout = 0.0 then {
					l.quit();
					callback("", result);
				}
			}
		}

		if timeout > 0.0 then {
			on wait(timeout) {
				l.quit();
 				callback("",result);
			}
		}
		else {
			l.quit();
			// Added to support -Xclock use
			on DataSources() : ds {
				if ds.errorMessage.length() != 0 then {
					result.clear();
					callback(ds.errorMessage, result);
				}
				else {
					result.appendSequence(ds.datasources);
					callback("", result);
				}
			}
		}

		RequestDataSources reqDss:=new RequestDataSources;
		reqDss.messageId	:= id;
		reqDss.extraParams	:= extraParams;
		
		if (_serialExecution) then {
			route reqDss;
		}
		else {			
			send RequestDataSourcesToContext(context.current(), reqDss) to _preSpawnContext;
		}
	}

	/**
	* Find the available data sources.
	*
	* @deprecated [This action has been deprecated due to the misleading name.
	* Please use findAvailableDataSourcesFull instead as it is identical in functionality
	* and a more acurately named action.]
	*/
	action findAvailableServersFull(
			float timeout, 
			dictionary<string,string> extraParams, 
			action < string, sequence<DataSource> > callback )
	{
		log "findAvailableServersFull has now been deprecated, Please see ADBCEvents.mon for more details" at WARN;
		findAvailableDataSourcesFull(timeout, extraParams, callback);
	}

	/**
	* Get the available Databases on a serviceId, using the service Id(s) 
	* returned from findAvailableDataSources. 
	*
	* @param  serviceId The serviceId to search on.
	* @param  locationURL The location of the database.
	* @param  user The user Id for the database (if required).
	* @param  password The password to the database (if required).
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*/
	action getDatabasesFull(
			string serviceId,
			string locationURL,
			string user, 
			string password, 
			dictionary<string,string> extraParams, 
			action < string, sequence<Database> > callback)
	{
		_setExecutionMode();

		sequence<Database> result := new sequence<Database>;
		integer id := integer.getUnique();
		Databases dbs;

		_databasesListener := on Databases(messageId = id) : dbs {
			if dbs.errorMessage.length() != 0 then {
				result.clear();
			}
			else {
				result.appendSequence(dbs.databases);
			}
			callback(dbs.errorMessage, result);
		}

		RequestDatabases reqDbs := new RequestDatabases;
		reqDbs.messageId		:= id;
		reqDbs.serviceId		:= serviceId;
		reqDbs.databaseLocation	:= locationURL;
		reqDbs.userName			:= user;
		reqDbs.password			:= password;
		reqDbs.extraParams		:= extraParams;
		
		if (_serialExecution) then {
			route reqDbs;
		}
		else {			
			send RequestDatabasesToContext(context.current(), reqDbs) to _preSpawnContext;
		}
	}

	/**
	* Provides details of all the loaded named queries. 
	* This action allows you to pass in any extra parameters.
	*
	* @param  serviceId The serviceId to search on.
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*/
	action getNamedQueriesFull(
			string serviceId, 
			dictionary<string,string> extraParams, 
			action<	string,	sequence<NamedQuery> > callback)
	{
		_setExecutionMode();

		sequence<NamedQuery> results := new sequence<NamedQuery>;
		integer id := integer.getUnique();
		NamedQueries nqs;
	
		_namedQueriesListener := on NamedQueries(messageId = id) : nqs {
			if nqs.errorMessage.length() != 0 then {
				results.clear();
			}
			else {
				results.appendSequence(nqs.namedQueries);
			}
			callback(nqs.errorMessage, results);
		}
	
		RequestNamedQueries reqNqs := new RequestNamedQueries;
		reqNqs.messageId	:= id;
		reqNqs.serviceId	:= serviceId;
		reqNqs.extraParams	:= extraParams;
		
		if (_serialExecution) then {
			route reqNqs;
		}
		else {			
			send RequestNamedQueriesToContext(context.current(), reqNqs) to _preSpawnContext;
		}
	}
	
	/** @private */
	action _setExecutionMode() {
		if not _initialized then {
			if (_preSpawnContext.getId() = 0 or _preSpawnContext.getId() = context.current().getId()) then {
				_serialExecution := true;
			}
			else {
				_serialExecution := false;
			}
			_initialized := true;
		}		
	}

    /**
    * OnBeginRecovery is called after all objects and listeners of persistent monitors have been recovered 
    * and before queued events are processed.
    */
	action onBeginRecovery() {
		_databasesListener.quit();
		_namedQueriesListener.quit();
	}
	
	//
	// The following fields are considered "private"
	//
	/** @private */
	context _preSpawnContext;
	/** @private */
	boolean _initialized;
	/** @private */
	boolean _serialExecution;
	/** @private */
	listener _databasesListener;
	/** @private */
	listener _namedQueriesListener;
}

/** Event containing the actions to perform operations on a database.
 * All operations except query are performed using this event.
 * The Query event is used for queries.
 *
 * See openDatabase(), closeDatabase(), forceCloseDatabase(), createStoreStatement(),
 * deleteStoreStatement(), storeEvent(), storeData(), setStoreErrorCallback(),
 * commitRequest(), runCommand(), and stopAllQueries().
 * 
 * Also see the "*Full" versions of the above for versions of the calls
 * that accept the complete set of parameters.
 */
event Connection
{
	/** 
	* Constant for param of open database actions.
	* 
	* This is used when you wish to leave username/password parameters empty, for example when passing this information via the url.
	*/
	constant string NOT_SET := "ADBC_NULL";

	/**
	 * Set context that is current prior to spawning.
	 *
	 * Optional: Only needed when using parallel processing.
	 *
	 * @param  preSpawnContext Current context prior to spawning.
	 *
	 */
	action initPreSpawnContext(context preSpawnContext) {
		_preSpawnContext := preSpawnContext;
	}

	/**
	* Opens the connection to a database.
	*
	* @param  serviceId The serviceId the database is on.
	* @param  databaseURL The location of the database.
	* @param  user The user Id for the database (if required). If you wish to set the username on the URL, you must set this parameter to the special value of Connection.NOT_SET
	* @param  password The password to the database (if required). If you wish to set the username on the URL, you must set this parameter to the special value of Connection.NOT_SET
	* @param  autoCommit Auto-commit mode.
	* <pre>
	* Requires (case insensitive values):
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.
	* </pre>
	* @param  callback Callback action.
	*
	* See OpenDatabaseFull.
	*/
	action openDatabase(
			string serviceId, 
			string databaseURL,
			string user, 
			string password,
			string autoCommit,
			action <Connection, string> callback)
	{
		openDatabaseFull(serviceId, databaseURL, user, password, autoCommit,
		               false, new dictionary<string,string>, callback);
	}

	/**
	* Closes a connection to the database.
	*
	* @param  callback Callback action.
	*
	* See closeDatabaseFull
	*/
	action closeDatabase(
			action <Connection, string> callback)
	{
		closeDatabaseFull(false, new dictionary<string,string>, callback);
	}

	/**
	* Closes a connection to the database, even if there are any Queries 
	* or commands still in progress.
	*
	* @param  callback Callback action.
	*
	* See closeDatabaseFull
	*/
	action forceCloseDatabase(
			action <Connection, string> callback)
	{
		closeDatabaseFull(true, new dictionary<string,string>, callback);
	}

	/**
	* If your application will use a prepared statement or a stored procedure then you 
	* need to first create a store statement on a relational database for the specified table.
	*
	* This can then be reused, based on the statement name returned in the storeStatementAck. 
	* You must list the correct values for the inputTypes in relation to the inputToNameMap.
	*
	* @param  name The name of the storeStatement.
	* @param  tablename The name of the table to create the statement against.
	* @param  statementString The statement to be created.
	* @param  inputTypes The input types for the statement.
	* @param  inputToNameMap The mapping between an identifier and the field names.		
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*/
	action createStoreStatement(
				string name,
				string tableName,
				string statementString,
				sequence<string> inputTypes,
				dictionary<integer,string> inputToNameMap,
				dictionary<string,string> extraParams,
				action<Connection,string,string> callback)
	{
		_setExecutionMode();
		
		// Return if database is not open
		if _dbId = 0 then {
			return;
		}

		integer id := integer.getUnique();
		StoreStatementAck ack;
		
		listener l;
		l := on StoreStatementAck(messageId = id):ack {
			if _storeStatementAckListeners.hasKey(ack.messageId) then {
				_storeStatementAckListeners.remove(ack.messageId);
			}
			_storeStatementAckCallback.remove(id);
			callback(self, ack.errorMessage, ack.statementName);
		}
		
		_storeStatementAckListeners.add(id, l);
		_storeStatementAckCallback.add(id, callback);

		CreateStoreStatement createStatement := new CreateStoreStatement;
		createStatement.messageId		:= id;
		createStatement.serviceId		:= _serviceId;
		createStatement.databaseId		:= _dbId;
		createStatement.name			:= name;
		createStatement.tableName		:= tableName;
		createStatement.statementString	:= statementString;
		createStatement.inputParamTypes	:= inputTypes;
		createStatement.inputToNameMap	:= inputToNameMap;
		createStatement.extraParams		:= extraParams;

		if (_serialExecution) then {
			route createStatement;
		}
		else {
			send CreateStoreStatementToContext(context.current(), createStatement) to _preSpawnContext;
		}
	
	}
	
	/**
	* Remove the store statment from the system for this table.
	*
	* @param  tablename The name of the table the statement was created against.
	* @param  statementString The statement to be deleted.
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*/
	action deleteStoreStatement(
				string statementName,
				string tableName,
				dictionary<string,string> extraParams,
				action<Connection,string,string> callback)
	{
		_setExecutionMode();
		
		// Return if database is not open
		if _dbId = 0 then {
			return;
		}

		integer id := integer.getUnique();
		StoreStatementAck ack;
		
		listener l;
		l := on StoreStatementAck(messageId = id):ack {
			if _storeStatementAckListeners.hasKey(ack.messageId) then {
				_storeStatementAckListeners.remove(ack.messageId);
			}
			_storeStatementAckCallback.remove(id);
			callback(self, ack.errorMessage, ack.statementName);
		}
		
		_storeStatementAckListeners.add(id, l);
		_storeStatementAckCallback.add(id, callback);

		DeleteStoreStatement deleteStatement := new DeleteStoreStatement;
		deleteStatement.messageId		:= id;
		deleteStatement.serviceId		:= _serviceId;
		deleteStatement.databaseId		:= _dbId;
		deleteStatement.statementName	:= statementName;
		deleteStatement.tableName		:= tableName;
		deleteStatement.extraParams		:= extraParams;

		if (_serialExecution) then {
			route deleteStatement;
		}
		else {
			send DeleteStoreStatementToContext(context.current(), deleteStatement) to _preSpawnContext;
		}
	
	}

	/**
	* Store the event contained in the event Data
	* without an acknowledgement.
	*
	* The acknowledge field is set to false indicating a 
	* StoreOperationAck should not be sent.
	*
	* @param  timestamp The timestamp of the event.
	* @param  eventString The event to store.
	* @param  tableName The table to store the event in.
	* @param  statementName A storeStatement that was previously created that references a prepared statement or stored procedure.
	* @param  timeColumn The column in the database where you want the event timestamp to be stored.
	* @param  extraParams Any additional settings.
	*
	* returns the messageId of the store event.
	*/
	action storeEvent(
			float timestamp,
			string eventString,
			string tableName,
			string statementName,
			string timeColumn,
			dictionary<string,string> extraParams) returns integer
	{
		_setExecutionMode();
		
		// Return if database is not open
		if _dbId = 0 then {
			return 0;
		}

		integer id := integer.getUnique();
		
		StoreEvent store := new StoreEvent;
		store.messageId		:= id;
		store.serviceId		:= _serviceId;
		store.databaseId	:= _dbId;
		store.timestamp		:= timestamp;
		store.eventData		:= eventString;
		store.acknowledge	:= false;
		store.tableName		:= tableName;
		store.statementName	:= statementName;
		store.timeColumn	:= timeColumn;
		store.extraParams	:= extraParams;

		if (_serialExecution) then {
			route store;
		}
		else {
			send StoreEventToContext(context.current(), store) to _preSpawnContext;
		}

			return id;
	}

	/**
	* Store the event contained in the event Data
	* with an acknowledgement.
	*
	* The acknowledge field is set to true indicating a 
	* StoreOperationAck should be sent, therefore any errors
	* will be returned in the errorMessage.
	*
	* @param  timestamp The timestamp of the event.
	* @param  eventString The event to store.
	* @param  tableName The table to store the event in.
	* @param  statementName A storeStatement that was previously created that references a prepared statement or stored procedure.
	* @param  timeColumn The column in the database where you want the event timestamp to be stored.
	* @param  extraParams Any additional settings.
	* @param  callback Callback action.
	*
	*/
	action storeEventWithAck(
			float timestamp,
			string eventString,
			string tableName,
			string statementName,
			string timeColumn,
			string token,
			dictionary<string,string> extraParams,
			action <Connection, string, string> callback)
	{
		_setExecutionMode();
		
		// Return if database is not open
		if _dbId = 0 then {
			callback(self, token, "Database not open");
		}

		integer id := integer.getUnique();
		StoreOperationAck ack;

		listener l;
		l := on StoreOperationAck(messageId = id):ack {
			if _storeOpAckListeners.hasKey(ack.messageId) then {
				_storeOpAckListeners.remove(ack.messageId);
			}
			_storeOpAckCallback.remove(id);
			_storeOpAckCallbackTokens.remove(id);
			callback(self, token, ack.errorMessage);
		}
		
		_storeOpAckListeners.add(id, l);
		_storeOpAckCallback.add(id, callback);
		_storeOpAckCallbackTokens.add(id, token);

		StoreEvent store := new StoreEvent;
		store.messageId		:= id;
		store.serviceId		:= _serviceId;
		store.databaseId	:= _dbId;
		store.timestamp		:= timestamp;
		store.eventData		:= eventString;
		store.acknowledge	:= true;
		store.tableName		:= tableName;
		store.statementName	:= statementName;
		store.timeColumn	:= timeColumn;
		store.extraParams	:= extraParams;

		if (_serialExecution) then {
			route store;
		}
		else {			
			send StoreEventToContext(context.current(), store) to _preSpawnContext;
		}
	}

	/**
	* Store the data contained in the fields dictionary
	* without an acknowledgement.
	*
	* The acknowledge field is set to false indicating a 
	* StoreOperationAck should not be sent
	*
	* @param  tableName The table to store the data in.
	* @param  statementName A storeStatement that was previously created that references a prepared statement or stored procedure.
	* @param  fields The column values to be stored.
	* @param  extraParams Any additional settings.
	*
	* returns the messageId of the storeData event.
	*/
	action storeData(
			string tableName,
			string statementName,
			dictionary<string,string> fields,
			dictionary<string,string> extraParams) returns integer
	{
		_setExecutionMode();

		// Return if database is not open
		if _dbId = 0 then {
			return 0;
		}

		integer id := integer.getUnique();

		StoreData store := new StoreData;
		store.messageId		:= id;
		store.serviceId		:= _serviceId;
		store.databaseId	:= _dbId;
		store.acknowledge	:= false;
		store.tableName		:= tableName;
		store.statementName	:= statementName;
		store.fieldValues	:= fields;
		store.extraParams	:= extraParams;

		if (_serialExecution) then {
			route store;
		}
		else {
			send StoreDataToContext(context.current(), store) to _preSpawnContext;
		}

		return id;
	}

	/**
	* Store the data contained in the fields dictionary
	* with an acknowledgement.
	*
	* The acknowledge field is set to true indicating a 
	* StoreOperationAck should be sent, therefore any errors
	* will be returned in the errorMessage.
	*
	* @param  tableName The table to store the data in.
	* @param  statementName A storeStatement that was previously created that references a prepared statement or stored procedure.
	* @param  fields The column values to be stored.
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  extraParams Any additional settings.
	* @param  callback Callback action.
	*
	*/
	action storeDataWithAck(
			string tableName,
			string statementName,
			dictionary<string,string> fields,
			string token,
			dictionary<string,string> extraParams,
			action <Connection, string, string> callback)
	{
		_setExecutionMode();

		// Return if database is not open
		if _dbId = 0 then {
			callback(self, token, "Database not open");
		}

		integer id := integer.getUnique();
		StoreOperationAck ack;

		listener l;
		l := on StoreOperationAck(messageId = id):ack {
			if _storeOpAckListeners.hasKey(ack.messageId) then {
				_storeOpAckListeners.remove(ack.messageId);
			}
			_storeDataOpAckCallback.remove(id);
			_storeDataOpAckCallbackTokens.remove(id);
			callback(self, token, ack.errorMessage);
		}
		
		_storeOpAckListeners.add(id, l);
		_storeDataOpAckCallback.add(id, callback);
		_storeDataOpAckCallbackTokens.add(id, token);

		StoreData store := new StoreData;
		store.messageId		:= id;
		store.serviceId		:= _serviceId;
		store.databaseId	:= _dbId;
		store.acknowledge	:= true;
		store.tableName		:= tableName;
		store.statementName	:= statementName;
		store.fieldValues	:= fields;
		store.extraParams	:= extraParams;

		if (_serialExecution) then {
			route store;
		}
		else {			
			send StoreDataToContext(context.current(), store) to _preSpawnContext;
		}
 	}


	/**
	* Store the data contained in the fields dictionary
	* with an acknowledgement.
	*
	* The acknowledge field is set to true indicating a 
	* StoreOperationAck should be sent, therefore any errors
	* will be returned in the errorMessage.
	* This action also returns the messageId of the StoreData event.
	*
	* @param  tableName The table to store the data in.
	* @param  statementName A storeStatement that was previously created that references a prepared statement or stored procedure.
	* @param  fields The column values to be stored.
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  extraParams Any additional settings.
	* @param  callback Callback action.
	*
	* returns the messageId of the storeData event.
	*/
	action storeDataWithAckId(
			string tableName,
			string statementName,
			dictionary<string,string> fields,
			string token,
			dictionary<string,string> extraParams,
			action <Connection, string, string> callback) returns integer
	{
		_setExecutionMode();

		// Return if database is not open
		if _dbId = 0 then {
			callback(self, token, "Database not open");
		}

		integer id := integer.getUnique();
		StoreOperationAck ack;

		listener l;
		l := on StoreOperationAck(messageId = id):ack {
			if _storeOpAckListeners.hasKey(ack.messageId) then {
				_storeOpAckListeners.remove(ack.messageId);
			}
			_storeDataOpAckCallback.remove(id);
			_storeDataOpAckCallbackTokens.remove(id);
			callback(self, token, ack.errorMessage);
		}
		
		_storeOpAckListeners.add(id, l);
		_storeDataOpAckCallback.add(id, callback);
		_storeDataOpAckCallbackTokens.add(id, token);

		StoreData store := new StoreData;
		store.messageId		:= id;
		store.serviceId		:= _serviceId;
		store.databaseId	:= _dbId;
		store.acknowledge	:= true;
		store.tableName		:= tableName;
		store.statementName	:= statementName;
		store.fieldValues	:= fields;
		store.extraParams	:= extraParams;

		if (_serialExecution) then {
			route store;
		}
		else {
			send StoreDataToContext(context.current(), store) to _preSpawnContext;
		}
		
		return id;
 	}

	/**
	* To specify an action to be used when an error is reported.
	*
	* @param  callback Callback action.
	*
	*/
	action setStoreErrorCallback(
			action<Connection, 
					integer,	// first id
					integer, 	// last id
					string>	callback)	// error message
	{
		if _storeErrorListenerActive then {
			_storeErrorListener.quit();
		}
		
		StoreOperationError error;
		_storeErrorCallback := callback;
		_storeErrorCallbackActive := true;

		if (_dbId > 0) then {
			_storeErrorListenerActive := true;
			_storeErrorListener := on all StoreOperationError(databaseId = _dbId) : error {
				_storeErrorCallback(self, error.lowestMessageId, error.highestMessageId, error.errorMessage);
			}
		}
	}

	/**
	* Send a commit request to the database.
	*
	* @param  callback Callback action.
	*
	* returns the messageId of the commitRequest.
	*
	* See also commitRequestFull.
	*/
	action commitRequest(
			action<Connection,integer,string,string> callback) returns integer
	{
		return commitRequestFull("", new dictionary<string,string>, callback);	
	}

	/**
	* Rollback uncommitted changes to database.
	*
	* @param  callback Callback action.
	*
	* returns the messageId of the rollbackRequest.
	*
	* See also rollbackRequestFull
	*/
	action rollbackRequest(
 			action<Connection,integer,string,string> callback) returns integer
 	{
 		return rollbackRequestFull("", new dictionary<string,string>, callback);	
 	}

	/**
	* Run the supplied command against the database.
	*
	* @param  commandString The SQL command to execute.
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  callback Callback action.
	*
	* See also runCommandFull.
	*/
	action runCommand(
			string commandString, 
			string token, 
			action <Connection,string,string> callback)
	{
		runCommandFull(commandString, token, new dictionary<string,string>, callback);
	}

	/**
	* Stop all queries that are currently running.
	*
	* @param  callback Callback action.
	*
	* See also stopAllQueriesFull.
	*/
	action stopAllQueries(
			action<Connection,string> callback) 
	{
		stopAllQueriesFull(new dictionary<string,string>, callback);		
	}

	/** 
	* Returns the service Id of the open database. 
	*/
	action getServiceId() returns string
	{
		return _serviceId;
	}

	/** 
	* Returns the URL of the open database. 
	*/
	action getDatabaseURL() returns string
	{
		return self._databaseURL;
	}

	/** 
	* Returns the Id of the open database. 
	*/
	action getDbId() returns integer
	{
		return _dbId;
	}

	/**
	* Open a shared connection to a database.
	*
	* If a connection is already open then this will reuse 
	* that, if one is not already open then it will open one.
	* 
	* @param  serviceId The serviceId the database is on.
	* @param  databaseURL The location of the database.
	* @param  user The user Id for the database (if required). If you wish to set the username on the URL, you must set this parameter to the special value of Connection.NOT_SET
	* @param  password The password to the database (if required). If you wish to set the username on the URL, you must set this parameter to the special value of Connection.NOT_SET
	* @param  autoCommit Auto-commit mode: (case insensitive values).
	* <pre>
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.</pre>
	* @param  readOnly Specifies if the connection should be read-only.
	* @param  extraParams Any additional settings.
	* @param  callback Callback action.
	*
	*/
	action openDatabaseShared (
			string serviceId, 
			string databaseURL, 
			string user, 
			string password,
			string autoCommit,
			boolean readOnly,
			dictionary<string,string> extraParams, 
			action <Connection, string> callback) 
	{
		extraParams.add("uniqueConnection","false");
		openDatabaseFull(serviceId, databaseURL, user, password, autoCommit,
		               readOnly, extraParams, callback);
	}

	/**
	* Open a connection to a database. This action allows you to 
	* pass in any extra parameters.
	*
	* @param  serviceId The serviceId the database is on.
	* @param  databaseURL The location of the database.
	* @param  user The user Id for the database (if required). If you wish to set the username on the URL, you must set this parameter to the special value of Connection.NOT_SET
	* @param  password The password to the database (if required). If you wish to set the password on the URL, you must set this parameter to the special value of Connection.NOT_SET
	* @param  autoCommit Auto-commit mode: (case insensitive values).
	* <pre>
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.</pre>
	* @param  readOnly Specifies if the connection should be read-only.
	* @param  extraParams Any additional settings.
	* @param  callback Callback action.
	*
	*/
	action openDatabaseFull (
			string serviceId, 
			string databaseURL, 
			string user, 
			string password,
			string autoCommit,
			boolean readOnly,
			dictionary<string,string> extraParams, 
			action <Connection, string> callback) 
	{
		_setExecutionMode();

		// Return if database is already open
		if _dbId != 0 then {
			callback(self, "Connection already in use");
			return;
		}
		_commandAckCallback.clear();
		_commandAckCallbackTokens.clear();
		_commitAckCallback.clear();
		_commitAckCallbackTokens.clear();
		_databaseOpAckCallback.clear();
		_stopAllQueriesAckCallback.clear();
		_storeOpAckCallback.clear();
		_storeOpAckCallbackTokens.clear();
		_storeDataOpAckCallback.clear();
		_storeDataOpAckCallbackTokens.clear();
		_storeStatementAckCallback.clear();
		// Initialize private fields
		_serviceId := serviceId;
		_databaseURL := databaseURL;
		_storeErrorListenerActive := false;
		_user := user;
		_password := password;
		_autoCommit := autoCommit;
		_readOnly := readOnly;
		_extraParams := extraParams;
		_callback := callback;

		integer id := integer.getUnique();
		DatabaseOperationAck ack;

		listener l;
		l := on DatabaseOperationAck(messageId = id) : ack {
			if _databaseOpAckListeners.hasKey(ack.messageId) then {
				_databaseOpAckListeners.remove(ack.messageId);
			}
			if ack.errorMessage.length() = 0 then {
				_dbId := ack.databaseId;
			}
			if (_storeErrorCallbackActive and not _storeErrorListenerActive) then {
				_storeErrorListenerActive := true;
				StoreOperationError error;
				_storeErrorListener := on all StoreOperationError(databaseId = _dbId) : error {
					_storeErrorCallback(self, error.lowestMessageId, error.highestMessageId, error.errorMessage);
				}
			}
			_databaseOpAckCallback.remove(id);
			callback(self, ack.errorMessage);
		}
		
		_databaseOpAckListeners.add(id, l);
		_databaseOpAckCallback.add(id, callback);
		
		OpenDatabase open := new OpenDatabase;
		open.messageId			:= id;
		open.serviceId			:= serviceId;
		open.databaseName		:= databaseURL;
		open.userName			:= user;
		open.password			:= password;
		open.autoCommit			:= autoCommit;
		open.readOnly			:= readOnly;
		open.extraParams		:= extraParams;
		
		if (_serialExecution) then {
			route open;
		}
		else {			
			send OpenDatabaseToContext(context.current(), open) to _preSpawnContext;
		}
	}
		
	/**
	* Close a connection to the database.
	* This action allows you to pass in any extra parameters.
	*
	* If shared connections are used then the database will
	* only be closed if it is the last connection to that database.
	*
	* @param  force If true will stop any running queries and commands.
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*
	*/
	action closeDatabaseFull(
			boolean force,
			dictionary<string,string> extraParams, 
			action<Connection,string> callback) 
	{
		_setExecutionMode();

		// Return if database is not open
		if _dbId = 0 then {
			callback(self, "Connection not opened");
			return;
		}

		integer id := integer.getUnique();
		DatabaseOperationAck ack;

		listener l;
		l := on DatabaseOperationAck(messageId = id) : ack {
			if _databaseOpAckListeners.hasKey(ack.messageId) then {
				_databaseOpAckListeners.remove(ack.messageId);
			}
			if ack.errorMessage.length() = 0 then {
				_dbId := 0;
			}
			if _storeErrorListenerActive then {
				_storeErrorListener.quit();
				_storeErrorListenerActive := false;
			}
			quitListeners();
			_databaseOpAckCallback.remove(id);
			callback(self, ack.errorMessage);
		}
		
		_databaseOpAckListeners.add(id, l);
		_databaseOpAckCallback.add(id, callback);

		CloseDatabase close := new CloseDatabase;
		close.messageId		:= id;
		close.serviceId		:= _serviceId;
		close.databaseId	:= _dbId;
		close.force			:= force;
		close.extraParams	:= extraParams;
		
		if (_serialExecution) then {
			route close;
		}
		else {			
			send CloseDatabaseToContext(context.current(), close) to _preSpawnContext;
		}
	}

	/**
	* Send a commit request to the database.
	* This action allows you to pass in any extra parameters.
	
	* This will result in anything outstanding being committed to the database.
	*
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*
	* returns the messageId of the commitRequest.
	*/
	action commitRequestFull(
			string token,
			dictionary<string,string> extraParams,
			action<Connection,integer,string,string> callback) returns integer
	{
		_setExecutionMode();

		// Return if database is not open
		if _dbId = 0 then {
			callback(self, 0, token, "Connection not opened");
			return 0;
		}

		integer id := integer.getUnique();
		CommitAck ack;

		listener l;
		l := on CommitAck(messageId = id) : ack {
			if _commitAckListeners.hasKey(ack.messageId) then {
				_commitAckListeners.remove(ack.messageId);
			}
			_commitAckCallback.remove(id);
			_commitAckCallbackTokens.remove(id);
			callback(self, ack.messageId, token, ack.errorMessage);
		}
		
		_commitAckListeners.add(id, l);
		_commitAckCallback.add(id, callback);
		_commitAckCallbackTokens.add(id, token);

		CommitRequest commit := new CommitRequest;
		commit.messageId	:= id;
		commit.serviceId	:= _serviceId;
		commit.databaseId	:= _dbId;
		commit.extraParams	:= extraParams;

		if (_serialExecution) then {
			route commit;
		}
		else {			
			send CommitRequestToContext(context.current(), commit) to _preSpawnContext;
		}

		return id;
	}
 
	/**
	* Rollback uncommitted changes to database.
	* This action allows you to pass in any extra parameters.
	*
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*
	* returns the messageId of the rollbackRequest. 
	*/
	action rollbackRequestFull(
			string token,
 			dictionary<string,string> extraParams,
 			action<Connection,integer,string,string> callback) returns integer
 	{
 		_setExecutionMode();

 		// Return if database is not open
 		if _dbId = 0 then {
 			callback(self, 0, token, "Connection not opened");
 			return 0;
 		}

 		integer id := integer.getUnique();
 		RollbackAck ack;

 		on RollbackAck(messageId = id) : ack
 		{
 			callback(self, ack.messageId, token, ack.errorMessage);
 		}

 		RollbackRequest rollback := new RollbackRequest;
 		rollback.messageId	:= id;
 		rollback.serviceId	:= _serviceId;
 		rollback.databaseId	:= _dbId;
 		rollback.extraParams	:= extraParams;

 		if (_serialExecution) then {
 			route rollback;
 		}
 		else {			
 			send RollbackRequestToContext(context.current(), rollback) to _preSpawnContext;
 		}

		return id;
 	}

	/**
	* Run the supplied command against the database.
	* This action allows you to pass in any extra parameters.
	*
	* @param  commandString The SQL command to execute.
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  extraParams Currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*
	*/
	action runCommandFull(
			string commandString, 
			string token,
			dictionary<string,string> extraParams, 
			action<Connection,string,string> callback) 
	{
		_setExecutionMode();

		// Return if database is not open
		if _dbId = 0 then {
			callback(self, token, "Connection not opened");
			return;
		}

		integer id := integer.getUnique();
		CommandAck ack;

		listener l;
		l := on CommandAck(messageId = id) : ack {
			if _commandAckListeners.hasKey(ack.messageId) then {
				_commandAckListeners.remove(ack.messageId);
			}
			_commandAckCallback.remove(id);
			_commandAckCallbackTokens.remove(id);
			callback(self, token, ack.errorMessage);
		}
		
		_commandAckListeners.add(id, l);
		_commandAckCallback.add(id, callback);
		_commandAckCallbackTokens.add(id, token);
		
		Command command := new Command;
		command.messageId		:= id;
		command.serviceId		:= _serviceId;
		command.databaseId		:= _dbId;
		command.operationString	:= commandString;
		command.extraParams		:= extraParams;

		if (_serialExecution) then {
			route command;
		}
		else {			
			send CommandToContext(context.current(), command) to _preSpawnContext;
		}
	}

	/**
	* Stop all Queries that are currently running.
	* This action allows you to pass in any extra parameters.
	*
	* @param  extraParams This is currently a container to allow additional information in the future.
	* @param  callback Callback action.
	*
	*/
	action stopAllQueriesFull(
			dictionary<string,string> extraParams, 
			action<Connection,string> callback) 
	{
		_setExecutionMode();

		// Return if database is not open
		if _dbId = 0 then {
			callback(self, "Connection not opened");
			return;
		}

		integer id := integer.getUnique();
		StopAllQueriesAck ack;
		
		listener l;
		l := on StopAllQueriesAck(messageId = id) : ack {
			if _stopAllQueriesAckListeners.hasKey(ack.messageId) then {
				_stopAllQueriesAckListeners.remove(ack.messageId);
			}
			_stopAllQueriesAckCallback.remove(id);
			callback(self, ack.errorMessage);
		}
		
		_stopAllQueriesAckListeners.add(id, l);
		_stopAllQueriesAckCallback.add(id, callback);
		
		StopAllQueries stopAll := new StopAllQueries;
		stopAll.messageId	:= id;
		stopAll.serviceId	:= _serviceId;
		stopAll.extraParams	:= extraParams;

		if (_serialExecution) then {
			route stopAll;
		}
		else {			
			send StopAllQueriesToContext(context.current(), stopAll) to _preSpawnContext;
		}
	}

	/** @private */
	action _setExecutionMode() {
		if not _initialized then {
			if (_preSpawnContext.getId() = 0 or _preSpawnContext.getId() = context.current().getId()) then {
				_serialExecution := true;
			}
			else {
				_serialExecution := false;
			}
			_initialized := true;
		}		
	}
	
	/** 
	* Reopen the connection to a database.
	*
	* @deprecated [This action has been deprecated.
	* Please use the action reopenWithAck instead.]
	*/
    action reopen() {
		reset();
    	openDatabaseFull(self._serviceId, self._databaseURL, self._user, self._password, self._autoCommit,
		                 self._readOnly, self._extraParams, self._callback);
    }
	
	/**
	* Reopen the connection to a database.
	*
	* The callback method will  be called on completion and will
	* be able to determine if it was successful or if there were any errors.
	*
	* @param  callback Callback action.
	*
	*/
	action reopenWithACK(action <Connection, string> callback) {
		reset();
    	openDatabaseFull(self._serviceId, self._databaseURL, self._user, self._password, self._autoCommit,
		                 self._readOnly, self._extraParams, callback);
    }
	    
	/** @private */
	action quitListeners() {
    	integer k;
    	for k in _storeOpAckListeners.keys() {
    		_storeOpAckListeners[k].quit();
    	}   	
    	for k in _databaseOpAckListeners.keys() {
    		_databaseOpAckListeners[k].quit();
    	}   	
    	for k in _commitAckListeners.keys() {
    		_commitAckListeners[k].quit();
    	}   	
    	for k in _commandAckListeners.keys() {
    		_commandAckListeners[k].quit();
    	}   	
    	for k in _stopAllQueriesAckListeners.keys() {
    		_stopAllQueriesAckListeners[k].quit();
    	}
    	_storeErrorListener.quit();
    	_storeErrorListenerActive := false;

    	_openStatusListener.quit();
   	
    	_storeOpAckListeners.clear();
    	_databaseOpAckListeners.clear();
    	_commitAckListeners.clear();
    	_commandAckListeners.clear();
    	_stopAllQueriesAckListeners.clear();
    }
 

	/**
	* OnConcludeRecovery is called after the recovery process has been completed, 
	* including all non persistent monitors having being re-loaded.
	*/
	action onConcludeRecovery() {
		quitListeners();
		integer id;


		for id in _commandAckCallback.keys() {
			_commandAckCallback[id](self, _commandAckCallbackTokens[id], "Correlator recovered");
		}
		_commandAckCallback.clear();
		_commandAckCallbackTokens.clear();
		
		for id in _commitAckCallback.keys() {
			_commitAckCallback[id](self, id, _commitAckCallbackTokens[id], "Correlator recovered");
		}
		_commitAckCallback.clear();
		_commitAckCallbackTokens.clear();
		
		for id in _databaseOpAckCallback.keys() {
			_databaseOpAckCallback[id](self, "Correlator recovered");
		}
		_databaseOpAckCallback.clear();
		
		for id in _stopAllQueriesAckCallback.keys() {
			_stopAllQueriesAckCallback[id](self, "Correlator recovered");
		}
		_stopAllQueriesAckCallback.clear();
		
		for id in _storeOpAckCallback.keys() {
			_storeOpAckCallback[id](self, _storeOpAckCallbackTokens[id], "Correlator recovered");
		}
		_storeOpAckCallback.clear();
		_storeOpAckCallbackTokens.clear();
		
		for id in _storeDataOpAckCallback.keys() {
			_storeDataOpAckCallback[id](self, _storeDataOpAckCallbackTokens[id], "Correlator recovered");
		}
		_storeDataOpAckCallback.clear();
		_storeDataOpAckCallbackTokens.clear();
		
		for id in _storeStatementAckCallback.keys() {
			_storeStatementAckCallback[id](self, "", "Correlator recovered");
		}
		_storeStatementAckCallback.clear();
		
		if _storeErrorCallbackActive then {
			_storeErrorCallback(self, 0, 0, "Correlator recovered");
		}
		_storeErrorCallbackActive:=false;
	}
    
	/**
	* Reset the state of the connection.
	*
	* Reset including all listeners and allow another connection to be opened.
	*/
	action reset() {
		quitListeners();
		_dbId := 0;
	}

	//
	// The following fields are considered "private"
	//
	/** @private */
	context 	_preSpawnContext;
	/** @private */
	boolean 	_initialized;
	/** @private */
	boolean 	_serialExecution;
	/** @private */
	integer		_dbId;			// database connection
	/** @private */
	string		_serviceId;		// service id "channel"
	/** @private */
	string		_databaseURL;	// database connection string
	/** @private */
	boolean		_storeErrorListenerActive;
	/** @private */
	listener	_storeErrorListener;
	/** @private */
	listener	_openStatusListener;
	/** @private */
	string		_user; 
	/** @private */
	string		_password;
	/** @private */
	string		_autoCommit;
	/** @private */
	boolean		_readOnly;
	/** @private */
	dictionary<string,string> _extraParams; 
	/** @private */
	action <Connection, string> _callback;
	
	/** @private */
	dictionary < integer, listener > _storeOpAckListeners;
	/** @private */
	dictionary < integer, listener > _databaseOpAckListeners;
	/** @private */
	dictionary < integer, listener > _commitAckListeners;
	/** @private */
	dictionary < integer, listener > _commandAckListeners;
	/** @private */
	dictionary < integer, listener > _stopAllQueriesAckListeners;
	/** @private */
	dictionary < integer, listener > _storeStatementAckListeners;
	
	/** @private */
	dictionary<integer, action<Connection,string,string> > _commandAckCallback;
	/** @private */
	dictionary<integer, string> _commandAckCallbackTokens;
	/** @private */
	dictionary<integer, action<Connection,integer,string,string> > _commitAckCallback;
	/** @private */
	dictionary<integer, string > _commitAckCallbackTokens;
	/** @private */
	dictionary<integer, action<Connection,string> > _databaseOpAckCallback;
	/** @private */
	dictionary<integer, action<Connection,string> > _stopAllQueriesAckCallback;
	/** @private */
	dictionary<integer, action<Connection,string,string> > _storeOpAckCallback;
	/** @private */
	dictionary<integer, string > _storeOpAckCallbackTokens;
	/** @private */
	dictionary<integer, action<Connection,string,string> > _storeDataOpAckCallback;
	/** @private */
	dictionary<integer, string > _storeDataOpAckCallbackTokens;
	/** @private */
	dictionary<integer, action<Connection,string,string> > _storeStatementAckCallback;

	/** @private */
	action<Connection,integer,integer,string> _storeErrorCallback;
	/** @private */
	boolean _storeErrorCallbackActive;
	
	/** @private */
	string _storeOpAckCallbackToken;
	/** @private */
	string _storeDataOpAckCallbackToken;
}

/** Event containing the actions to create a prepared query statement on a database.
*
* See init(), create(), delete(), and setExtraParms()
*/
event PreparedQuery
{
	/**
	 * Set context that is current prior to spawning.
	 *
	 * Optional: Only needed when using parallel processing.
	 *
	 * @param  preSpawnContext Current context prior to spawning.
	 *
	 */
	action initPreSpawnContext(context preSpawnContext) {
		_preSpawnContext := preSpawnContext;
	}

	/**
	* Initialize the prepared query elements
	* with the query string, input and output types.
	*
	* @param  conn The database connection.
	* @param  queryString The SQL query string.
	* @param  inputTypes Optional, if using replaceable parameters you need to specify the types.				
	* @param  outputTypes Optional, If it will be used as a stored procedure that uses output parameters.
	*
	*/
	action init(
	     		Connection conn, 
	     		string queryString,
	     		sequence<string> inputTypes,
	     		sequence<string> outputTypes)
	{
		_setDefaults();
		_conn					:= conn;
		_queryString			:= queryString;
		_id						:= integer.getUnique();
		_inputTypes				:= inputTypes;
		_outputTypes			:= outputTypes;
		_initialized			:= true;
	}

	/** @private */
	action _setDefaults()
	{
		_id						:= -1;
		_queryString			:= "";
		_extraParams			:= new dictionary<string,string>;
		_preparedQueryId		:= 0;
		_inputTypes				:= new sequence<string>;
		_outputTypes			:= new sequence<string>;
	}

	/**
	*
	* Additional parameters for the prepared query.
	*
	* @param  extra This is currently a container to allow additional information in the future.
	*
	*/
	action setExtraParams(dictionary<string,string> extra)
	{
		_extraParams := extra;
	}

	/**
	* Create the prepared query on the database, which 
	* can later be reused against queries.
	*
	* This is created based on the elements that were 
	* set up in the init action. 
	*
	* @param  callback Callback action.
	*
	*/
	action create(
			action<PreparedQuery,string> callback)
	{
		_setExecutionMode();

		if not _initialized then {
			callback(self, "PreparedQuery not initialized");
			return;
		}
		else if _preparedQueryId != 0 then {
			callback(self, "PreparedQuery already created");
			return;
		}
		integer id:=integer.getUnique();
		_callback.add(id, callback);


		PreparedQueryAck ack;
		_createPreparedQueryAckListener := on PreparedQueryAck(messageId = _id) : ack {
			if ack.errorMessage.length() = 0 then {
				_preparedQueryId := ack.queryId;
			}
			_callback.remove(id);
			callback(self, ack.errorMessage);
		}

		CreatePreparedQuery create := new CreatePreparedQuery;
		create.messageId			:= _id;
		create.serviceId			:= _conn._serviceId;
		create.databaseId			:= _conn.getDbId();
		create.query				:= _queryString;
		create.inputParamTypes		:= _inputTypes;
		create.outputParamTypes		:= _outputTypes;
		create.extraParams			:= _extraParams;

		if (_serialExecution) then {
			route create;
		}
		else {			
			send CreatePreparedQueryToContext(context.current(), create) to _preSpawnContext;
		}
	}

	/**
	* Delete the prepared query form the database.
	*
	* @param  callback Callback action.
	*
	*/
	action delete(
  			action<PreparedQuery,string> callback)
  	{
  		_setExecutionMode();


  		if not _initialized then {
  			callback(self, "PreparedQuery not initialized");
  			return;
  		}
  		else if _preparedQueryId = 0 then {
  			callback(self, "PreparedQuery already deleted");
  			return;
  		}
  		integer id := integer.getUnique();
		_callback.add(id, callback);

  		PreparedQueryAck ack;
  		_deletePreparedQueryAckListener := on PreparedQueryAck(messageId = id) : ack {
  			if ack.errorMessage.length() = 0 then {
  				_preparedQueryId := ack.queryId;
  			}
			_callback.remove(id);
  			callback(self, ack.errorMessage);
  		}
  		
  		DeletePreparedQuery delete := new DeletePreparedQuery;
  		delete.messageId			:= id;
  		delete.serviceId			:= _conn._serviceId;
  		delete.databaseId			:= _conn.getDbId();
  		delete.queryId				:= _preparedQueryId;
  		delete.extraParams			:= _extraParams;

  		if (_serialExecution) then {
  			route delete;
  		}
  		else {			
  			send DeletePreparedQueryToContext(context.current(), delete) to _preSpawnContext;
  		}
  	}

	/**
	* Returns the messageId that was used to 
	* create the prepared query.
	*/
	action getId() returns integer
	{
		return _id;
	}

	/**
	* Returns the connection to the database.
	*/
	action getConnection() returns Connection
	{
		return _conn;
	}

	/**
	* Returns the id of the prepared query.
	*/
	action getPreparedQueryId() returns integer
	{
		return _preparedQueryId;
	}

	/**
	* Returns the query string that was used to 
	* create the prepared query.
	*/
	action getQueryString() returns string
	{
		return _queryString;
	}

	/** @private */
	action _setExecutionMode() {
		if not _preinitialized then {
			if (_preSpawnContext.getId() = 0 or _preSpawnContext.getId() = context.current().getId()) then {
				_serialExecution := true;
			}
			else {
				_serialExecution := false;
			}
			_preinitialized := true;
		}
	}
	
	/**
	* OnBeginRecovery is called after all objects and listeners of persistent monitors have been recovered 
	* and before queued events are processed.
	*/
	action onBeginRecovery() {
		_createPreparedQueryAckListener.quit();
		_deletePreparedQueryAckListener.quit();
	}

	/**
	* OnConcludeRecovery is called after the recovery process has been completed, 
	* including all non persistent monitors having being re-loaded.
	*/
	action onConcludeRecovery() {
		integer id;
		for id in _callback.keys() {
			_callback[id](self, "Correlator recovered");
		}
		_callback.clear();
	}

	//
	// The following fields are considered "private"
	//
	/** @private */
	context 	_preSpawnContext;
	/** @private */
	boolean 	_preinitialized;
	/** @private */
	boolean 	_serialExecution;
	/** @private */
	integer 	_id;
	/** @private */
	boolean 	_initialized;
	/** @private */
	Connection 	_conn;
	/** @private */
	string 		_queryString;
	/** @private */
	integer		_preparedQueryId;
	/** @private */
	sequence<string> _inputTypes;
	/** @private */
	sequence<string> _outputTypes;
	/** @private */
	dictionary<string,string> _extraParams; // Optional parameters
	/** @private */
	listener _createPreparedQueryAckListener;
	/** @private */
	listener _deletePreparedQueryAckListener;
	/** @private */
	dictionary<integer, action<PreparedQuery,string> > _callback;
}

/** Event containing the actions to perform a query on a database.
 *
 * See initQuery(), initNamedQuery(), setReturnType(),
 * setBatchSize(), makeBacktest(), setStatusCallback(), setEventType(),
 * setBatchDoneCallback(), setSchemaCallback(),setExtraParmas(),
 * start(), stop(), pause() and getNextBatch().
 *
 * Also see stopFull(), pauseFull() and getNextBatchFull() for a version
 * of the call that accepts the complete set of parameters.
 */
event Query
{
	/** 
	* Constant for return type ResultEvent.
	* 
	* This return type is used for general database queries.
	*/
	constant string RESULT_EVENT := "ResultEvent";
	/** 
	* Constant for return type ResultEventHetero.
	*
	* This return type is intended for advanced database queries.
	* It is not applicable to SQL databases.
	*/
	constant string RESULT_EVENT_HETERO := "ResultEventHetero";
	/** 
	* Constant for return type Native.
	*
	* This return type is also used for playback.
	*
	* The query needs to specify the event type to be returned 
	* and the name of the database table's column that contains 
	* the event's time stamp.
	*/
	constant string NATIVE := "Native";
	/** 
	* Constant for return type Historical.
	*
	* This return type is most commonly used for playback.
	*
	* The Matching event will be 'wrapped' in a container
	* event. The container event will have a name 
	* based on that of the event name.
	*/
	constant string HISTORICAL := "Historical";

	/**
	* Constant for param type ADBC_NULL.
	*
	* This can be used when you require passing null to the database, for example when using prepared queries and stored procedures, as EPL doesn't support null values.
	*/
	constant string ADBC_NULL := "ADBC_NULL";

	/**
	 * Set context that is current prior to spawning.
	 *
	 * Optional: Only needed when using parallel processing.
	 *
	 * @param  preSpawnContext Current context prior to spawning.
	 *
	 */
	action initPreSpawnContext(context preSpawnContext) {
		_preSpawnContext := preSpawnContext;
	}

	/**
	* Initialize the query with the query string
	* for this database.
	*
	* @param  conn The database connection.
	* @param  queryString The SQL query string.
	*
	*/
	action initQuery(
			Connection conn, 
			string queryString)
	{
		_setDefaults();
		_conn				:= conn;
		_queryString		:= queryString;
		_id					:= integer.getUnique();
		_initialized		:= true;
	}

	/**
	* Initialize the named query with the template
	* name and template paramteres for this database.
	*
	* @param  conn The database connection.
	* @param  templateName The name of the query template.
	* @param  parameters Additional named query's parameters.	
	*
	*/
	action initNamedQuery(
			Connection conn, 
			string templateName,
			dictionary<string,string> parameters)
	{
		_setDefaults();
		_conn			:= conn;
		_queryString	:= templateName;
		_queryTemplateParams	:= parameters;
		_id				:= integer.getUnique();
		_isNamedQuery	:= true;
		_initialized	:= true;
	}

	/**
	* Initialize the prepared query with pre-created
	* instance of a prepared query.
	*
	* @param  preparedQuery The prepared query to be initialized from.
	*
	*/
	action initPreparedQuery(
			PreparedQuery preparedQuery)
	{
		_setDefaults();
		_conn					:= preparedQuery.getConnection();
		_preparedQuery			:= preparedQuery;
		_id						:= integer.getUnique();
		_usePreparedQuery		:= true;
		_initialized			:= true;
	}

	/** @private */
	action _setDefaults()
	{
		_id						:= -1;
		_initialized			:= false;
		_queryString			:= "";
		_isNamedQuery			:= false;
		_queryTemplateParams	:= new dictionary<string,string>;
		_returnType				:= RESULT_EVENT;
		_eventType				:= "";
		_backtest				:= false;
		_statusFreq				:= 0.0;
		_batchSize				:= 0;
		_batchSizeIncludesTimeEvents	:= false;
		_backtestSpeed			:= 0.0;
		_runUntilTime			:= 0.0;
		_usePreparedQuery		:= false;
		_inputParams			:= new sequence<string>;
		_extraParams			:= new dictionary<string,string>;
		_batchListenerActive	:= false;
		_statusListenerActive	:= false;
		_schemaListenerActive	:= false;
		_stopped				:= true;
	}

	/**
	* Set the return type based using the constants provided.
	*
	* @param  returnType The return type of this query.
	*
	*/
	action setReturnType(string returnType)
	{
		if returnType = RESULT_EVENT
		or returnType = RESULT_EVENT_HETERO
		or returnType = NATIVE
		or returnType = HISTORICAL then {
			_returnType := returnType;
		}
	}

	/**
	* Set the eventType when it is an event query.
	*
	* This should be set when the returnType is NATIVE.
	* Also in this case you should set the TimeColumn.
	* In addition, you need to add mapping rules to the ADBC adapter's
				configuration file for the event type being returned.
	*
	* @param  eventType The type of this query when working with event queries.
	*
	*/
	action setEventType(string eventType)
	{
		_eventType := eventType;
	}

	/**
	* If the return type is Native specify the database table column that stores the event's timestamp.
	*
	* You also need to add mapping rules for this event type to the ADBC adapter's configuration file.
	* 
	* @param  timeColumn The name of the database table's column that contains the event's time stamp.
	*
	*/
	action setTimeColumn(string timeColumn)
	{
		_timeColumn := timeColumn;
	}

	/**
	* Set the query results to be back-testable in the data player.
	*
	* @param backtestSpeed The playback speed of the events.
	*/
	action makeBacktest(float backtestSpeed)
	{
		_backtestSpeed := backtestSpeed;
		_backtest := true;
	}

	/**
	* Set the Batch Size If the query will return a large number of results.
	*
	* If you set a batchsize, also use the Query event's setBatchDoneCallback 
	* action passing in values for the token and callback parameters. 
	*
	* @param  batchSize The maximum size of a batch.
	*
	*/
	action setBatchSize(integer batchSize)
	{
		_batchSize := batchSize;
	}

	/**
	* The following action has been added for running playback queries.
	*
	* @deprecated [deprecated as this is an unnecessary method.]
	*
	* @param  runUntilTime The time the playback query should run.
	*
	*/
	action setRunUntilTime(float runUntilTime)
	{
		_runUntilTime := runUntilTime;
	}

	/**
	* Set the input parameters for this query.
	* This should be set when using a prepared query.
	*
	* @param  inputParams The values to be used for the replaceable parameters.
	*
	*/
	action setInputParams(sequence<string> inputParams)
	{
		_inputParams := inputParams;
	}

	/**
	* Additional parameters for the query.
	*
	* @param  extra This is currently a container to allow additional information in the future.
	*
	*/
	action setExtraParams(dictionary<string,string> extra)
	{
		_extraParams := extra;
	}

	/**
	* Set the callback action to handle status notifications.
	*
	* @param  statusFreq The frequency of status events.
	* @param  callback Callback action.
	*
	*/
	action setStatusCallback(
			float statusFreq,
			action<integer,float> callback)
	{
		if _statusListenerActive then {
			_statusListener.quit();
		}
		
		QueryStatus status;
		_statusFreq := statusFreq;
		_statusListenerActive := true;

		_statusListener := on all QueryStatus(messageId = _id) : status {
			callback(status.eventCount, status.lastEventTime);
		}
	}

	/**
	* Set the callback action to handle schema notifications.
	*
	* @param  callback Callback action.
	*
	*/
	action setSchemaCallback(
			action< sequence<string>,
				dictionary<string,string>,
				sequence<string> > callback)
	{
		if _schemaListenerActive then {
			_schemaListener.quit();
		}
		
		ResultSchema schema;
		_schemaListenerActive := true;

		_schemaListener := on all ResultSchema(messageId = _id) : schema {
			callback(schema.fieldOrder, schema.fieldTypes,schema.indexFields);
		}
	}

	/**
	* Set the callback action to handle batch done notifications.
	*
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  callback Callback action.
	*
	*/
	action setBatchDoneCallback(
			string token,
			action<Query,string,integer,float,string, string> callback)
	{
		if _batchListenerActive then {
			_batchListener.quit();
		}
		
		BatchDone batch;
		_batchListenerActive := true;
		_batchDoneToken := token;

		_batchListener := on all BatchDone(messageId = _id) : batch {
			callback(self, batch.reason, batch.eventCount, batch.lastEventTime, batch.lastEvent, token);
		}
	}

	/**
	* Set the callback action to handle result event notifications.
	*
	* @param  token A user-defined string to be passed in that will be returned in the callback action.
	* @param  callback Callback action.
	*
	*/
	action setResultEventCallback(
			string token,
			action<Query,ResultEvent,string> callback)
	{
		if _resultListenerActive then {
			_resultListener.quit();
		}
		
		ResultEvent res;
		_resultListenerActive := true;
		_queryResultToken := token;

		_resultListener := on all ResultEvent(messageId = _id) : res{
			callback(self, res, token);
		}
	}

	/** @private */
	action quitListeners()
	{
		if _statusListenerActive then {
			_statusListener.quit();
			_statusListenerActive := false;
		}
		if _batchListenerActive then {
			_batchListener.quit();
			_batchListenerActive := false;
		}
		if _schemaListenerActive then {
			_schemaListener.quit();
			_schemaListenerActive := false;
		}
		if _resultListenerActive then {
			_resultListener.quit();
			_resultListenerActive := false;
		}
		if _queryDoneListenerActive then {
			_queryDoneListener.quit();
			_queryDoneListenerActive := false;
		}

		//The query has been somehow stopped so it's no longer necessary to trigger the callback on recovery
		_queryDoneCallback.clear();
	}
	
	/**
	* Reset the state of this query.
	*
	* Reset including all listeners and allow another Query to be setup.
	*/
	action reset() 
	{
		quitListeners();
		_stopped := true;
	}
	
	/**
	* OnBeginRecovery is called after all objects and listeners of persistent monitors have been recovered 
	* and before queued events are processed.
	*/
	action onBeginRecovery() 
	{
		reset();
	}
	
	/**
	* OnConcludeRecovery is called after the recovery process has been completed, 
	* including all non persistent monitors having being re-loaded.
	*/
	action onConcludeRecovery() 
	{
		integer id;
		for id in _queryDoneCallback.keys() {
			_queryDoneCallback[id](self, "Correlator recovered", 0, 0.0); 
		}
		_queryDoneCallback.clear();
	}
	
	/**
	* Start the execution of this query, with information passed
	* in using set and init actions.
	*
	* @param  callback Callback action.
	*
	*/
	action start(
			action<Query,string,integer,float> callback)
	{
		_setExecutionMode();

		if not _initialized then {
			callback(self, "Query not initialized",0,0.0);
			return;
		}
		else if not _stopped then {
			callback(self, "Query already running",0,0.0);
			return;			
		}

		_queryDoneCallback.add(_id, callback);

		QueryDone done;
		_queryDoneListenerActive := true;
		_queryDoneListener := on QueryDone(messageId = _id) : done {
			_queryDoneListenerActive := false;
			reset();
			callback(self, done.errorMessage, done.eventCount, done.lastEventTime);
		}

		StartQuery start := new StartQuery;
		start.messageId				:= _id;
		start.serviceId				:= _conn._serviceId;
		start.databaseId			:= _conn.getDbId();
		start.query					:= _queryString;
		start.namedQueryParameters	:= _queryTemplateParams;
		start.namedQuery			:= _isNamedQuery;
		start.returnType			:= _returnType;
		start.eventType				:= _eventType;
		start.backtest				:= _backtest;
		start.statusFrequency		:= _statusFreq;
		start.batchSize				:= _batchSize;
		start.batchSizeIncludesTimeEvents	:= _batchSizeIncludesTimeEvents;
		start.timeColumn			:= _timeColumn;
		start.backtestSpeed			:= _backtestSpeed;
		start.runUntilTime			:= _runUntilTime;
		start.extraParams			:= _extraParams;
		if _usePreparedQuery then {
			start.preparedQueryId := _preparedQuery.getPreparedQueryId();
			if _inputParams.size() > 0 then {
				start.inputParameters := _inputParams;
			}
		}
		
		if (_serialExecution) then {
			route start;
		}
		else {			
			send StartQueryToContext(context.current(), start) to _preSpawnContext;
		}

		_stopped := false;
	}

	/**
	 * Stops the running query.
	 *
	 * Return true if the query is already stopped
	 * else false if it has not yet stopped.
	 */
	action stop() returns boolean
	{
		return stopFull(new dictionary<string,string>);
	}
	
	/**
	* Pause the query.
	*
	* See also PauseQueryFull.
	*/
	action pause()
	{
		pauseFull(new dictionary<string,string>);
	}

	/**
	* Retrieve the next batch of results.
	* 
	* The number of which is based on the batchSize, and will 
	* result in that amount of results to be sent to the 
	* dedicated callback method which was set up using setBatchCallback.
	*
	* @param  batchSize The maximum size of a batch.
	*
	* See also getNextBatchFull.
	*/
	action getNextBatch(integer batchSize)
	{		
		getNextBatchFull(0.0, batchSize, false, 0.0, 0.0, new dictionary<string,string>);
	}

	/**
	* Returns the mesageId of the query.
	*/
	action getId() returns integer
	{
		return _id;
	}

	/**
	* Retrieves the token that has been assigned to the queryResult callback action.
	*
	* @deprecated [deprecated as this is an unnecessary method.]
	*/
	action getQueryResultToken() returns string
	{
		return _queryResultToken;
	}

	/**
	* Retrieves the token that has been assigned to the batchDone callback action.
	*
	* @deprecated [deprecated as this is an unnecessary method.]
	*/
	action getBatchDoneToken() returns string
	{
		return _batchDoneToken;
	}

	/**
	* Stops the running query.This action allows you to 
	* pass in any extra parameters.
	*
	* Return true if the query is already stopped
	* else false if it has not yet stopped.
	*
	* @param  extraParams This is currently a container to allow additional information in the future.
	*
	*/
	action stopFull(
			dictionary<string,string> extraParams) returns boolean
	{
		_setExecutionMode();

		if not _stopped then {
			StopQuery stop := new StopQuery;
			stop.messageId		:= _id;
			stop.serviceId		:= _conn.getServiceId();
			stop.extraParams	:= extraParams;
			
			if (_serialExecution) then {
				route stop;
			}
			else {			
				send StopQueryToContext(context.current(), stop) to _preSpawnContext;
			}
		}

		// May be redundent as QueryDone listener does this, but need to do as QueryDone might not have been received
		quitListeners();

		return _stopped;
	}

	/**
	* Pauses the running query.
	*
	* @param  extraParams This is currently a container to allow additional information in the future.
	*
	*/
	action pauseFull(
			dictionary<string,string> extraParams)
	{
		_setExecutionMode();

		if _stopped then {
			return;
		}
		
		PauseQuery pause := new PauseQuery;
		pause.messageId		:= _id;
		pause.serviceId		:= _conn.getServiceId();
		pause.extraParams	:= extraParams;
		
		if (_serialExecution) then {
			route pause;
		}
		else {			
			send PauseQueryToContext(context.current(), pause) to _preSpawnContext;
		}
	}

	/**
	* Retrieve the next batch of results.This action allows you to 
	* pass in any extra parameters.
	*
	* The number of which is based on the batchSize, and will 
	* result in that amount of results to be sent to the 
	* dedicated callback method which was set up using setBatchCallback.
	*
	* @param  statusFrequency The frequency of status events.
	* @param  batchSize The maximum size of a batch.
	* @param  batchSizeIncludesTimeEvents If true the batch size will count &Time events, if false it will not.
	* @param  backtestSpeed Speed for back testing: <= 0.0 is as fast as possible, > 0.0 is some multiple of playback speed.
	* @param  runUntilTime The time the playback query should run.
	* @param  extraParams This is currently a container to allow additional information in the future.	
	*
	*/
	action getNextBatchFull(
			float statusFrequency,
			integer batchSize,
			boolean batchSizeIncludesTimeEvents,
			float backtestSpeed,
			float runUntilTime,
			dictionary<string,string> extraParams)
	{
		_setExecutionMode();

		if _stopped then {
			return;
		}
		
		// Update private fields
		_statusFreq		:= statusFrequency;
		_batchSize		:= batchSize;
		_batchSizeIncludesTimeEvents	:= batchSizeIncludesTimeEvents;
		_backtestSpeed	:= backtestSpeed;
		_runUntilTime	:= runUntilTime;

		GetNextBatch getNext := new GetNextBatch;
		getNext.messageId		:= _id;
		getNext.serviceId		:= _conn.getServiceId();
		getNext.statusFrequency	:= _statusFreq;
		getNext.batchSize		:= _batchSize;
		getNext.batchSizeIncludesTimeEvents	:= _batchSizeIncludesTimeEvents;
		getNext.backtestSpeed	:= _backtestSpeed;
		getNext.runUntilTime	:= _runUntilTime;
		getNext.extraParams		:= extraParams;
		
		if (_serialExecution) then {
			route getNext;
		}
		else {			
			send GetNextBatchToContext(context.current(), getNext) to _preSpawnContext;
		}
	}

	/** @private */
	action _setExecutionMode() {
		if not _preinitialized then {
			if (_preSpawnContext.getId() = 0 or _preSpawnContext.getId() = context.current().getId()) then {
				_serialExecution := true;
			}
			else {
				_serialExecution := false;
			}
			_preinitialized := true;
		}
	}

	//
	// The following fields are considered "private"
	//
	/** @private */
	context 	_preSpawnContext;
	/** @private */
	boolean 	_preinitialized;
	/** @private */
	boolean 	_serialExecution;
	/** @private */
	integer 	_id;
	/** @private */
	boolean 	_initialized;
	/** @private */
	Connection 	_conn;
	/** @private */
	string 		_queryString;
	/** @private */
	boolean 	_isNamedQuery;
	/** @private */
	dictionary<string,string> _queryTemplateParams;
	/** @private */
	string 		_returnType;
	/** @private */
	string 		_eventType;
	/** @private */
	boolean 	_backtest;
	/** @private */
	float 		_statusFreq;
	/** @private */
	integer 	_batchSize;
	/** @private */
	boolean 	_batchSizeIncludesTimeEvents;
	/** @private */
	float 		_backtestSpeed;
	/** @private */
	string 		_timeColumn;
	/** @private */
	float 		_runUntilTime;
	/** @private */
	string		_queryResultToken;
	/** @private */
	string		_batchDoneToken;
	/** @private */
	PreparedQuery _preparedQuery;
	/** @private */
 	boolean		_usePreparedQuery;
	/** @private */
	sequence<string> _inputParams;
	/** @private */
	dictionary<string,string> _extraParams; // Optional query parameters
	/** @private */
	dictionary<integer, action<Query,string,integer,float> > _queryDoneCallback;
	/** @private */
	boolean		_batchListenerActive;
	/** @private */
	listener	_batchListener;
	/** @private */
	boolean 	_statusListenerActive;
	/** @private */
	listener	_statusListener;
	/** @private */
	boolean		_schemaListenerActive;
	/** @private */
	listener 	_schemaListener;
	/** @private */
	boolean		_resultListenerActive;
	/** @private */
	listener 	_resultListener;
	/** @private */
	listener 	_queryDoneListener;
	/** @private */
	boolean		_queryDoneListenerActive;
	/** @private */
	boolean		_stopped;
}
 00000034 C:\SoftwareAG\Apama\adapters\monitors\ADBCEvents.mon
TIME 0000000e 1474016712.3,1
MONF 00005d73 package com.apama.database;

//*****************************************************************************
// ADBC (Apama Database Connector) Status Manager service.
//
// Manages status subscriptions for the ADBC adapter and the application.
//
// $Copyright(c) 2009-2012 Progress Software Corporation (PSC). All rights reserved.$
// $Copyright (c) 2013,2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
// Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
//
// Version:  1.0
//
// Requires: IAFStatusManager.mon, StatusSupport.mon, ADBCEvents.mon
//*****************************************************************************

using com.apama.adapters.AdapterError;
using com.apama.adapters.AdapterStatusDeregister;
using com.apama.adapters.AdapterStatusRegister;
using com.apama.adapters.AdapterUp;
using com.apama.adapters.ConnectionClosed;
using com.apama.adapters.ConnectionOpened;
using com.apama.database.DataSource;
using com.apama.database.Discovery;
using com.apama.database.RequestDataSources;
using com.apama.statusreport.Status;
using com.apama.statusreport.StatusError;
using com.apama.statusreport.SubscribeStatus;
using com.apama.statusreport.UnsubscribeStatus;

/**
 * Send this event from any service monitor used with the ADBC adapter.
 *
 * If the service monitor is unloaded (send from unload or ondie if
 * no spawning within the monitor), this will cause the ADBCStatusManager to
 * notify the application and discontinue the sending of status
 * information. This is purely optional.
 */
event ServiceMonitorUnloaded
{
	/** The name of the monitor. */
	string monitorName;
	/** Optional message with additional details. */
	string message;
}

/**
 * Send this event from any service monitor used with the ADBC adapter
 * when the monitor comes online.  
 *
 * The ADBCStatusManager only caches monitors that have died and waits for
 * them to come back online before resuming status information passing,
 * so there is no requirement for the ADBCStatusManager to be injected before
 * other services for ADBC.  This is purely optional.
 */
event ServiceMonitorOnline
{
	/** The name of the monitor. */
	string monitorName;
}

/**
 * Internal event for adapter instance management
 * @private
 */
event _AdapterStatus
{
	/** The name of the adapter. */
	string name;
	/** True if the adapter is up. */
	boolean isAdapterUp;
	/** True if the adapter is connected. */
	boolean isConnected;
	/** Not curently set. */
	boolean isLoggedOn;
	/** Not curently set. */
	boolean isOpen;
	/** Indicates a change in the current connection is happening. */
	boolean connectionChange;
	/** True if the connection is closed. */
	boolean connectionClose;
	/** The connection name. */
	string connection;
	/** Keep track of the number of status subscriptions from the application
	* so we won't stop sending status events until all subscribers are gone. */
	integer statusSubscriptionCount;
	/** The status of the codec. */
	dictionary<string, string> codecStatus;
	/** The status of the transport. */
	dictionary<string, string> transportStatus;
	/** The status event. */
	Status statusEvent;
	/** The status error event. */
	StatusError statusErrorEvent;
}

/**
 * This monitor implements both sides of the
 * status interface, i.e., the IAFStatusManager events on the adapter
 * side, and the StatusSupport events on the application side.
 *
 * @routes AdapterStatusRegister, AdapterStatusDeregister
 * @listens SubscribeStatus, AdapterUp, RequestDataSources, 
 *    UnsubscribeStatus, ConnectionOpened, ConnectionClosed, AdapterError,
 *    ServiceMonitorUnloaded
 * @sends Status, StatusError
 */
monitor ADBCStatusManager
{
	/** Constant - prefix of the adapter name. */
	string ADAPTER_NAME_PREFIX := "ADBC-";
	/** Constant - Transport name.
	* @private */
	string TRANSPORT_NAME := "ADBCTransport";
	/** Constant - code name.
	* @private */
	string CODEC_NAME := "NullCodec";
	/** Constant - monitor name.
	* @private */
	string MONITOR_NAME := "ADBCStatusManager";
	/** Constant - Single connection string.
	* @private */
	string SINGLE_CONNECTION_STRING := "CONNECTION";
	/** Constant - Multiple connection string.
	* @private */
	string MULTIPLE_CONNECTION_STRING := "CONNECTION_";

	/** Could be used in the summary for the status event to the application. 
	* @private */
	string CONNECTED := "Connected";
	/** Could be used in the summary for the status event to the application. 
	* @private */
	string LOGGED_ON := "Logged On";
	/** Could be used in the summary for the status event to the application. 
	* @private */
	string OPEN := "Open";

	/**
	* This is prepended to the front of the status dictionary entires
	* for the codec when merging these into one dictionary
	* for the status events to the application.
	*/
	string CODEC_DICT_PREFIX := "CODEC_";
	/**
	* This is prepended to the front of the status dictionary entires
	* for the transport when merging these into one dictionary
	* for the status events to the application.
	*/
	string TRANSPORT_DICT_PREFIX := "TRANSPORT_";

	/** 
	* Adapters that have been registered. 	
	*
	* @private
	*/
	dictionary<string, _AdapterStatus> adapters := new dictionary<string, _AdapterStatus>;

	/** 
	* Adapters that have not yet been registered. 
	*
	* @private
	*/
	sequence<string> newAdapters := new sequence<string>;

	/** 
	* Count of requests for status from any ADBC adapter. 
	*
	* @private
	*/
	integer wildcardSubscriptionCount := 0;
	/** 
	* Requests for status from any ADBC adapter. 
	*
	* @private
	*/
	dictionary<string,integer> wildcardSubscriptions := new dictionary<string, integer>;

	/** 
	* Discovery listener (active until an adapter is found). A second 
	* discovery attempt is made after an adapter is found to allow for 
	* start up of multiple adapters. 
	*
	* @private
	*/
	listener discoveryListener;

	// General status support.
	/** @private */
	boolean haveDeadMonitors := false;
	/** @private */
	boolean discoveryRunning := false;
	/** @private */
	dictionary<string, string> deadServiceMonitors := new dictionary<string, string>;
	/** @private */
	ServiceMonitorUnloaded monitorUnloaded;
	/** @private */
	ServiceMonitorOnline monitorOnline;
	/** @private */
	AdapterUp adapterUpEvent;
	/** @private */
	AdapterError adapterErrorEvent;
	/** @private */
	ConnectionOpened connectionOpened;
	/** @private */
	ConnectionClosed connectionClosed;
	/** @private */
	Discovery discover;
	/** @private */
	RequestDataSources dsReq;
	/** @private */
	SubscribeStatus subscribe;

	/**
	* Sets up the necessary discovery and subscription listeners.
	*
	* @listens SubscribeStatus, AdapterUp, RequestDataSources.
	*/
	action onload()
	{
		log "Loaded " + MONITOR_NAME at INFO;

		discoveryRunning := true;
		discover := new Discovery;
		discoverAdapters();

		establishServiceMonitorListeners();
		monitor.subscribe("SubscribeStatus");
		// Setup wildcard listener for when any ADBC adapter becomes available
		on all SubscribeStatus() : subscribe {
			if subscribe.serviceID = "" or subscribe.serviceID = "ADBC" then {
				if not wildcardSubscriptions.hasKey(subscribe.serviceID) then {
					wildcardSubscriptionCount := wildcardSubscriptionCount + 1;
					establishWildcardStatusListeners(subscribe.serviceID);
				}
			}
		}

		/**
		* Rerun discovery if a new ADBC adapter starts up
		*/
		on all AdapterUp():adapterUpEvent
		{
			if adapterUpEvent.adapterName.find("ADBC") >= 0 and not adapters.hasKey(adapterUpEvent.adapterName) then {
				log adapterUpEvent.adapterName + " adapter is up: " + adapterUpEvent.toString() at DEBUG;

				// Add adapter to list of new adapters not discovered yet
				if newAdapters.indexOf(adapterUpEvent.adapterName) < 0 then {
					newAdapters.append(adapterUpEvent.adapterName);
				}

				// Rerun discovery to get details on new ADBC adapter
				if not discoveryRunning then {
					discoverAdapters();
				}
			}
		}

		// Listen for all discovery requests and unload this monitor
		// if the Data player is being used.
		on all RequestDataSources(): dsReq {
			// The DP will add "xclock:true" to the request's extra params
			if dsReq.extraParams.hasKey("xclock") then {
				if dsReq.extraParams["xclock"].toBoolean() then {
					// Unload the Status monitor
					log "Data Player using externally clocked correlator, stopping monitor." at INFO;
					die;
				}
			}
		}
	}

	/** 
	* Find all ADBC adapters connected to the correlator.
	*/
	action discoverAdapters()
	{
		// Initial discovery request.
		discover.findAvailableDataSources(0.0, discoveryHandler);

		// Repeating listener to look for an adapter until one is found.
		discoveryListener := on all wait(5.0) {
			// Retry discovery then wait 5 seconds for responses
			discover.findAvailableDataSources(3.0, discoveryHandler);
		}
	}

	/** 
	* On discovery of an ADBC Adapter connected to the correlator, if there 
	* are no errors attempt to register for status monitoring.
	*
	* @private
	*/
	action discoveryHandler(string error, sequence<DataSource> dataSources)
	{
		if error.length() != 0 then {
			log "Error occurred in ADBC adapter discovery: " + error at ERROR;
		}
		else {
			if dataSources.size() > 0 then {

				boolean oneFound := adapters.size() > 0;
				DataSource ds;
				for ds in dataSources {

					log "ADBC adapter found: " + ds.toString() at DEBUG;

					string name := ADAPTER_NAME_PREFIX + ds.name;

					// Use codec and transport names if provided.
					string codecName := CODEC_NAME;
					string transportName := TRANSPORT_NAME;
					if ds.extraParams.hasKey("codecName") then {
						codecName := ds.extraParams["codecName"];
					}
					if ds.extraParams.hasKey("transportName") then {
						transportName := ds.extraParams["transportName"];
						name := transportName;
					}

					// Skip if adapter is already registered
					if adapters.hasKey(name) then {
						continue;
					}

					// Remove from list if this is a known new adapter
					integer index := newAdapters.indexOf(name);
					if index >= 0 then {
						newAdapters.remove(index);
					}

					// Initialize status event
					_AdapterStatus status := new _AdapterStatus;
					status.name := name;
					status.statusEvent := new Status;
					status.statusErrorEvent := new StatusError;
					status.isAdapterUp := true;

					// Add adapter status to known list
					adapters.add(name, status);

					// Setup listeners
					establishStatusListeners(name, ds.serviceId);

					// Register for IAFStatus events
					route AdapterStatusRegister(name, codecName, transportName, "", "", "", ds.serviceId);
					log "Adapter " + name + "(" + codecName + "," + transportName + ") registered for status monitoring." at INFO;
				}

				// Don't terminate listener until the second discovery attempt.
				// This should allow enough time for start up of multiple adapters
				if oneFound and newAdapters.size() = 0 then {
					discoveryListener.quit();
					discoveryRunning := false;
				}
			}
			else {
				log "ADBC adapter to register not found." at DEBUG;
				discoveryListener.quit();
				// Continue to look for an adapter until one is found.
				discoveryListener := on all wait(15.0) {
					// Retry discovery then wait 5 seconds for responses
					discover.findAvailableDataSources(5.0, discoveryHandler);
				}
			}
		}
	}

	/**
	 * Create all the listeners necessary for the status implementation on
	 * both sides of the service monitor, i.e. the IAFStatusManager on the
	 * adapter side and the StatusSupport interface on the application side.
	 */
	action establishStatusListeners(string name, string serviceName)
	{

		adapters[name].statusEvent.serviceID := serviceName;
		adapters[name].statusErrorEvent.serviceID := serviceName;

		// TODO: Handle the complete matrix of subscription requests.
		// Handle (un)subscriptions from the application for status.
		// Don't worry about object, subserviceid, and connection for
		// now.
		on all SubscribeStatus(serviceID = serviceName)
		{
			adapters[name].statusSubscriptionCount := adapters[name].statusSubscriptionCount + 1;

			// When an application first subscribes, send out a status.
			sendStatusReport(name);
		}

		on all UnsubscribeStatus(serviceID = serviceName)
		{
			if adapters[name].statusSubscriptionCount > 0 then {
				adapters[name].statusSubscriptionCount := adapters[name].statusSubscriptionCount - 1;
			}
		}

		// The rest are for the adapter side, i.e., with the IAFStatusManager.

		on all ConnectionOpened(adapterName = name):connectionOpened
		{
			log name + " connection opened: " + connectionOpened.connectionName +
				" "  + connectionOpened.connectionGeneration at INFO;

			// Update status info and send to application.
			adapters[name].isAdapterUp := true;
			adapters[name].isConnected := true;
			adapters[name].connectionChange := true;
			adapters[name].connectionClose := false;
			adapters[name].connection := connectionOpened.connectionName;

			// Update our cached information for the status event.
			if (connectionOpened.connectionName = "") then
			{
				adapters[name].transportStatus.add(SINGLE_CONNECTION_STRING, connectionOpened.connectionGeneration);
			}
			else if (not adapters[name].transportStatus.hasKey(MULTIPLE_CONNECTION_STRING + connectionOpened.connectionName)) then
			{
				adapters[name].transportStatus.add(MULTIPLE_CONNECTION_STRING + connectionOpened.connectionName, connectionOpened.connectionGeneration);
			}

			sendStatusReport(name);

			// Reset connection changed indicators
			adapters[name].connectionChange := false;
			adapters[name].connectionClose := false;
			adapters[name].connection := "";

		}

		on all ConnectionClosed(adapterName = name):connectionClosed
		{
			log name + " connection closed: " + connectionClosed.connectionName +
			" "  + connectionClosed.connectionGeneration at INFO;

			// Update status info and send to application.
			adapters[name].isAdapterUp := true;
			adapters[name].isConnected := false;
			adapters[name].connectionChange := true;
			adapters[name].connectionClose := true;
			adapters[name].connection := connectionClosed.connectionName;

			// Update our cached information for the status event.
			if (connectionClosed.connectionName = "" and adapters[name].transportStatus.hasKey(SINGLE_CONNECTION_STRING)) then
			{
				adapters[name].transportStatus.remove(SINGLE_CONNECTION_STRING);
			}
			else if (adapters[name].transportStatus.hasKey(MULTIPLE_CONNECTION_STRING + connectionClosed.connectionName)) then
			{
				adapters[name].transportStatus.remove(MULTIPLE_CONNECTION_STRING + connectionClosed.connectionName);
			}

			// Check if we are dealing with multiple connections and if there are
			// still connections left.  It's possible that we can still have
			// another connection open and we don't want to declare disconnected.
			string key;

			for key in adapters[name].transportStatus.keys()
			{
				if (key.find(MULTIPLE_CONNECTION_STRING) >= 0) then
				{
					adapters[name].isConnected := true;
					break;
				}
			}

			sendStatusReport(name);

			// Reset connection changed indicators
			adapters[name].connectionChange := false;
			adapters[name].connectionClose := false;
			adapters[name].connection := "";
		}

		on all AdapterUp(adapterName = name):adapterUpEvent
		{
			// Log only if this is a change in status
			if not adapters[name].isAdapterUp then {
				log name + " adapter is up: " + adapterUpEvent.codecStatus.toString() +
					" " + adapterUpEvent.transportStatus.toString() at INFO;
			}

			// Update status info and send to application.
			adapters[name].isAdapterUp := true;

			// Cache these for use with the status report when a connection
			adapters[name].codecStatus := adapterUpEvent.codecStatus;
			adapters[name].transportStatus := adapterUpEvent.transportStatus;

			sendStatusReport(name);
		}

		on all AdapterError(adapterName = name):adapterErrorEvent
		{
			// Log only if this is a change in status
			if adapters[name].isAdapterUp then {
				log name + " adapter is down: " + adapterErrorEvent.description at INFO;
			}

			// Update status info and send to application.
			adapters[name].isAdapterUp := false;
			adapters[name].isConnected := false;

			sendStatusReport(name);
		}
	}

	/**
	 * Create all the listeners necessary for the status implementation on
	 * both sides of the service monitor, i.e. the IAFStatusManager on the
	 * adapter side and the StatusSupport interface on the application side.
	 */
	action establishWildcardStatusListeners(string serviceName)
	{

		if not wildcardSubscriptions.hasKey(serviceName) then {
			wildcardSubscriptions.add(serviceName, 1);

			// When an application first subscribes, send out status.
			string name;
			for name in adapters.keys() {
				sendStatusReport(name);
			}
		}

		on all SubscribeStatus(serviceID = serviceName)
		{
			// Check for any collisions with specific adapter names
			if not adapters.hasKey(serviceName) then {
				if wildcardSubscriptions.hasKey(serviceName) then {
					wildcardSubscriptions[serviceName] := wildcardSubscriptions[serviceName] + 1;
					wildcardSubscriptionCount := wildcardSubscriptionCount + 1;
				}

				// When an application first subscribes, send out a status.
				// for all known adapters
				string name;
				for name in adapters.keys() {
					sendStatusReport(name);
				}
			}
		}

		on all UnsubscribeStatus(serviceID = serviceName)
		{
			// Check for any collisions with specific adapter names
			if not adapters.hasKey(serviceName) then {
				if wildcardSubscriptions.hasKey(serviceName) then {
					if wildcardSubscriptions[serviceName] > 0 then {
						wildcardSubscriptions[serviceName] := wildcardSubscriptions[serviceName] - 1;
						if wildcardSubscriptionCount > 0 then {
							wildcardSubscriptionCount := wildcardSubscriptionCount - 1;
						}
					}
				}
			}
		}
	}

	/**
	 * Create all the listeners necessary for any other service monitors
	 * associated with this.
	 */
	action establishServiceMonitorListeners()
	{

		// If we get this event from inside this package, we have a service
		// monitor down for this adapter.  We can't run without it, so send an
		// error event and stop sending status information from the adapter.
		on all ServiceMonitorUnloaded():monitorUnloaded
		{
			if (not deadServiceMonitors.hasKey(monitorUnloaded.monitorName)) then
			{
				deadServiceMonitors.add(monitorUnloaded.monitorName, monitorUnloaded.message);
			}

			haveDeadMonitors := true;
			string message := "ADBC: Service monitor " + monitorUnloaded.monitorName +
				" has been unloaded!  Can't continue until it is reloaded.  " + monitorUnloaded.message;
			string name;
			for name in adapters.keys() {
				adapters[name].statusErrorEvent.description := message;
				adapters[name].statusErrorEvent.failed := false;
				send adapters[name].statusErrorEvent to StatusError.CHANNEL;
			}
			log message at INFO;

		}

		// The service monitor has come back up, allow status message to
		// go through again.
		on all ServiceMonitorOnline():monitorOnline
		{
			if (deadServiceMonitors.hasKey(monitorOnline.monitorName)) then
			{
				deadServiceMonitors.remove(monitorOnline.monitorName);
			}

			if (deadServiceMonitors.size() = 0) then
			{
				haveDeadMonitors := false;
			}

			string name;
			for name in adapters.keys() {
				sendStatusReport(name);
			}

			log "ADBC: Service monitor " + monitorOnline.monitorName + " back online." at INFO;

		}
	}

	/**
	 * Send a Status event if we have subscribers.
	 */
	action sendStatusReport(string name)
	{
		adapters[name].statusEvent.summaries := new sequence<string>;
		adapters[name].statusEvent.connection := "";

		if (adapters[name].statusSubscriptionCount > 0 or wildcardSubscriptionCount > 0) then
		{
			if (adapters[name].isAdapterUp and not haveDeadMonitors) then
			{
				if (adapters[name].isConnected) then
				{
					adapters[name].statusEvent.description := name + " adapter up and connected to data source.";
					adapters[name].statusEvent.available := true;
					adapters[name].statusEvent.summaries.append(CONNECTED);
					if (adapters[name].connectionChange) then
					{
						string state := "opened.";
						if (adapters[name].connectionClose) then
						{
							state := "closed.";
						}
						adapters[name].statusEvent.connection := adapters[name].connection;
						adapters[name].statusEvent.description := name + " connection " + state;
					}
				}
				else
				{
					if (adapters[name].connectionChange) then
					{
				   		adapters[name].statusEvent.connection := adapters[name].connection;
					}
					adapters[name].statusEvent.description := name + " adapter up but not connected to a data source.";
					adapters[name].statusEvent.available := false;
				}

				if (adapters[name].isLoggedOn) then
				{
					adapters[name].statusEvent.summaries.append(LOGGED_ON);
				}

				if (adapters[name].isOpen) then
				{
					adapters[name].statusEvent.summaries.append(OPEN);
				}

				adapters[name].statusEvent.extraParams := mergeCodecAndTransportStatusDictionaries(adapters[name].codecStatus, adapters[name].transportStatus);

				send adapters[name].statusEvent to Status.CHANNEL;
			}
			else if (not haveDeadMonitors) then
			{
				adapters[name].statusErrorEvent.description := name + ": " + adapterErrorEvent.description;
				adapters[name].statusErrorEvent.failed := false;

				send adapters[name].statusErrorEvent to StatusError.CHANNEL;
			}
			else
			{
				// In this case we have dead monitors; don't send any status at all
				// as we've already sent an error and we don't want to give false
				// information about the health of the system if, say, the adapter is
				// up and a service monitor is down.
			}
		}
	}


	/**
	 * Utility action for merging the two status dictionaries from the codec and transport.
	 *
	 * Some keys in these dictionaries may be identical.  For this reason we prepend a
	 * prefix to the front of all keys notifying the application which keys are from which
	 * status dictionary; transport or codec.  See CODEC_DICT_PREFIX and TRANSPORT_DICT_PREFIX.
	 */
	action mergeCodecAndTransportStatusDictionaries(dictionary<string, string> codecStatus, dictionary<string, string> transportStatus) returns dictionary<string, string>
	{
		dictionary<string, string> mergedStatus := new dictionary<string, string>;
		string key;

		for key in codecStatus.keys()
		{
			mergedStatus.add(CODEC_DICT_PREFIX + key, codecStatus[key]);
		}

		for key in transportStatus.keys()
		{
			mergedStatus.add(TRANSPORT_DICT_PREFIX + key, transportStatus[key]);
		}

		return mergedStatus;
	}


	/**
	 * Handle any cleanup.
	 */
	action ondie()
	{
		string name;
		for name in adapters.keys() {

			// Unsubscribe from IAFStatusManager
			route AdapterStatusDeregister(name);

			// notify application
			adapters[name].statusErrorEvent.description := MONITOR_NAME + " is terminating.";
			adapters[name].statusErrorEvent.failed := true;
			send adapters[name].statusErrorEvent to StatusError.CHANNEL;
		}
		log MONITOR_NAME + " has been asked to terminate.  Removing monitor now." at INFO;
	}
}
 0000003b C:\SoftwareAG\Apama\adapters\monitors\ADBCStatusManager.mon
MONF 0000cc7c //*****************************************************************************
// ADBC (Apama Database Connector) Helper Events.
//
// Public helper utility for use with the ADBC adapter.
//
// $Copyright(c) 2009-2013 Progress Software Corporation (PSC). All rights reserved.$
// $Copyright (c) 2013, 2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
// Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
//
// Author:   Mark Scannell, Frank Stoessel
// Version:  1.0
// Requires: ADBCEvents.mon, StatusSupport.mon
//
//*****************************************************************************

package com.apama.database;

/**
 *
 * DBUtil actions:
 *
 *  Use setPrespawnContext to determine what the main context is (before spawning)
 *   setPrespawnContext( context c )
 *
 *  Whether to log all queries (and SQLCmd and SQLCommit) to log file or not
 *   action setLogQueries( boolean logQueries )
 *
 *  Quick open operations - default batch size, auto commit, etc.
 *  Pass in the default error handler.
 *   action openQuickJDBC( string URL, string user, string password, action < string > handleError )
 *   action openQuickODBC( string URL, string user, string password, action < string > handleError )
 *
 *  Set the instance name for the adapter to use with openQuickJDBC or openQuickODBC
 *  Only needed if using the openQuick actions with an adapter having a non-default instance name
 *   action setAdapterInstanceName( string instanceName )
 *
 *  Set the policy for the dealing with adapter connection errors. The reconnectPolicy parameter must be one of the constants from
 *  DBReconnectPolicy event. See DBReconnectPolicy event for the policy descriptions. The default reconnect policy is DO_NOT_RECONNECT.
 *   action setReconnectPolicy( string reconnectPolicy )
 *
 *  Set the timout for the reconnection after a conection error. The default value is set by the open action
 *   to be twice as long as its timeOut parameter
 *   action setReconnectTimeout(float timeOut)
 *
 *  Full open - specified serviceId, as well as autoCommit string, batchSize, timeOut for waiting for adapter, reconnect and cancelPendingRequestsOnReconnect
 *   action open( string serviceId, string URL, string user, string password, string autoCommit, boolean readOnly, integer batchSize, float timeOut, action < string > handleError)
 *
 *  Open shared - Use an already open existing matching connection if available or open a new one
 *   action openShared( string serviceId, string URL, string user, string password, string autoCommit, boolean readOnly, integer batchSize, float timeOut, action < string > handleError )
 *
 *  Execute SQL commands that expect no response.
 *	 action doSQLCmd( string queryString )
 *  The SQLCmdOnError specifies to execute this command *only* if the previous (non onError) SQL command failed
 *     - this is useful for doing, for example, "select * from table", and then OnError "create table ..".
 *   action doSQLCmdOnError( string queryString )
 *  The SQLCmdAck specifes an acknowledgement is requested for this operation, as well as whether it is triggered by an error.
 *     - using an ackNum of -1 will disable the acknowledgement and instead use the default error handler if an error occurs
 *	 action doSQLCmdAck( string queryString, integer ackNum, boolean onError )
 *
 *  Execute SQL Commit. Same as above except there is no OnError option.
 *   action doSQLCommit()
 *   action doSQLCommitAck( integer ackNum )
 *
 *  Execute SQL Rollback. Same as above except there is an OnError option.
 *   action doSQLRollback()
 *   action doSQLRollbackOnError()
 *   action doSQLRollbackAck( integer ackNum, boolean onError )
 *
 *  Execute SQL Query (expect response). Same as SQLCmd except there is a handleResult specified which is the monitor handler
 *  for the results set.
 *   action doSQLQuery( string queryString, action< dictionary< string, string > > handleResult )
 *   action doSQLQueryWithCallback( string queryString, action< dictionary< string, string > > handleResult, action < string, integer> handleDone )
 *   action doSQLQueryOnError( string queryString, action< dictionary< string, string > > handleResult )
 *   action doSQLQueryAck( string queryString, action < dictionary< string, string > > handleResult, integer ackNum, boolean onError )
 *
 *  Execute SQL Query for Events (expect response). Same as SQLQuery except events will be returned instead of the
 *  results set.
 *   action doSQLEventQuery( string queryString, string eventType )
 *   action doSQLEventQueryWithCallback( string queryString, string eventType, action < string, integer> handleDone )
 *   action doSQLEventQueryOnError( string queryString, string eventType )
 *   action doSQLEventQueryAck( string queryString, string eventType, integer ackNum, boolean onError )
 *
 *  The following getSchema* actions are only valid in the handleResult action specified above (when dealing with a returned row)
 *   action getSchemaFieldOrder()  returns sequence< string >
 *   action getSchemaFieldTypes()  returns dictionary< string, string >
 *   action getSchemaIndexFields() returns sequence< string >
 *
 *  Call stopAll(). This also cancels all outstanding queries in the queue.
 *   action stopAll()
 *
 *  Close the database. This also cancels all outstanding queries in the queue. It also prevents new queries from being placed into the queue.
 *   action close( boolean doStopAll )
 */

using com.apama.statusreport.SubscribeStatus;
using com.apama.statusreport.UnsubscribeStatus;
using com.apama.statusreport.Status;
using com.apama.statusreport.StatusError;

/**
* Internal (private) event
* @private
*/
event DBOperation {
	boolean isCommand;
	boolean isCommit;
	boolean isRollback;
	boolean isEventQuery;
	boolean onError;
	string queryString;
	string eventType;
	action< dictionary< string, string > > handleResult;
	action< string, integer > handleDone;
	integer ackNum;
}

/**
* Event sent when an acknowledgement is requested
* using the doSQLCmdAck, doSQLCommitAck, doSQLRollbackAck,
* doSQLQueryAck, or doSQLEventQueryAck actions.
*
*/
event DBAcknowledge
{
	/** The id of this acknowledge based on the original request. */
	integer ackNum;
	/** Indicates success or failure of the original request. */
	boolean success;
	/** Contains the error message if there was an error, emtpy if there is no error. */
	string  error;
}

/**
* An enumeration containing constants for the reconnection policy
* used when an adapter connection error occurs.
*
*/
event DBReconnectPolicy
{
	/**
	*  Try to reconnect and leave the management of pending requests
	*  to the client. The client can handle the pending requests
	*  in the error handler.
	*/
	constant string RECONNECT := "RECONNECT";
	/**
	*  Try to reconnect and leave the pending requests unchanged.
	*  Retry the last request upon the successful database reconnection.
	*/
	constant string RECONNECT_AND_RETRY_LAST_REQUEST := "RECONNECT_AND_RETRY_LAST_REQUEST";
	/**
	*  Try to reconnect and remove all the pending requests.
	*/
	constant string RECONNECT_AND_CLEAR_REQUEST_QUEUE := "RECONNECT_AND_CLEAR_REQUEST_QUEUE";
	/**
	*  Do not try to reconnect.
	*/
	constant string DO_NOT_RECONNECT := "DO_NOT_RECONNECT";
}

/**
 * Utility event to present a simplified and easier
 * to use event interface to ADBC.
 *
 */
event DBUtil
{
	/** @private */
	sequence< DBOperation > _queries;

	/** @private */
	Connection _conn;

	/** @private */
	integer _batchSize;
	/** @private */
	float   _timeOut;

	/** @private */
	boolean _isBusy;
	/** @private */
	boolean _isOpen;
	/** @private */
	boolean _closing;
	/** @private */
	boolean _wantsClose;
	/** @private */
	boolean _isStopping;
	/** @private */
	boolean _logQueries;
	/** @private */
	boolean _connError;
	/** @private */
	boolean _isReconnecting;
	/** @private */
	string  _type;
	/** @private */
	string  _reconnectPolicy;
	/** @private */
	float  _reconnectTimeOut;
	/** @private */
	float _triggerTheReconnectTimeout;
	/** @private */
	string _serviceId;
	/** @private */
	string _serviceId2; // Needed for new default ODBC/JDBC service names
	/** @private */
	string _adapterInstanceName;
	/** @private */
	integer _invalidId;
	
	/** @private */
	boolean _parallel;
	/** @private */
	context _preSpawnContext;

	/** @private */
	listener _queryListener;
	/** @private */
	listener _statusListener;
	/** @private */
	listener _connectionStatusListener;
	/** @private */
	listener _reopenListener;
	/** @private */
	listener _reopenTimeoutListener;
	/** @private */
	listener _triggerTheReconnectTimeoutListener;

	/** @private */
	action< string > handleError;

	/** @private */
	sequence<string>          _schema_fieldOrder;
	/** @private */
	dictionary<string,string> _schema_fieldTypes;
	/** @private */
	sequence<string>          _schema_indexFields;

	/**
	* Set context that is current prior to spawning.
	* Optional: Only needed when using parallel processing.
	*
	* @param  ctx Current context prior to spawning.
	*
	*/
	action setPrespawnContext( context ctx ) {
		_preSpawnContext := ctx;
		_parallel := true;

		_conn.initPreSpawnContext( ctx);
	}

	/**
	* Enable (or disable) the logging of all
	* operations (SQLQuery, SQLCmd and SQLCommit).
	* The default setting is disabled (false).
	*
	* @param logQueries If true will enable logging.
	*/
	action setLogQueries( boolean logQueries ) {
		_logQueries := logQueries;
	}

	/**
	* Set the adapter instance name for use with the openQuick actions.
	* This is only needed to use the openQuick actions with an ODBC/ODBC
	* adapter that is using an instance name other than the default (INSTANCE_1).
	*
	* @param  instanceName The ODBC/JDBC adapter instance name.
	*
	*/
	action setAdapterInstanceName(string instanceName) {
		_adapterInstanceName := instanceName;
	}

	/**
	* Set the policy for the dealing with adapter connection errors. 
	*
	* The reconnectPolicy parameter must be one of the constants from
	* DBReconnectPolicy event. See DBReconnectPolicy event for the policy descriptions. The default reconnect policy is DO_NOT_RECONNECT.
	*
	* @param reconnectPolicy Indicates the reconnection policy to use after a disconnect.
	*
	*/
	action setReconnectPolicy(string reconnectPolicy) {
		_reconnectPolicy := reconnectPolicy;
	}

	/**
	* Set the timout for the reconnection after a conection error. The default value is set by the open action
	* to be twice as long as its timeOut parameter.
	*
	* @param  timeOut The reconnection timeout.
	*
	*/
	action setReconnectTimeout(float timeOut) {
		_reconnectTimeOut := timeOut;
	}

	/**
	* Test if the connection is open (or in the process of opening).
	*/
	action isOpen() returns boolean {
		return (_isBusy or _isOpen);
	}

	/**
	* Quick open operation for JDBC.
	* Uses default batch size, auto commit, etc.
	*
	* @param  URL Database connection string.
	* @param  user User name (if required).
	* @param  password User password (if required).
	* @param  errorHandler Callback action for any failures.
	*
	*/
	action openQuickJDBC( string URL, string user, string password, action < string > handleError ) {
		_isReconnecting := false;
		open( "JDBC", "com.apama.adbc.JDBC", URL, user, password, "true", false, 100, 30.0, handleError );
	}

	/**
	* Quick open operation for ODBC.
	* Uses default batch size, auto commit, etc.
	*
	* @param  URL Database connection string.
	* @param  user User name (if required).
	* @param  password User password (if required).
	* @param  errorHandler Callback action for any failures.
	*
	*/
	action openQuickODBC( string URL, string user, string password, action < string > handleError ) {
		_isReconnecting := false;
		open( "ODBC", "com.apama.adbc.ODBC", URL, user, password, "true", false, 100, 30.0, handleError );
	}

	/**
	* Open that allows for specifying the serviceId, as well
	* as autoCommit, batchSize, and the timeOut for waiting for adapter.
	*
	* @param  type Data source type (ODBC, JDBC, Sim, etc,).
	* @param  serviceId Service id for the adapter (com.apama.adbc.ODBC, com.apama.adbc.JDBC, etc.).
	* @param  URL Database connection string.
	* @param  user User name (if required).
	* @param  password User password (if required).
	* @param  autoCommit Auto-commit mode (case insensitive values):
	* <pre>
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.</pre>
	* @param  readOnly Should connection be readOnly.
	* @param  batchSize The maximum size of a batch.
	* @param  timeOut Time to wait for adapter to become ready.
	* @param  errorHandler Callback action for any failures.
	*
	*/
	action open( string type, string serviceId, string URL, string user,
			string password, string autoCommit, boolean readOnly,
			integer batchSize, float timeOut, action < string > errorHandler )
	{
		// Check to see if database is already open
		if _isOpen then {
			handleError( "Database " + URL + ": error opening: Database " + _conn.getDatabaseURL() + " already open" );
			return;
		}
		else if _isReconnecting then {
			handleError( "Database " + URL + ": error opening: Database " + _conn.getDatabaseURL() + " is already reconnecting" );
			return;
		}

		// Set specified values
		handleError  := errorHandler;
		_batchSize   := batchSize;
		_timeOut     := timeOut;
		_serviceId   := serviceId;
		_serviceId2  := serviceId;
		_type        := type;

		// Initialize default values
		_closing := false;
		_connError := false;
		_isReconnecting := false;

		// Set defaults only if not already set
		if _reconnectPolicy.length() = 0 then {
			_reconnectPolicy   := DBReconnectPolicy.DO_NOT_RECONNECT;
		}
		if _reconnectTimeOut = 0.0 then {
			_reconnectTimeOut := 2.0*_timeOut;
		}
		if _triggerTheReconnectTimeout = 0.0 then {
			_triggerTheReconnectTimeout := 60.0;
		}
		if _adapterInstanceName.length() = 0 then {
			_adapterInstanceName := "INSTANCE_1";
		}

		// Need to handle calls from openQuickJDBC and openQuickODBC
		// where the channel name is specified in old default format for
		// backward compatability but the adapter could be using either
		// the old or new format. If channel specified is the old default
		// value then set the 2nd var to use the new format.
		if serviceId = "com.apama.adbc.JDBC" or serviceId = "com.apama.adbc.ODBC" then {
			_serviceId2  := serviceId + "_" + _adapterInstanceName;
		}

		// Mark ourselves as busy
		_isBusy := true;

		subscribeStatus();

		Status status;
		on (Status( serviceID = serviceId ) : status or Status( serviceID = _serviceId2 ) : status) and not wait ( _timeOut ) {
			_conn.openDatabaseFull( status.serviceID, URL, user, password, autoCommit, readOnly, new dictionary< string, string >, handleDoneOpenDatabase );
		}

		handleStatus();
	}

	/**
	* Open that uses an already open existing matching connection if available
	* or will open a new one.  Allows for specifying the serviceId, as well
	* as autoCommit, batchSize, and the timeOut for waiting for adapter.
	*
	* @param  type Data source type (ODBC, JDBC, Sim, etc,).
	* @param  serviceId Service id for the adapter (com.apama.adbc.ODBC, com.apama.adbc.JDBC, etc.).
	* @param  URL Database connection string.
	* @param  user User name (if required).
	* @param  password User password (if required).
	* @param  autoCommit Auto-commit mode (case insensitive values):
	* <pre>
	*		default - Use setting from config file.<br>
	*		false - Disable auto-commit.<br>
	*		x.x - Value (seconds) for ADBC adapter timed auto-commit.<br>
	*		true - Enable data source specific auto-commit.</pre>
	* @param  readOnly Should connection be readOnly.
	* @param  batchSize The maximum size of a batch.
	* @param  timeOut Time to wait for adapter to become ready.
	* @param  errorHandler Callback action for any failures.
	*
	*/
	action openShared( string type, string serviceId, string URL, string user,
			string password, string autoCommit, boolean readOnly,
			integer batchSize, float timeOut, action < string > errorHandler )
	{
		// Check to see if database is already open
		if _isOpen then {
			handleError( "Database " + URL + ": error opening: Database " + _conn.getDatabaseURL() + " already open" );
			return;
		}
		else if _isReconnecting then {
			handleError( "Database " + URL + ": error opening: Database " + _conn.getDatabaseURL() + " is already reconnecting" );
			return;
		}

		// Set specified values
		handleError  := errorHandler;
		_batchSize   := batchSize;
		_timeOut     := timeOut;
		_serviceId   := serviceId;
		_serviceId2  := serviceId;
		_type        := type;

		// Initialize default values
		_closing := false;
		_connError := false;
		_isReconnecting := false;

		// Set defaults only if not already set
		if _reconnectPolicy.length() = 0 then {
			_reconnectPolicy   := DBReconnectPolicy.DO_NOT_RECONNECT;
		}
		if _reconnectTimeOut = 0.0 then {
			_reconnectTimeOut := 2.0*_timeOut;
		}
		if _triggerTheReconnectTimeout = 0.0 then {
			_triggerTheReconnectTimeout := 60.0;
		}
		if _adapterInstanceName.length() = 0 then {
			_adapterInstanceName := "INSTANCE_1";
		}

		// Need to handle calls from openQuickJDBC and openQuickODBC
		// where the channel name is specified in old default format for
		// backward compatability but the adapter could be using either
		// the old or new format. If channel specified is the old default
		// value then set the 2nd var to use the new format.
		if serviceId = "com.apama.adbc.JDBC" or serviceId = "com.apama.adbc.ODBC" then {
			_serviceId2  := serviceId + "_" + _adapterInstanceName;
		}

		// Mark ourselves as busy
		_isBusy := true;

		subscribeStatus();

		Status status;
		on (Status( serviceID = serviceId ) : status or Status( serviceID = _serviceId2 ) : status) and not wait ( _timeOut ) {
			dictionary< string, string > extraParams := {"uniqueConnection":"false"};
			_conn.openDatabaseFull( status.serviceID, URL, user, password, autoCommit, readOnly, extraParams, handleDoneOpenDatabase );
		}

		handleStatus();
	}

	/** @private */
	action handleStatus() {
		// If timeout before the adapter is up, then record last known error
		on wait ( _timeOut ) and not (Status( serviceID = _serviceId ) or Status( serviceID = _serviceId2 )) {
			handleError( "Database adapter " + _adapterInstanceName + "  not up after " + _timeOut.formatFixed( 1 ) + " second(s)" );
			_statusListener.quit();
		}

		// Once adapter is up, listen for the adapter going down
		StatusError statusError;
		_statusListener := on all (StatusError ( serviceID = _serviceId  ): statusError or StatusError ( serviceID = _serviceId2  ): statusError)
		                      and (Status( serviceID = _serviceId ) or  Status( serviceID = _serviceId2 )) {
			handleConnectionError(statusError.description, false);
		}
	}

	/** @private */
	action handleConnectionStatus() {

		string connectionId := "ADBC_" + _type + "_" + _conn.getDbId().toString();
		string serviceId := _serviceId;
		if not _serviceId = _serviceId2 then {
			serviceId := _serviceId2;
		}

		Status status;
		_connectionStatusListener := on Status ( serviceID = serviceId, connection = connectionId  ): status {
			if (status.description = connectionId + " connection closed" or 
					status.description.find("adapter up but not connected to a data source") != -1) and not _closing then {
				handleConnectionError(status.description, false);
			} else {
				_connError := false;
				handleConnectionStatus();
			}

		}
	}

	/** 
	* Shutdown the connection when there is an error.
	*
	* Marks the connection as closed and quits all listeners, 
	* this also unsubscribes from ADBC status information.
	*/
	action shutdownConnectionOnError() {
		removeRequestsOnError();

		// In the case of the adapter still runnig it has already closed the database connection
		// so there is no need to call _conn.closeDatabase
		// In the case of the adapter down _conn.closeDatabase is meaningless
		// All we need to do is to mark the connection as closed and quit the listeners
		_conn.reset();

		// Unsubscribe from adapter status
		UnsubscribeStatus unsubscribe := new UnsubscribeStatus;
		unsubscribe.serviceID := "ADBC"; // unsubscribe from status for all ADBC adapters

		try {
			monitor.unsubscribe(Status.CHANNEL);
			send unsubscribe to UnsubscribeStatus.CHANNEL;
		} catch(com.apama.exceptions.Exception e) {
			// ignore multiple unsubscriptions - if close is called multiple times, then we don't need to send an Unsubscribe event.
		}


	}

	/** @private */
	action removeRequestsOnError() {
		if _reconnectPolicy = DBReconnectPolicy.RECONNECT_AND_CLEAR_REQUEST_QUEUE  or
			_reconnectPolicy = DBReconnectPolicy.DO_NOT_RECONNECT then {
			// remove all pending requests
			while _queries.size() > 0 {
				removeQuery( false, "error running query, database is closed: " + _queries[ 0 ].queryString );
			}
		} else if _reconnectPolicy = DBReconnectPolicy.RECONNECT then {
			// remove the last request
			removeQuery( false, "error running query, database is closed: " + _queries[ 0 ].queryString );
		} else {
			// do not remove anything from the request queue => retry the last request
		}
	}

	/** @private */
	action waitForStatusAndReconnect() {
		Status status;
		_reopenListener := on (Status( serviceID=_serviceId ) or Status( serviceID=_serviceId2 ):status)
			and not wait ( _timeOut ) {
				log "Trigger the reopen " at INFO;				
				_isBusy := true;
				handleStatus();				
				_conn.reopenWithACK(handleDoneOpenDatabase);
			}
	}
	
	/** @private */
	action waitForNoStatusAndTerminate() {	
		Status status;	
		_reopenTimeoutListener := on wait ( _reconnectTimeOut )
			and not (Status( serviceID=_serviceId, available=true ) or Status( serviceID=_serviceId2, available=true ):status) {
				log "Trigger Timeout and terminate" at INFO;
				shutdownConnectionOnError();
				_isReconnecting := false;
				_isBusy := false;
				handleError( "Database adapter not up after " + _timeOut.formatFixed( 1 ) + " second(s). Terminating connection" );
			}
	}

	/** @private */
	action waitForReconnectTriggerTimeoutAndReconnect() {
		_triggerTheReconnectTimeoutListener := on all wait(_triggerTheReconnectTimeout) {
			if (not _isOpen) then {
				//quit the timeout listener as it could terminate a connection straight after it's been reopeend
				//and restart it again
				_reopenTimeoutListener.quit();
				waitForNoStatusAndTerminate();

				// if no iaf status is received then nothing will trigger the reopen
				log "Trigger the reopen Timeout" at INFO;

				_reopenListener.quit();			
				_isBusy := true;
				_isReconnecting := true;
				_conn.reopenWithACK(handleDoneOpenDatabase);
			}
		}
	}

	/** @private */
	action handleConnectionError(string errorDescription, boolean waitForStatusError) {

		// When we have an error on a connection we will always want to try to reconnect
		// therefore update the relevant variables to reflect this
		_isOpen := false;
		_connError := true;
		_isBusy := false;
		// Should always attempt to remove any requests on error, this will check the reconnection policy to decide what to remove
		removeRequestsOnError();

		// When we have an error on a connection we will always want to try to reconnect
		if not _reconnectPolicy = DBReconnectPolicy.DO_NOT_RECONNECT then {
			
		
			//if the IAF has gone down and is restarted the new Iaf will not have seen the first reopen, therefore 
			// check that an reconnection has not already been signalled, which is set to true when a previous reconnect is triggered
			if (not _isReconnecting) then
			{
				log "DBUtil: Trying to reconnect after a connection error  " + errorDescription at INFO;
				// Kill any listeners from a previous reopen.
				_triggerTheReconnectTimeoutListener.quit();
				_reopenTimeoutListener.quit();
				_reopenListener.quit();

				_isOpen := false;					
					
				// This is only set to true when the actual reopen is called on the connection, ie the 
				// handleConnectionError could be called again before the first reopen is actually triggered
				_isReconnecting := true;


				// Acknowledgment handlers will check _connError and
				// leave the last request on the queue
				_connError := true;

				// "close" the database without the sending the CloseDatabase event
				// (the adapter is down or the connection has already been closed)
				_conn.reset();

				// Check for the adapter coming back up within the timeout duration
				if waitForStatusError then {
					on (StatusError ( serviceID = _serviceId  ) or StatusError ( serviceID = _serviceId2  ))
					or (Status( serviceID = _serviceId, available = false) or  Status( serviceID = _serviceId2 ,available = false)) 
					and not wait ( _timeOut ) {
						waitForStatusAndReconnect();
						waitForNoStatusAndTerminate();
						waitForReconnectTriggerTimeoutAndReconnect();

					}
				} else {
					waitForStatusAndReconnect();
					waitForNoStatusAndTerminate();
					waitForReconnectTriggerTimeoutAndReconnect();
				}
			}
			else{
				log "DBUtil: Already attempting to reconnect after a connection error  " + errorDescription at INFO;
			}
		// If timeout before the adapter is up, then record last known error
		} else { // do not reconnect			
			_isOpen := false;
			_connError := true;
			shutdownConnectionOnError();
		}
		handleError( "Database adapter problem: " + errorDescription );	
	}

	/** 
	* Subscribe to the status information from all ADBC adapters.
	*/
	action subscribeStatus() {
		// Subscribe for adapter status
		SubscribeStatus subscribe := new SubscribeStatus;
		subscribe.serviceID := "ADBC"; // subscribe for status from all ADBC adapters
		monitor.subscribe(Status.CHANNEL);
		send subscribe to SubscribeStatus.CHANNEL;
	}

	/** 
	* Subscribe to the status on the connection.
	*
	* @deprecated [This action has been deprecated due to it being unnecessary.
	* Please use subscribeStatus instead.]
	*/
	action subscribeConnectionStatus(string serviceId, string connectionId) {

		// Subscribe for status on the connection
		SubscribeStatus subscribe := new SubscribeStatus;
		subscribe.serviceID := serviceId;

		monitor.subscribe(Status.CHANNEL);
		send subscribe to SubscribeStatus.CHANNEL;
	}

	/**
	* OnConcludeRecovery is called after the recovery process has been completed, 
	* including all non persistent monitors having being re-loaded.
	*/
	action onConcludeRecovery() {
		_queryListener.quit();
		_statusListener.quit();
		_connectionStatusListener.quit();
		while _queries.size() > 0 {
			removeQuery(false, "The correlator was recovered");
		}
		_isBusy := true;

		subscribeStatus();

		Status status;
		on (Status( serviceID = _serviceId ) : status or Status( serviceID = _serviceId2 ) : status) and not wait ( _timeOut ) {
			_isBusy := false;
			checkNextOperation();
		}

		handleStatus();
	}

	/**
	* Execute a SQL command that does not return results.
	*
	* @param  cmdString Command to execute.
	*
	*/
	action doSQLCmd( string cmdString ) {
		doSQLCmdAck( cmdString, -1, false );
	}

	/**
	* Execute an SQL command that does not return results unless 
	* the previous operation (SQLCmd, SQLQuery, or SQLCommit) failed.
	*
	* @param  cmdString Command to execute.
	*
	*/
	action doSQLCmdOnError( string cmdString ) {
		doSQLCmdAck( cmdString, -1, true );
	}

	/**
	* Execute an SQL command that does not return results and receive 
	* a DBAcknowledge event to indicate success or failure.
	*
	* Setting onError to true will cause this command to only be run 
	* if the previous operation (SQLCmd, SQLQuery, or SQLCommit) failed.
	* Setting ackNum to -1 will disable sending the DBAcknowledge
	* event and instead use the default error handler if an error occurs.
	*
	* @param  cmdString Command to execute.
	* @param  ackNum The id of DBAcknowledge event.
	* @param  onError Is command to be run only after an error?.
	*
	*/
	action doSQLCmdAck( string cmdString, integer ackNum, boolean onError )
	{
		// Ensure that there is no pending close operation
		if _wantsClose then {
			handleError( "Database " + _conn.getDatabaseURL() + ": SQLCmd after close requested: " + cmdString );
			return;
		}

		// Append this query to the work list
		_queries.append( DBOperation( true, false, false, false, onError, cmdString, "", handleResultsDefault, handleDoneDefault, ackNum ) );

		// Execute any pending operations (if possible)
		checkNextOperation();
	}

	/**
	* Commit changes to database.
	*
	*/
	action doSQLCommit() {
		doSQLCommitAck( -1 );
	}

	/**
	* Commit changes to database and receive a DBAcknowledge event 
	* to indicate success or failure.
	*
	* Setting ackNum to -1 will disable sending the DBAcknowledge
	* event and instead use the default error handler if an error occurs.
	*
	* @param  ackNum The id for the DBAcknowledge event.
	*
	*/
	action doSQLCommitAck( integer ackNum )
	{
		// Ensure that there is no pending close operation
		if _wantsClose then {
			handleError( "Database " + _conn.getDatabaseURL() + ": SQLCommit after close requested" );
			return;
		}

		// Append this query to the work list
		_queries.append( DBOperation( false, true, false, false, false, "", "", handleResultsDefault, handleDoneDefault, ackNum ) );

		// Execute any pending operations (if possible)
		checkNextOperation();
	}

	/**
	* Rollback uncommitted changes to database.
	*/
	action doSQLRollback() {
		doSQLRollbackAck( -1, false );
	}

	/**
	* Rollback uncommitted changes to database but only if the 
	* previous operation (SQLCmd, SQLQuery, or SQLCommit) failed.
	*/
	action doSQLRollbackOnError() {
		doSQLRollbackAck( -1, true );
	}

	/**
	* Rollback uncommitted changes to database and receive a DBAcknowledge 
	* event to indicate success or failure.
	*
	* Setting onError to true will cause this to run only if
	* the previous operation (SQLCmd, SQLQuery, or SQLCommit) failed.
	* Setting ackNum to -1 will disable sending the DBAcknowledge
	* event and instead use the default error handler if an error occurs.
	*
	* @param  ackNum The id for the DBAcknowledge event.
	* @param  onError Indicates if it should run only when there is an error.
	*
	*/
	action doSQLRollbackAck( integer ackNum, boolean onError )
	{
		// Ensure that there is no pending close operation
		if _wantsClose then {
			handleError( "Database " + _conn.getDatabaseURL() + ": SQLRollback after close requested" );
			return;
		}

		// Append this query to the work list
		_queries.append( DBOperation( false, false, true, false, onError, "", "", handleResultsDefault, handleDoneDefault, ackNum ) );

		// Execute any pending operations (if possible)
		checkNextOperation();
	}

	/**
	* Execute a SQL query that returns results.
	*
	* @param  queryString Query to execute.
	* @param  handleResult Callback to be called for each ResultEvent (row).
	*
	*/
	action doSQLQuery( string queryString, action< dictionary< string, string > > handleResult ) {
		doSQLQueryAck( queryString, handleResult, -1, false );
	}

	/**
	* Execute a SQL query that returns results and runs the specified 
	* action when the query completes.
	* The handleDone action parameters are :
	*  string error - Will be empty on success else the error message
	*  integer count - Number of rows returned by the query.
	*
	* @param  queryString Query to execute.
	* @param  handleResult Callback to be called for each ResultEvent (row).
	* @param  handleDone Callback to be called for each ResultEvent (row).
	*
	*/
	action doSQLQueryWithCallback( string queryString, action< dictionary< string, string > > handleResult, action<string, integer> handleDone ) {

		// Ensure there is no pending close operation
		if _wantsClose then {
			handleError( "Database " + _conn.getDatabaseURL() + ": SQLQuery after close requested: " + queryString );
			return;
		}
		// Append this query to the work list
		_queries.append( DBOperation( false, false, false, false, false, queryString, "", handleResult, handleDone, -1 ) );

		// Execute any pending operations (if possible)
		checkNextOperation();
	}

	/**
	* Execute an SQL query that returns results but only if the previous 
	* operation (SQLCmd, SQLQuery, or SQLCommit) failed.
	*
	* @param  queryString Query to execute.
	* @param  handleResult Callback to be called for each ResultEvent (row).
	*
	*/
	action doSQLQueryOnError( string queryString, action< dictionary< string, string > > handleResult ) {
		doSQLQueryAck( queryString, handleResult, -1, true );
	}

	/**
	* Execute a SQL query that returns results and receive a 
	* DBAcknowledge event to indicate success or failure.  
	*
	* Setting onError to true will cause this command to only 
	* be run if the previous operation (SQLCmd, SQLQuery, or SQLCommit) failed.
	* Setting ackNum to -1 will disable sending the DBAcknowledge
	* event and instead use the default error handler if an error occurs.
	*
	* @param  queryString Query to execute.
	* @param  ackNum The id for the DBAcknowledge event.
	* @param  onError Indicates if it should run only when there is an error.
	* @param  handleResult Callback to be called for each ResultEvent (row).
	*
	*/
	action doSQLQueryAck( string queryString, action < dictionary< string, string > > handleResult, integer ackNum, boolean onError )
	{
		// Ensure there is no pending close operation
		if _wantsClose then {
			handleError( "Database " + _conn.getDatabaseURL() + ": SQLQuery after close requested: " + queryString );
			return;
		}
		// Append this query to the work list
		_queries.append( DBOperation( false, false, false, false, onError, queryString, "", handleResult, handleDoneDefault, ackNum ) );

		// Execute any pending operations (if possible)
		checkNextOperation();
	}

	/**
	* Execute an SQL query that returns events.
	*
	* @param  queryString Query to execute.
	* @param  eventType Event type for mapping rule (_ADBCType).
	*
	*/
	action doSQLEventQuery( string queryString, string eventType ) {
		doSQLEventQueryAck( queryString, eventType, -1, false );
	}

	/**
	* Execute a SQL query that returns events and runs the specified 
	* action when the query completes.
	* The handleDone action parameters are :
	*  string error - Will be empty on success else the error message
	*  integer count - Number of rows returned by the query.
	*
	* @param  queryString Query to execute.
	* @param  handleDone Callback to be called for each ResultEvent (row).
	*
	*/
	action doSQLEventQueryWithCallback( string queryString, string eventType, action<string, integer> handleDone ) {

		// Ensure there is no pending close operation
		if _wantsClose then {
			handleError( "Database " + _conn.getDatabaseURL() + ": SQLEventQuery after close requested: " + queryString );
			return;
		}
		// Append this query to the work list
		_queries.append( DBOperation( false, false, false, true, false, queryString, eventType, handleResultsDefault, handleDone, -1 ) );

		// Execute any pending operations (if possible)
		checkNextOperation();
	}

	/**
	* Execute a SQL query that returns events but only if the previous 
	* operation (SQLCmd, SQLQuery, SQLEventQuery, or SQLCommit) failed.
	*
	* @param  queryString Query to execute.
	* @param  eventType Event type for mapping rule (_ADBCType).
	*
	*/
	action doSQLEventQueryOnError( string queryString, string eventType ) {
		doSQLEventQueryAck( queryString, eventType, -1, true );
	}

	/**
	 * Execute a SQL query that returns events and receive a 
	 * DBAcknowledge event to indicate success or failure.  
	 *
	 * Setting onError to true will cause this command to only be run if
	 * the previous operation (SQLCmd, SQLQuery, SQLEventQuery, or SQLCommit) failed.
	 * Setting ackNum to -1 will disable sending the DBAcknowledge
	 * event and instead use the default error handler if an error occurs.
	 *
	 * @param  queryString Query to execute.
	 * @param  eventType Event type for mapping rule (_ADBCType).
	 * @param  ackNum The id for the DBAcknowledge event.
	 * @param  onError Indicates if it should run only when there is an error.
	 *
	 */
	action doSQLEventQueryAck( string queryString, string eventType, integer ackNum, boolean onError )
	{
		// Ensure there is no pending close operation
		if _wantsClose then {
			handleError( "Database " + _conn.getDatabaseURL() + ": SQLEventQuery after close requested: " + queryString );
			return;
		}
		// Append this query to the work list
		_queries.append( DBOperation( false, false, false, true, onError, queryString, eventType, handleResultsDefault, handleDoneDefault, ackNum ) );

		// Execute any pending operations (if possible)
		checkNextOperation();
	}

	/** Get the current schemas field order associated with the current result. */
	action getSchemaFieldOrder()  returns sequence< string >           { return _schema_fieldOrder; }
	/** Get the current schemas field types associated with the current result. */
	action getSchemaFieldTypes()  returns dictionary< string, string > { return _schema_fieldTypes; }
	/** Get the current schemas index field associated with the current result. */
	action getSchemaIndexFields() returns sequence< string >           { return _schema_indexFields; }

	/**
	* Stop all operations.
	* This also cancels all outstanding queries in the queue.
	*
	*/
	action stopAll()
	{
		// We are in the midst of stopping
		_isStopping := true;

		// remove queued queries
		while ( _queries.size() > 0 ) {
			removeQuery( false, "user stopped, canceling query " + _queries[0].queryString );
		}

		_conn.stopAllQueries( handleDoneStopAllQueries );
	}

	/**
	* Close the database. Setting doStopAll to
	* true will cancel the current and all outstanding
	* operations.
	*
	* @param  doStopAll Should all pending operations be canceled.
	*
	*/
	action close( boolean doStopAll )
	{
		// if there was connection error don't do anything
		if _connError then {
			log "DBUtil: Close not called because there was a connection error" at INFO;
			return;
		}

		_closing := true;

		// If we also want to stopAll, then call it -- this clears out the queue as well!
		if doStopAll then {
			stopAll();
		}

		// Mark a close being desired
		_wantsClose := true;

		// Execute any pending requests
		checkNextOperation();
	}

	/**
	* Callback action to handle a ResultEvent from a query.
	* Called for each ResultEvent (row) returned.
	*
	* This is a no-op handler.
	*
	*/
	action handleResultsDefault( dictionary< string, string > r ) {
	}

	/**
	* Callback action to handle the completion of a query.
	* Called after entire ResultSet is returned.
	*
	* This is a no-op handler.
	*
	*/
	action handleDoneDefault(string error, integer count) {
	}

	/**
	* Check next pending operation against the database.
	*
	* Internal (private) action.
	* @private
	*/
	action checkNextOperation() {

		// If we are busy, stopping, or there is a connection error then there's nothing to be done yet
		if _isBusy or _isStopping or _connError then {
			return;
		}

		// Execute the next query if there are any
		if _queries.size() > 0 then {
			runNextQuery();
			return;
		}

		// If it wants to be closed, then we close it
		if _wantsClose then {
			// Stop the adapter status listeners
			// if this is not a status detected error
			if not _connError then {
				_statusListener.quit();
				_connectionStatusListener.quit();
			}

			_conn.closeDatabase( handleDoneCloseDatabase );

			// Unsubscribe from adapter status
			UnsubscribeStatus unsubscribe := new UnsubscribeStatus;
			unsubscribe.serviceID := "ADBC"; // unsubscribe from status for all ADBC adapters
			try {
				monitor.unsubscribe(Status.CHANNEL);
				send unsubscribe to UnsubscribeStatus.CHANNEL;	
			} catch(com.apama.exceptions.Exception e) {
				// ignore multiple unsubscriptions - if close is called multiple times, then we don't need to send an Unsubscribe event.
			}

			return;
		}
	}

	/**
	* Execute an operation against the database.
	*
	* Internal (private) action.
	* @private
	*/
	action runNextQuery()
	{
		// Mark the database as now being busy
		_isBusy := true;

		// Query to run is the top of the queue
		DBOperation dbOperation := _queries[ 0 ];

		// If it is a command, then send queryString directly
		if dbOperation.isCommand then {

			if _logQueries then {
				log "DBUtil: DB " + _conn.getDatabaseURL() + ": Executing SQL Command: " + dbOperation.queryString at INFO;
			}

			_conn.runCommandFull( dbOperation.queryString, "", new dictionary< string, string >, handleDoneSQLCommand );
			return;
		}

		// If it is a commit then do commit
		else if dbOperation.isCommit then {
			if _logQueries then {
				log "DBUtil: DB " + _conn.getDatabaseURL() + ": Executing Commit" at INFO;
			}
			integer i := _conn.commitRequest( handleCommitDone );
		}

		// If it is a rollback then do a rollback
		else if dbOperation.isRollback then {
			if _logQueries then {
				log "DBUtil: DB " + _conn.getDatabaseURL() + ": Executing Rollback" at INFO;
			}
			integer i := _conn.rollbackRequest( handleRollbackDone );
		}

		// If it is an event query then run a query returning events
		else if dbOperation.isEventQuery then {
			if _logQueries then {
				log "DBUtil: DB " + _conn.getDatabaseURL() + ": Executing SQL Event Query: " + dbOperation.queryString at INFO;
			}

			// Initialize query event appropriately
			Query query := new Query;
			if _parallel then {
				query.initPreSpawnContext( _preSpawnContext );
			}
			query.initQuery(_conn, dbOperation.queryString );
			query.setBatchSize( _batchSize );
			query.setBatchDoneCallback("", handleBatchDone);
			query.setSchemaCallback(handleSchema);
			query.setReturnType( Query.NATIVE );
			query.setEventType(dbOperation.eventType);
			query.start( handleDoneQuery );
		}

		// Otherwise then send a query with results
		else {
			if _logQueries then {
				log "DBUtil: DB " + _conn.getDatabaseURL() + ": Executing SQL Query: " + dbOperation.queryString at INFO;
			}

			// Initialize query event appropriately
			Query query := new Query;
			if _parallel then {
				query.initPreSpawnContext( _preSpawnContext );
			}
			query.initQuery(_conn, dbOperation.queryString );
			query.setBatchSize( _batchSize );
			query.setBatchDoneCallback("", handleBatchDone);
			query.setSchemaCallback(handleSchema);
			query.start( handleDoneQuery );

			// Capture result events until we get the done message  (maybe have a way to communicate to the handleResult that we are done?)
			ResultEvent re;
			integer id := query.getId();
			_queryListener := on all ResultEvent( messageId = id ):re {
				dbOperation.handleResult( re.row );
			}
		}
	}

	/**
	* Remove an operation from queue and execute
	* onError operation as needed.
	*
	* Internal (private) action.
	* @private
	*/
	action removeQuery( boolean success, string error ) {

		// Extract query ack number
		integer ackNum := -1;

		// Remove query
		if _queries.size() > 0 then {
			ackNum := _queries[ 0 ].ackNum;
			_queries.remove( 0 );
		}

		// If successful, remove any on error trailing
		boolean onErrorFollowing := false;
		if success then {
			if _queries.size() > 0 and _queries[ 0 ].onError then {
				if _logQueries then {
					log "DBUtil: DB operation succeeded, removing the onError operation - " + _queries[ 0 ].queryString at INFO;
				}
				_queries.remove( 0 );
				onErrorFollowing := true;
			}
		}

		// If an acknowledgement is requested, use that instead of generic error handler
		if ackNum >= 0 then {
			route DBAcknowledge( ackNum, success, error );
		}

		// Else if still error, then we use generic handler
		else if not success then {
			handleError( "Database " + _conn.getDatabaseURL() + ": " + error );
		}
	}

	/**
	* Finished committing a database.
	*
	* Internal (private) action.
	* @private
	*/
	action handleCommitDone( Connection conn, integer i, string token, string error ) {

		// If there is an error, report it
		if error.length() != 0 then {
			if not _connError then {
				removeQuery( false, "error committing: " + error );
			} else { // stop processing queries if _connError
				_isBusy := false;
				return;
			}
		}
		else {
			removeQuery( true, "" );
		}

		// No longer busy - other operations can now execute
		_isBusy := false;

		// Execute next operation
		checkNextOperation();
	}

	/**
	* Finished rolling back a database.
	*
	* Internal (private) action.
	* @private
	*/
	action handleRollbackDone( Connection conn, integer i, string token, string error ) {

		// If there is an error, report it
		if error.length() != 0 then {
			if not _connError then {
				removeQuery( false, "error in rollback: " + error );
			} else { // stop processing queries if _connError
				_isBusy := false;
				return;
			}
		}
		else {
			removeQuery( true, "" );
		}

		// No longer busy - other operations can now execute
		_isBusy := false;

		// Execute next operation
		checkNextOperation();
	}

	/**
	* Finished opening a database.
	*
	* Internal (private) action.
	* @private
	*/
	action handleDoneOpenDatabase( Connection conn, string error )
	{
		_isReconnecting := false;
		// No longer busy - other operations can now execute
		_isBusy := false;

		// No longer reconnecting (if applicable)
		_isReconnecting := false;

		// Check error opening
		if error.length() != 0 then {
			handleConnectionError("Database " + conn.getDatabaseURL() + ": opening error: " + error, true);
		}

		// If no error, then we are open!
		else {

			// Monitor connection status
			handleConnectionStatus();

			_isOpen := true;
			_connError := false;

			// Reset the _invalidId
			_invalidId := 0;
		}

		// Execute any pending operations
		checkNextOperation();
	}

	/**
	* Capture the current schema details for the upcoming results.
	*
	* Internal (private) action.
	* @private
	*/
	action handleSchema( sequence<string> fieldOrder, dictionary<string,string> fieldTypes, sequence<string> indexFields )
	{
		_schema_fieldOrder  := fieldOrder;
		_schema_fieldTypes  := fieldTypes;
		_schema_indexFields := indexFields;
	}

	/**
	* Handle batch finished.
	*
	* Internal (private) action.
	* @private
	*/
	action handleBatchDone(Query query, string reason, integer eventCount, float lastEventTime, string lastEvent, string token)
	{
		// Get the next batch.
		query.getNextBatch( _batchSize );
	}

	/**
	* Finished the SQL query.
	*
	* Internal (private) action.
	* @private
	*/
	action handleDoneQuery( Query query, string error, integer eventCount, float lastEventTime )
	{
		// If there is an error, report it
		if error.length() != 0 then {

			checkAndHandleInvalidId(error);

			// remove the query only if there was no connection error or the query was stopped using stopAll action
			if not _connError and not (error.find("Query incomplete, adapter is stopping") = 0 and not _isStopping) then {
				// Call user specified query done callback
				// If not specified the default is a no-op action)
				string qstring := "query";
				if _queries.size() > 0 then {
					qstring := _queries[ 0 ].queryString;
					_queries[0].handleDone(error, eventCount);
				}
				removeQuery( false, "error executing " + qstring + ": " + error );
			} else if _connError then { // stop processing queries if _connError
				_isBusy := false;
				return;
			} else if (error.find("Query incomplete, adapter is stopping") = 0 and not _isStopping) then {
				_isBusy := false;
				handleConnectionError("The adapter is stopping.", true);
			}
		}
		else {
			// Call user specified query done callback
			// If not specified the default is a no-op action)
			if _queries.size() > 0 then {
				_queries[0].handleDone(error, eventCount);
			}
			removeQuery( true, "" );
		}

		// No longer busy with this operation
		_isBusy := false;

		// Remove the queryListener
		_queryListener.quit();

		// Execute next operation
		checkNextOperation();
	}

	/**
	* Internal (private) action.
	* @private
	*/
	action invalidIdHandleDoneCloseDatabase( Connection conn, string error )
	{
		// Report error and/or mark database as closed as appropriate
		if error.length() != 0 then {
			handleError( "Database " + conn.getDatabaseURL() + ": Close error: " + error );
		}
		else {
			_isOpen := false;
		}

		removeRequestsOnError();

		if not _reconnectPolicy = DBReconnectPolicy.DO_NOT_RECONNECT then {
			_isBusy := true;
			handleStatus();
			_conn.reopenWithACK(handleDoneOpenDatabase);
		}
	}

	/**
	* Internal (private) action.
	* @private
	*/
	action checkAndHandleInvalidId( string error )
	{
		// Perform a reconnect if error is due to an invalid connection id.
		// This can occur if the adapter goes down and comes back up before
		// the IAF status system can detect it went down (less than ~20 seconds).
		if error.find("Invalid database Id") = 0 then {
			// Ignore if a reconnect has already been done
			if error.find("Invalid database Id: " + _invalidId.toString()) = 0 then {
				log "DBUtil: Already reconnected this invalid db connection" at INFO;
			}
			else {
				if _isOpen and not _connError and not _wantsClose then {
					_isOpen := false;
					_connError := true;
					// Record _invalid Id
					_invalidId := _conn._dbId;

					_conn.reset();
					log "DBUtil: Disconnecting invalid db connection" at INFO;
					_conn.closeDatabase(invalidIdHandleDoneCloseDatabase);
				}
			}
		}
	}

	/**
	* Finished the SQL command.
	*
	* Internal (private) action.
	* @private
	*/
	action handleDoneSQLCommand( Connection conn, string token, string error )
	{
		// Error - report error
		if error.length() > 0 then {

			checkAndHandleInvalidId(error);

			if not _connError then {
				removeQuery( false, "error executing " + _queries[ 0 ].queryString + ": " + error );
			} else { // stop processing queries if _connError
				_isBusy := false;
				return;
			}
		}
		else {
			removeQuery( true, "" );
		}

		// No longer busy
		_isBusy := false;

		// Execute any pending operations
		checkNextOperation();
	}

	/**
	* Finished closing the database.
	*
	* Internal (private) action.
	* @private
	*/
	action handleDoneCloseDatabase( Connection conn, string error )
	{
		// No longer want to close the database
		_wantsClose := false;
		_closing := false;

		// Report error and/or mark database as closed as appropriate
		if error.length() != 0 then {
			handleError( "Database " + conn.getDatabaseURL() + ": Close error: " + error );
		}
		else {
			_isOpen := false;
		}
	}

	/**
	* Finished stopping all queries.
	*
	* Internal (private) action.
	* @private
	*/
	action handleDoneStopAllQueries( Connection conn, string error )
	{
		// No longer stopping all
		_isStopping := false;

		// If there is an error, report it - nothing much else we can do
		if error.length() != 0 then {
			handleError( "Database " + _conn.getDatabaseURL() + ": " + error );
		}

		// Check to see if there is anything else
		checkNextOperation();
	}

}
 00000034 C:\SoftwareAG\Apama\adapters\monitors\ADBCHelper.mon
DISC 0000003c 6330853572159242574:6330572097182531918 from 127.0.0.1:53178
EVNT 000000b2 6330853567618646350 "",com.apama.database.DataSources(0,"",[com.apama.database.DataSource("com.apama.adbc.JDBC","JDBC",{"codecName":"NullCodec","transportName":"ADBC-JDBC"})],{})
TIME 0000000e 1474016712.4,1
EVNT 0000010e 6330853567618646350 "",com.apama.adapters.IAFStatusResponse_1(1,1,44850,"C:\\Users\\moray\\Development\\cycle-monitor\\test\\testcases\\CycleMonitor_cor_009\\Output\\win32\\adbc.xml","NullCodec","ADBC-JDBC",true,true,{"VERSION":"9.10.0.3"},{"VERSION":"9.10.0.3.284318"})
CONN 0000003c 6330853572028432718:6330572097051722062 from 127.0.0.1:53181
TIME 0000000e 1474016712.5,1
MONF 00000439 /**
 * $Copyright (c) 2015 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.$
 * Use, reproduction, transfer, publication or disclosure is prohibited except as specifically provided for in your License Agreement with Software AG
 */

package com.softwareag.connectivity;

/**
 * Utilities for interacting with Connectivity Plug-ins.
 */
event ConnectivityPlugins {
	/** @private */
	import "ConnectivityPlugin" as _plugin;

	/** Should be called by EPL applications after all EPL has been injected
	 * and initialized, to indicate that the application is ready to receive
	 * events from connectivity plug-ins.
	 *
	 * This will also enable reception of JMS events if correlator-integrated
	 * JMS is enabled (i.e. it implicitly calls
	 * JMSPlugin.onApplicationInitialized)
	 *
	 * Invoking this action more than once is an error and will throw an
	 * exception.
	 *
	 */
	action onApplicationInitialized() {
		_plugin.onApplicationInitialized();
	}
}
 00000034 C:\SoftwareAG\Apama\monitors\ConnectivityPlugins.mon
DISC 0000003c 6330853572028432718:6330572097051722062 from 127.0.0.1:53181
CONN 0000003c 6330853571870097742:6330572096893387086 from 127.0.0.1:53184
TIME 0000000e 1474016712.6,1
MONF 000009ce //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.util;

//the interface
event IKeyedDataView {
	action<> returns string getDVName;
	action<> returns string getDVDisplayName;
	action<> returns string getDVDescription;
	action<> returns sequence<string> getDVFieldNames;    
	action<> returns sequence<string> getDVFieldTypes; 
	action<> returns sequence<string> getDVFieldValues; 
	action<> returns sequence<string> getDVKeyFields;  
	action<> returns sequence<string> getDVKeyFieldValues;  
}

//the helper for creation of the definition and addition of entries
event KeyedDataViewHelper {
	action register(IKeyedDataView dv) {
		com.apama.dataview.DataViewAddDefinition add := new com.apama.dataview.DataViewAddDefinition;
		add.msgId := integer.getUnique().toString();
		add.dvName := dv.getDVName();
		add.dvDisplayName := dv.getDVDisplayName();
		add.dvDescription := dv.getDVDescription();
		add.fieldNames := dv.getDVFieldNames();
		add.fieldTypes := dv.getDVFieldTypes();
		add.keyFields := dv.getDVKeyFields();
		route add;		
	}

	action add(IKeyedDataView entry) {
		com.apama.dataview.DataViewAddOrUpdateItem item := new com.apama.dataview.DataViewAddOrUpdateItem;
		item.msgId := integer.getUnique().toString();
		item.dvName := entry.getDVName();
		item.owner := "*";
		item.timeStamp := currentTime;
		item.fieldValues := entry.getDVFieldValues();
		route item;		
	}

	action remove(IKeyedDataView entry) {
		com.apama.dataview.DataViewDeleteItem delete := new com.apama.dataview.DataViewDeleteItem;
		delete.msgId := integer.getUnique().toString();
		delete.dvName := entry.getDVName();
		delete.dvItemId := -1;
		delete.keyFields := entry.getDVKeyFieldValues();
		route delete;
	}
}
 0000004d C:\Users\moray\Development\cycle-monitor\monitors\utils\DataViewKeyFields.mon
DISC 0000003c 6330853571870097742:6330572096893387086 from 127.0.0.1:53184
CONN 0000003c 6330853572005626190:6330572097028915534 from 127.0.0.1:53187
TIME 0000000e 1474016712.7,1
MONF 0000093a //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.config;

event DBConfig {
	string serviceId;
	string databaseURL;
	string user;
	string password;
}

event DBLoaded {}

event DBKey { 
	string city; 
	integer id; 
}

event CreateUpperBoundaryAlert {
	integer id;
	string city;
	float threshold;
}

event CreateLowerBoundaryAlert {
	integer id;
	string city;
	float threshold;
}

event CreateRateThresholdAlert {
	integer id;
	string city;
	float threshold;
	float duration;
}

event DBBoundaryAlert {
	string city;
	integer id;
	float threshold;
	boolean upper;

	action fromResult(dictionary<string, string> data) returns DBBoundaryAlert {
		DBBoundaryAlert alert := new DBBoundaryAlert; 
		alert.city := data.getOr("city","");
		alert.id := data.getOr("id", "-1").toInteger();
		alert.threshold := data.getOr("threshold", "-1.0").toFloat();
		if (data.getOr("upper", "true")="1") then { alert.upper:= true; }
		return alert;
	}

	action pprint() returns string {
		return " | ".join(self.getFieldValues());
	}
}

event DBRateAlert {
	string city;
	integer id;
	float percent;
	float duration;

	action fromResult(dictionary<string, string> data) returns DBRateAlert {
		DBRateAlert alert := new DBRateAlert; 
		alert.city := data.getOr("city","");
		alert.id := data.getOr("id", "-1").toInteger();
		alert.percent := data.getOr("percent", "1.0").toFloat();
		alert.duration := data.getOr("duration", "60.0").toFloat();
		return alert;
	}

	action pprint() returns string {
		return " | ".join(self.getFieldValues());
	}
}
 00000050 C:\Users\moray\Development\cycle-monitor\monitors\configuration\DBDefinition.mon
DISC 0000003c 6330853572005626190:6330572097028915534 from 127.0.0.1:53187
CONN 0000003c 6330853572102619470:6330572097125908814 from 127.0.0.1:53190
TIME 0000000e 1474016712.9,1
MONF 000003a8 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.config;
using com.apama.database.DBUtil;

event ILoader {
	action < > doQuery;
	action < > returns boolean isDone;
} 00000053 C:\Users\moray\Development\cycle-monitor\monitors\configuration\loaders\ILoader.mon
MONF 0000053f //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.config;
using com.apama.database.DBUtil;

event BaseLoader {
	DBUtil db;
	action<> onDone;
	boolean _done;

	action init(DBUtil db, action<> onDone) returns ILoader {
		self.db := db;
		self.onDone := onDone;

		ILoader this := new ILoader;
		this.doQuery := doQuery;
		this.isDone := isDone;
		return this;
	}

	action doQuery() {}

	action isDone() returns boolean { return self._done; }
	action onQueryDone() { self._done:=true; onDone(); }	
	action onError(string message) { log message at ERROR; }
}
 00000056 C:\Users\moray\Development\cycle-monitor\monitors\configuration\loaders\BaseLoader.mon
MONF 0000099c //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.config;
using com.apama.database.DBUtil;

event BoundaryLoader {	
	ILoader this;
	BaseLoader base;
	dictionary< DBKey, DBBoundaryAlert > upperEntries;
	dictionary< DBKey, DBBoundaryAlert > lowerEntries;

	action init(DBUtil db, action<> onDone) returns ILoader {
		ILoader this := base.init(db, onDone);
		this.doQuery := doQuery;
		return this;
	}

	action doQuery() {
		log "Performing query for boundary alert details; " at INFO;
		base.db.doSQLQueryWithCallback("select * from boundary_alert_config", queryResult, queryDone);
	}

	action getUpperCache() returns dictionary< DBKey, DBBoundaryAlert > {
		return upperEntries;
	}

	action getLowerCache() returns dictionary< DBKey, DBBoundaryAlert > {
		return lowerEntries;
	}

	action queryResult(dictionary<string, string> data) {
		string city := data.getOrDefault("city");  
		string id := data.getOrDefault("id");
		string upper := data.getOrDefault("upper");
		if (id!="" and city!="" and upper="1") then {        
			upperEntries.add(DBKey(city, id.toInteger()),(new DBBoundaryAlert).fromResult(data));
		}
		else if (id!="" and city!="" and upper="0") then {        
			lowerEntries.add(DBKey(city, id.toInteger()),(new DBBoundaryAlert).fromResult(data));
		}
	}

	action queryDone(string error, integer eventCount) {
		log "Query complete, boundary alerts are; " at INFO;
		DBKey key; 
		for key in upperEntries.keys() {
			log "  Upper Boundary alert: " + upperEntries[key].pprint() at INFO;
		}
		for key in lowerEntries.keys() {
			log "  Lower Boundary alert: " + lowerEntries[key].pprint() at INFO;
		}
		base.onQueryDone();
	}
} 0000005a C:\Users\moray\Development\cycle-monitor\monitors\configuration\loaders\BoundaryLoader.mon
MONF 00000776 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.config;
using com.apama.database.DBUtil;

event RateLoader {	
	ILoader this;
	BaseLoader base;
	dictionary< DBKey, DBRateAlert > alerts;

	action init(DBUtil db, action<> onDone) returns ILoader {
		ILoader this := base.init(db, onDone);
		this.doQuery := doQuery;
		return this;
	}

	action doQuery() {
		log "Performing query for rate alert details; " at INFO;
		base.db.doSQLQueryWithCallback("select * from rate_alert_config", queryResult, queryDone);
	}

	action getCache() returns dictionary< DBKey, DBRateAlert > {
		return alerts;
	}

	action queryResult(dictionary<string, string> data) {
		string city := data.getOrDefault("city");  
		string id := data.getOrDefault("id"); 
		if (id!="" and city!="") then {                     
			alerts.add(DBKey(city, id.toInteger()),(new DBRateAlert).fromResult(data));
		}
	}

	action queryDone(string error, integer eventCount) {
		log "Query complete, rate alerts are; " at INFO;
		DBKey key; for key in alerts.keys() {
			log "  Rate alert: " + alerts[key].pprint() at INFO;
		}
		base.onQueryDone();
	}
} 00000056 C:\Users\moray\Development\cycle-monitor\monitors\configuration\loaders\RateLoader.mon
DISC 0000003c 6330853572102619470:6330572097125908814 from 127.0.0.1:53190
CONN 0000003c 6330853571953721678:6330572096977011022 from 127.0.0.1:53193
TIME 0000000c 1474016713,1
MONF 000007d3 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.config;

event DBLoader {
	action<> onLoaded;

	RateLoader rateLoader;
	BoundaryLoader boundaryLoader;
	sequence<ILoader> loaders;

	action init(DBConfig config, action<> onLoaded) {
		self.onLoaded := onLoaded;

		//open database
		com.apama.database.DBUtil db := new com.apama.database.DBUtil;
		db.open("JDBC", config.serviceId, config.databaseURL, config.user, config.password, "false", false, 100, 10.0, onError);        

		//load data 
		loaders.append( rateLoader.init(db, onDone) );
		loaders.append( boundaryLoader.init(db, onDone) );
		ILoader l; for l in loaders { l.doQuery();}

		//close database
		db.close(false);
	}

	action upperBoundaryConfig() returns dictionary< DBKey, DBBoundaryAlert > { return boundaryLoader.getUpperCache(); }
	action lowerBoundaryConfig() returns dictionary< DBKey, DBBoundaryAlert > { return boundaryLoader.getLowerCache(); }
	action rateThreholdConfig() returns dictionary< DBKey, DBRateAlert > { return rateLoader.getCache(); }

	action onError(string message) {
		log message at ERROR;
	}

	action onDone() {
		ILoader l; for l in loaders { 
			if not l.isDone() then { return;}
		}		
		onLoaded();
	}
} 0000004c C:\Users\moray\Development\cycle-monitor\monitors\configuration\DBLoader.mon
MONF 00000802 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.config;

monitor DBManager {
	DBConfig config;
	DBLoader cacheLoader;

	action onload {
		on DBConfig():config load(onLoaded);
	}

	action load(action<> onDone) {
		log "Starting loading for connection with parameters; ";
		log "  databaseURL: " + config.databaseURL at INFO;
		log "  user: " + config.user at INFO;
		log "  password: " + config.password at INFO;
		cacheLoader.init(config, onDone);
	}

	action onLoaded() {
		log "Database is loaded, creating strategies";
		createUpperBoundaries();
		createLowerBoundaries();
		createRateThreshold();
		route DBLoaded();
	}

	action createUpperBoundaries() {
		DBBoundaryAlert value; 
		for value in cacheLoader.upperBoundaryConfig().values() {
			route CreateUpperBoundaryAlert(value.id, value.city, value.threshold);
		}
	}

	action createLowerBoundaries() {
		DBBoundaryAlert value; 
		for value in cacheLoader.lowerBoundaryConfig().values() {
			route CreateLowerBoundaryAlert(value.id, value.city, value.threshold);
		}
	}

	action createRateThreshold() {
		DBRateAlert value; 
		for value in cacheLoader.rateThreholdConfig().values() {
			route CreateRateThresholdAlert(value.id, value.city, value.percent, value.duration);
		}
	}
} 0000004d C:\Users\moray\Development\cycle-monitor\monitors\configuration\DBManager.mon
DISC 0000003c 6330853571953721678:6330572096977011022 from 127.0.0.1:53193
CONN 0000003c 6330853576297447758:6330572101320737102 from 127.0.0.1:53196
TIME 0000000e 1474016713.2,1
MONF 00000479 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.source;

event Start { }
event Stop { }
event Poll { }

event StationUpdate {
	string city;
	integer id;
	string name;
	float lat;
	float lng;
	float ratio;
	integer docked;
	integer empty;
	boolean active;
	float timestamp;

	action pprint() returns string {
		return " | ".join(self.getFieldValues());
	}
}
 0000004e C:\Users\moray\Development\cycle-monitor\monitors\source\StationDefinition.mon
MONF 00000510 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.source;
using com.jtech.config.DBLoaded;
using com.softwareag.connectivity.ConnectivityPlugins;

monitor StationManager {

	action onload {
		log "Station manager loaded" at INFO;

		// start the adapter when db is loaded
		on DBLoaded() { 
			log "The application is ready to initialise connectivity plugins";
			(new ConnectivityPlugins).onApplicationInitialized();
			
			log "Requesting city bikes connector to start";
			emit Start() to "CITY-BIKES";
		}
	}
} 0000004b C:\Users\moray\Development\cycle-monitor\monitors\source\StationManager.mon
DISC 0000003c 6330853576297447758:6330572101320737102 from 127.0.0.1:53196
CONN 0000003c 6330853576403353934:6330572101426643278 from 127.0.0.1:53199
TIME 0000000e 1474016713.3,1
MONF 00000447 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;
using com.jtech.source.StationUpdate;

event IAlertStrategy {
	action <> returns string  getCity;
	action <> returns integer getId;
	action <> returns string  getType;
	action <> returns string  getColor;
	action <StationUpdate> check;
	action <string> pprint;
}
 0000004d C:\Users\moray\Development\cycle-monitor\monitors\strategy\IAlertStrategy.mon
MONF 000003d8 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;
using com.jtech.source.StationUpdate;

event IAlertManager {
	action <IAlertStrategy, StationUpdate, string> addAlert;
	action <IAlertStrategy> clearAlert;
} 0000004c C:\Users\moray\Development\cycle-monitor\monitors\strategy\IAlertManager.mon
MONF 00000a3f //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;

event RemoveAlerts {}

event RemoveAlert {
	string city;
	integer id;
	string type;
}

event Alert {
	string city;
	integer id;
	string name;
	float lat;
	float lng;
	float ratio;
	integer docked;
	integer empty;
	float timestamp;
	string type;
	string message;
	string color;

	action init(string city, integer id, string type) returns Alert {
		Alert alert := new Alert;
		alert.city := city;
		alert.id := id;
		alert.type := type;
		return alert;
	}

	action iface() returns com.jtech.util.IKeyedDataView {
		com.jtech.util.IKeyedDataView this := new com.jtech.util.IKeyedDataView;
		this.getDVName := getDVName;
		this.getDVDisplayName := getDVDisplayName;
		this.getDVDescription := getDVDescription;
		this.getDVFieldNames := getDVFieldNames;
		this.getDVFieldTypes := getDVFieldTypes;
		this.getDVFieldValues := getDVFieldValues;
		this.getDVKeyFields := getDVKeyFields;
		this.getDVKeyFieldValues := getDVKeyFieldValues;
		return this;
	}

	action getDVName() returns string { return "StationAlert"; }
	action getDVDisplayName() returns string { return "StationAlert"; }
	action getDVDescription() returns string { return "An alert accociated with the cycle station"; }
	action getDVFieldNames() returns sequence<string> { return self.getFieldNames(); }
	action getDVFieldTypes() returns sequence<string> { return self.getFieldTypes(); }
	action getDVFieldValues() returns sequence<string> { return self.getFieldValues(); }
	action getDVKeyFields() returns sequence<string> {
		sequence<string> ret := new sequence<string>;
		ret := ["city","id","type"];
		return ret;
	}
	action getDVKeyFieldValues() returns sequence<string> {
		sequence<string> ret := new sequence<string>;
		ret := [city,id.toString(),type];
		return ret;
	}
} 0000004e C:\Users\moray\Development\cycle-monitor\monitors\strategy\AlertDefinition.mon
MONF 00000984 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;
using com.jtech.source.StationUpdate;

event AlertStrategy { 
	IAlertStrategy this;
	integer id;
	string  city;
	string  type;
	string  color;
	string  prefix;
	boolean alerted;
	IAlertManager manager;

	action init(integer id, string city, string type, string color, IAlertManager manager) returns IAlertStrategy {
		self.id := id;
		self.city := city;
		self.type := type;
		self.color := color;
		self.manager := manager;
		__setupRemoveListener();

		prefix := "["+city+"."+id.toString()+"."+type+"] ";
		log prefix+"Created strategy instance";

		this := new IAlertStrategy;
		this.getCity := getCity;
		this.getId := getId;
		this.getType := getType;
		this.getColor := getColor;
		this.check := check;
		this.pprint := pprint;
		return this;
	}

	//base implementations
	action pprint(string message) {log prefix+message;}
	action getCity() returns string { return city; }
	action getId() returns integer { return id; }
	action getType() returns string { return type; }
	action getColor() returns string { return color; }

	//abstract (implement in extending event)
	action check(StationUpdate update) {}

	//methods on the base (visible to extending event)
	action alert(StationUpdate update, string message) {
		alerted := true;
		manager.addAlert(this, update, message);
	}

	action clear() {
		alerted := false;
		manager.clearAlert(this);
	}

	//called within the base only
	action __setupRemoveListener() {
		on all (RemoveAlerts() or RemoveAlert(city,id,type) ){
			if (alerted) then { 
				alerted := false;
				clear(); 
			}
		}	
	}
} 0000004c C:\Users\moray\Development\cycle-monitor\monitors\strategy\AlertStrategy.mon
MONF 0000068a //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;
using com.jtech.source.StationUpdate;

event UpperBoundary {
	constant string  TYPE :="UPPER_BOUNDARY";
	constant string  COLOR:="blue";

	AlertStrategy base;
	float threshold;

	action init(integer id, string city, float threshold, IAlertManager manager) returns IAlertStrategy {    
		IAlertStrategy this := base.init(id, city, TYPE, COLOR, manager);
		this.check := check;

		self.threshold := threshold;
		return this;
	}

	//implementation of required actions
	action check(StationUpdate update) {
		log base.prefix+"Ratio" + update.ratio.toString() at DEBUG;
		if (not base.alerted and update.ratio > threshold) then {
			base.alert(update, "Ratio of " + update.ratio.formatFixed(2) + " exceeds limit " + threshold.toString());
		}
		else if (base.alerted and update.ratio <= threshold) then {
			base.clear();
		}
	}
} 0000004c C:\Users\moray\Development\cycle-monitor\monitors\strategy\UpperBoundary.mon
MONF 0000068a //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;
using com.jtech.source.StationUpdate;

event LowerBoundary {
	constant string  TYPE :="LOWER_BOUNDARY";
	constant string  COLOR:="red";

	AlertStrategy base;
	float threshold;

	action init(integer id, string city, float threshold, IAlertManager manager) returns IAlertStrategy {    
		IAlertStrategy this := base.init(id, city, TYPE, COLOR, manager);
		this.check := check;

		self.threshold := threshold;
		return this;
	}

	//implementation of required actions
	action check(StationUpdate update) {
		log base.prefix+"Ratio=" + update.ratio.toString() at DEBUG;
		if (not base.alerted and update.ratio < threshold) then {
			base.alert(update, "Ratio of " + update.ratio.formatFixed(2) + " exceeds limit " + threshold.toString());
		}
		else if (base.alerted and update.ratio >= threshold) then {
			base.clear();
		}
	}
} 0000004c C:\Users\moray\Development\cycle-monitor\monitors\strategy\LowerBoundary.mon
MONF 000009d5 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;
using com.apama.aggregates.first;
using com.apama.aggregates.last;
using com.jtech.source.StationUpdate;

event Result { float first; float last; }
event Data { string city; integer id; float ratio;  }

event RateThreshold {
	constant string  TYPE :="RATE_THRESHOLD";
	constant string  COLOR:="yellow";

	AlertStrategy base;
	float threshold;
	float duration;
	StationUpdate current;

	action init(integer id, string city, float threshold, float duration, IAlertManager manager) returns IAlertStrategy {    
		IAlertStrategy this := base.init(id, city, TYPE, COLOR, manager);
		this.check := check;

		self.threshold := threshold;
		self.duration := duration;
		setupStreamListener();
		return this;
	}

	//implementation of required actions
	action check(StationUpdate update) {
		current := update;
		route Data(base.city, base.id, update.ratio);
	}

	action setupStreamListener() {
		Result result;
		from u in all Data(city=base.city, id=base.id) within duration
		  select Result(first(u.ratio),last(u.ratio)):result {
			float change:=0.0;
			if ( not (result.first=result.last or result.first.isNaN() or result.last.isNaN()) ) then {
				change := (result.last-result.first).abs(); 
				log base.prefix+"Change="+change.formatFixed(3)+",first="+result.first.toString()+",last="+result.last.toString() at DEBUG;
				checkForAlert(change);
			}
		}
	} 

	action checkForAlert(float change) {
		if (not base.alerted and change > threshold) then {
			base.alert(current, "Change of " +change.formatFixed(2) + " exceeds limit " + threshold.toString());
		}
		else if (base.alerted and change <= threshold) then {
			base.clear();
		}
	}
}
 0000004c C:\Users\moray\Development\cycle-monitor\monitors\strategy\RateThreshold.mon
MONF 00000da7 //Cycle Monitor, Copyright (C) 2016  M.B.Grieve
//This file is part of the Cycle Monitor example application.

//Cycle Monitor is free software: you can redistribute it and/or modify
//it under the terms of the GNU General Public License as published by
//the Free Software Foundation, either version 3 of the License, or
//(at your option) any later version.

//Cycle Monitor is distributed in the hope that it will be useful,
//but WITHOUT ANY WARRANTY; without even the implied warranty of
//MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//GNU General Public License for more details.

//You should have received a copy of the GNU General Public License
//along with City Monitor.  If not, see <http://www.gnu.org/licenses/>.

//Contact: moray.grieve@me.com

package com.jtech.alert;
using com.jtech.config.DBKey;
using com.jtech.config.CreateUpperBoundaryAlert;
using com.jtech.config.CreateLowerBoundaryAlert;
using com.jtech.config.CreateRateThresholdAlert;
using com.jtech.source.StationUpdate;

monitor AlertManager {
	IAlertManager this;
	com.jtech.util.KeyedDataViewHelper dvHelper;
	dictionary < DBKey, sequence<IAlertStrategy> > strategies;

	action onload {
		log "Alert manager loaded" at INFO;

		//register the alert dataview
		dvHelper.register((new Alert).iface());

		//initialise the interface
		this.addAlert := addAlert;
		this.clearAlert := clearAlert;

		//handle create requests
		createUpperBoundary();
		createLowerBoundary();
		createRateThreshold();

		//pass updates to the relevant strategies
		StationUpdate stationUpdate;
		on all StationUpdate():stationUpdate {
			DBKey key := DBKey(stationUpdate.city, stationUpdate.id);
			if (strategies.hasKey(key)) then {
				IAlertStrategy strategy; for strategy in strategies[key] {
					strategy.check(stationUpdate);
				}
			}
		}
	}

	action addAlert(IAlertStrategy strategy, StationUpdate update, string message) {
		strategy.pprint(message+", adding alert to strategy");
		dvHelper.add((Alert(update.city, update.id, update.name, update.lat, update.lng, 
		                    update.ratio, update.docked, update.empty, update.timestamp, 
		                    strategy.getType(), message, strategy.getColor())).iface());
	}

	action clearAlert(IAlertStrategy strategy) {
		strategy.pprint("Removing alert from strategy");
		dvHelper.remove(((new Alert).init(strategy.getCity(),strategy.getId(),strategy.getType())).iface());
	}

	action createUpperBoundary() {
		CreateUpperBoundaryAlert uba;
		on all CreateUpperBoundaryAlert():uba {
			DBKey key := getKey(uba.city, uba.id);
			strategies[key].append( (new UpperBoundary).init(uba.id, uba.city, uba.threshold, this) );
		}
	}

	action createLowerBoundary() {
		CreateLowerBoundaryAlert lba;
		on all CreateLowerBoundaryAlert():lba {
			DBKey key := getKey(lba.city, lba.id);
			strategies[key].append( (new LowerBoundary).init(lba.id, lba.city, lba.threshold, this) );
		}
	}

	action createRateThreshold() {
		CreateRateThresholdAlert rta;
		on all CreateRateThresholdAlert():rta {
			DBKey key := getKey(rta.city, rta.id);
			strategies[key].append( (new RateThreshold).init(rta.id, rta.city, rta.threshold, rta.duration, this) );
		}
	}

	action getKey(string city, integer id) returns DBKey {
		DBKey key := DBKey(city, id);
		if not strategies.hasKey(key) then {
			strategies.add(key, new sequence<IAlertStrategy> );
		}
		return key;
	}
} 0000004b C:\Users\moray\Development\cycle-monitor\monitors\strategy\AlertManager.mon
DISC 0000003c 6330853576403353934:6330572101426643278 from 127.0.0.1:53199
CONN 0000003c 6330853576329691470:6330572101352980814 from 127.0.0.1:53202
EVNT 000000bc 14826393442096512897 "",com.apama.scenario.RequestInstancesOnChannel("DV_StationAlert",3620350030317551746,"com.apama.scenario.private_3620350030317551745_1474016713320_InstanceDiscovery")
TIME 0000000e 1474016713.4,1
EVNT 000000c8 6330853576329691470 "",com.jtech.config.DBConfig("com.apama.adbc.JDBC","jdbc:sqlite:C:\\Users\\moray\\Development\\cycle-monitor\\test\\testcases\\CycleMonitor_cor_009\\Output\\win32\\store.db","","")
TIME 0000000e 1474016713.5,1
DISC 0000003c 6330853576329691470:6330572101352980814 from 127.0.0.1:53202
EVNT 0000006b 6330853567618646350 "",com.apama.database.DatabaseOperationAck(2,"com.apama.adbc.JDBC","",1474016711460000)
TIME 0000000e 1474016713.6,1
EVNT 0000008a 6330853567618646350 "",com.apama.adapters.AdapterConnectionOpened("NullCodec","ADBC-JDBC","ADBC_JDBC_1474016711460000","1474016711460000")
EVNT 000000e1 6330853567618646350 "",com.apama.database.ResultSchema(3,"com.apama.adbc.JDBC",1,["city","id","percent","duration"],{"city":"string","id":"string","percent":"string","duration":"string"},["city","id","percent","duration"],{})
EVNT 00000058 6330853567618646350 "",com.apama.database.QueryDone(3,"com.apama.adbc.JDBC","",0,0.0,{})
EVNT 000000df 6330853567618646350 "",com.apama.database.ResultSchema(4,"com.apama.adbc.JDBC",1,["city","id","upper","threshold"],{"city":"string","id":"integer","upper":"integer","threshold":"float"},["city","id","upper","threshold"],{})
EVNT 000000b2 6330853567618646350 "",com.apama.database.ResultEvent(4,"com.apama.adbc.JDBC",0,{"upper":"0","schemaID":"1","city":"London","_ADBCType":"ResultEvent","id":"1","threshold":"0.3"})
EVNT 000000b2 6330853567618646350 "",com.apama.database.ResultEvent(4,"com.apama.adbc.JDBC",0,{"upper":"1","schemaID":"1","city":"London","_ADBCType":"ResultEvent","id":"2","threshold":"0.6"})
EVNT 00000058 6330853567618646350 "",com.apama.database.QueryDone(4,"com.apama.adbc.JDBC","",2,0.0,{})
TIME 0000000e 1474016713.7,1
EVNT 0000006b 6330853567618646350 "",com.apama.database.DatabaseOperationAck(5,"com.apama.adbc.JDBC","",1474016711460000)
EVNT 0000008a 6330853567618646350 "",com.apama.adapters.AdapterConnectionClosed("NullCodec","ADBC-JDBC","ADBC_JDBC_1474016711460000","1474016711460000")
EVNT 0000007b 14826393442096512897 "",com.apama.scenario.RequestScenarios("com.apama.scenario.private_3620350030317551744_1474016711773")
CONN 0000003c 6330853576379236686:6330572101402526030 from 127.0.0.1:53205
EVNT 0000007c 6330853576379236686 "",com.jtech.source.StationUpdate("London",1,"Hyde Park",51.512303,-0.159988,0.09,2,20,false,1435476177)
EVNT 0000007f 6330853576379236686 "",com.jtech.source.StationUpdate("London",2,"Regent Street",51.512304,-0.15998,0.91,20,2,false,1435476177)
TIME 0000000c 1474016714,1
DISC 0000003c 6330853576379236686:6330572101402526030 from 127.0.0.1:53205
CONN 0000003c 6330853580693340494:6330572105716629838 from 127.0.0.1:53208
EVNT 00000035 6330853580693340494 "",com.jtech.alert.RemoveAlerts()
TIME 0000000e 1474016714.3,1
DISC 0000003c 6330853580693340494:6330572105716629838 from 127.0.0.1:53208
TIME 0000000e 1474016714.7,1
DISC 0000003c 6330853567618646350:6330572092641935694 from 127.0.0.1:53164
TIME 0000000e 1474016717.3,1
TIME 0000000e 1474016717.4,1
TIME 0000000e 1474016720.3,1
TIME 0000000e 1474016722.4,1
